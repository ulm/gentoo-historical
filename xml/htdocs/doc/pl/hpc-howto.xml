<?xml version='1.0' encoding="UTF-8"?>
<!DOCTYPE guide SYSTEM "/dtd/guide.dtd">
<!-- $Header: /var/cvsroot/gentoo/xml/htdocs/doc/pl/hpc-howto.xml,v 1.9 2006/07/07 07:40:00 neysx Exp $ -->

<guide link="/doc/pl/hpc-howto.xml" lang="pl">

<title>Obliczenia wielkoskalowe na Gentoo</title>

<author title="Autor">
  <mail link="marc@adelielinux.com">Marc St-Pierre</mail>
</author>
<author title="Autor">
  <mail link="benoit@adelielinux.com">Benoit Morin</mail>
</author>
<author title="Asystent">
  <mail link="jean-francois@adelielinux.com">Jean-Francois Richard</mail>
</author>
<author title="Asystent">
  <mail link="olivier@adelielinux.com">Olivier Crete</mail>
</author>
<author title="Korekta">
  <mail link="dberkholz@gentoo.org">Donnie Berkholz</mail>
</author>
<author title="Tłumacz">
  Mateusz Kotyrba
</author>

<!-- No licensing information; this document has been written by a third-party
     organisation without additional licensing information.

     In other words, this is copyright adelielinux R&D; Gentoo only has
     permission to distribute this document as-is and update it when appropriate
     as long as the adelie linux R&D notice stays
-->

<abstract>
Ten dokument został napisany przez ludzi z Centrum Adelie Linux R&amp;D
&lt;http://www.adelielinux.com&gt; jako przewodnik pomagający w zamienieniu
Gentoo w system obliczeń wielkoskalowych.
</abstract>

<version>1.2</version>
<date>2003-08-01</date>

<chapter>
<title>Wstęp</title>
<section>
<body>

<p>
Gentoo to wyjątkowo konfigurowalna i nadająca się do optymalizowania dystrybucja
Linuksa, której dodatkowymi zaletami są świetna społeczność użytkowników i
developeróaw.
</p>

<p>
Gentoo dzięki technologii może stać się bezpiecznym serwerem, stacją
roboczą developera, profesjonalnym systemem biurkowym, maszyną do gier lub ...
systemem obliczeń wielkoskalowych. Dzięki swojej niemal nieograniczonej
przystosowalności można nazwać Gentoo metadystrybucją.
</p>

<p>
Ten dokument wyjaśnia jak zmienić Gentoo w system obliczeń wielkoskalowych.
Krok po kroku omówimy w nim, które pakiety należy zainstalować, a następnie jak je
skonfigurować.
</p>

<p>
Pierwszy krok to oczywiście instalacja Gentoo zgodnie ze wskazówkami spisanymi
w odpowiedniej <uri link="/doc/pl/">dokumentacji</uri>.
</p>

</body>
</section>
</chapter>

<chapter>
<title>Konfigurowanie Gentoo dla klasteryzacji (ang. clustering).</title>
<section>
<title>Zalecane optymalizacje.</title>
<body>

<note>
W tym akapicie nawiązujemy do <uri link="/doc/pl/handbook/">Podręcznika
Gentoo</uri>.
</note>

<p>
Podczas instalacji należy ustawić flagi USE w <path>/etc/make.conf</path>.
Zalecane jest wyłączenie wszystkich domyślnych zmiennych (patrz
<path>/etc/make.profile/make.defaults</path>) poprzez zanegowanie ich w
make.conf. Można pozostawić jedynie takie zmienne USE jak: x86, 3dnow,
gpm, mmx, sse, ncurses, pam and tcpd. Więcej informacji można znaleźć w
dokumentacji flag USE.
</p>

<pre caption="Flagi USE">
USE="-oss 3dnow -apm -arts -avi -berkdb -crypt -cups -encode -gdbm
-gif gpm -gtk -imlib -java -jpeg -kde -gnome -libg++ -libwww -mikmod
mmx -motif -mpeg ncurses -nls -oggvorbis -opengl pam -pdflib -png
-python -qt -qtmt -quicktime -readline -sdl -slang -spell -ssl
-svga tcpd -truetype -X -xml2 -xmms -xv -zlib"
</pre>

<p>
Lub po prostu:
</p>

<pre caption="Flagi USE - wersja uproszczona">
USE="-* 3dnow gpm mmx ncurses pam sse tcpd"
</pre>

<note>
Należy zauważyć, że flaga <e>tcpd</e> podnosi bezpieczeństwo, podobnie jak flaga
xinetd.
</note>

<p>
Dla wielkoskalowego systemu polecamy wybranie jądra vanilla-sources ze względu
na jego dużą stabilność, o ile nie są potrzebne dodatkowe łatki, zawierające np.
obsługę XFS. Źródła tego jądra można pobrać ze strony
<uri>http://www.kernel.org/</uri>.
</p>

<pre caption="Instalowanie jądra vanilla-sources">
# <i>emerge -p syslog-ng vanilla-sources</i>
</pre>

<p>
Polecamy zainstalowanie następujących pakietów:
</p>

<pre caption="Instalowanie potrzebnych pakietów">
# <i>emerge -p nfs-utils portmap tcpdump ssmtp iptables xinetd</i>
</pre>

</body>
</section>
<section>
<title>Warstwa komunikacyjna (sieć TCP/IP).</title>
<body>

<p>
Klaster wymaga warstwy komunikacyjnej do łączenia węzłów podrzędnych z węzłem
głównym. Zwykle Fast Ethernet lub Giga Ethernet LAN mogą być użyte ponieważ mają
dobry stosunek cena/jakość. Inne możliwości uwzględniają użycie produktów
takich jak: <uri link="http://www.myricom.com/">Myrinet</uri>, <uri
link="http://quadrics.com/">QsNet</uri> lub innych.
</p>

<p>
Zwykle klaster składa się z węzła głównego oraz kilku węzłów podrzędnych.
</p>

<p>
Węzeł główny jest serwerem klastra. Jest on odpowiedzialny za wyznaczanie zadań
dla węzłów podrzędnych. Serwer zwykle ma uruchomione takie demony jak:
dhcpd, nfs, pbs-server i pbs-sched. Węzeł główny będzie pozwalał na
interakcyjne sesje dla użytkowników oraz będzie zatwierdzał wykonywanie zadań.
</p>

<p>
Węzły podrzędne nasłuchują instrukcji (poprzez ssh/rsh) z węzła głównego.
Powinny one być poświęcone na opracowywanie wyników, a zatem nie powinny
mieć uruchomionych żadnych niepotrzebnych usług.
</p>

<p>
Skonfigurujemy teraz plik hosts dla naszego klastra. Każdy węzeł powinien
posiadać plik hosts (<path>/etc/hosts</path>) z wpisami dla każdego węzła
będącego w klastrze.
</p>

<pre caption="/etc/hosts">
# Adelie Linux Research &amp; Development Center
# /etc/hosts

127.0.0.1  localhost

192.168.1.100  master.adelie master

192.168.1.1  node01.adelie node01
192.168.1.2  node02.adelie node02
</pre>

<p>
Aby ustawić sieć LAN oddaną naszemu klastrowi należy zedytować plik
<path>/etc/conf.d/net</path> na węźle głównym.
</p>

<pre caption="/etc/conf.d/net">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License, v2 or later

# Global config file for net.* rc-scripts

# This is basically the ifconfig argument without the ifconfig $iface
#

iface_eth0="192.168.1.100 broadcast 192.168.1.255 netmask 255.255.255.0"
# Network Connection to the outside world using dhcp -- configure as required for you network
iface_eth1="dhcp"
</pre>


<p>
W końcu ustawiamy demon DHCP na węźle głównym, dzięki czemu unikniemy
konfigurowania sieci na każdym z węzłów podrzędnych.
</p>

<pre caption="/etc/dhcp/dhcpd.conf">
# Adelie Linux Research &amp; Development Center
# /etc/dhcp/dhcpd.conf

log-facility local7;
ddns-update-style none;
use-host-decl-names on;

subnet 192.168.1.0 netmask 255.255.255.0 {
        option domain-name "adelie";
        range 192.168.1.10 192.168.1.99;
        option routers 192.168.1.100;

        host node01.adelie {
    # MAC address of network card on node 01
                hardware ethernet 00:07:e9:0f:e2:d4;
                fixed-address 192.168.1.1;
        }
        host node02.adelie {
    # MAC address of network card on node 02
                hardware ethernet 00:07:e9:0f:e2:6b;
                fixed-address 192.168.1.2;
        }
}
</pre>

</body>
</section>
<section>
<title>NFS/NIS.</title>
<body>

<p>
Network File System (NFS) został stworzony, aby umożliwić komputerom
zamontowanie partycji z maszyny zdalnej, tak jakby była ona na lokalnym
dysku, co pozwala na szybkie i jednolite współdzielenie plików w całej sieci.
</p>

<p>
Są inne systemy, które dają podobne możliwości jak NFS i które również mogłyby
zostać użyte w środowisku klastra. <uri link="http://www.openafs.org/">Andrew
File System od IBM</uri>, którego kod został ostatnio otwarty to dobry
mechanizm do współdzielenia plików z pewnymi dodatkowymi funkcjami
bezpieczeństwa oraz wydajności. <uri link="http://www.coda.cs.cmu.edu/">Coda
File System</uri> jest ciągle rozwijany. Jest to system zaprojektowany do
dobrego współdziałania z odłączonymi klientami. Wiele cech systemów plików
Andrew i Coda na pewno zostanie włączonych do następnej wersji <uri
link="http://www.nfsv4.org">NFS (wersja 4)</uri>. Zaletą NFS jest fakt, że jest
to system bardzo dopracowany i przez wielu uznawany już za standard, a ponadto
powszechnie znany oraz obsługiwany na bardzo wielu różnych architekturach.
</p>

<pre caption="Ebuildy dające obsługę NFS">
# <i>emerge -p nfs-utils portmap</i>
# <i>emerge nfs-utils portmap</i>
</pre>

<p>
Należy skonfigurować i zainstalować jądro aby wspierało NFS v3 na wszystkich
węzłach:
</p>

<pre caption="Wymagane dla NFS opcje konfiguracji jądra">
CONFIG_NFS_FS=y
CONFIG_NFSD=y
CONFIG_SUNRPC=y
CONFIG_LOCKD=y
CONFIG_NFSD_V3=y
CONFIG_LOCKD_V4=y
</pre>

<p>
Na węźle głównym należy edytować plik <path>/etc/hosts.allow</path> i
zezwolić na połączenia z węzłów podrzędnych. Jeśli sieć klastra jest na
192.168.1.0/24, to <path>hosts.allow</path> będzie tak wyglądał:
</p>

<pre caption="hosts.allow">
portmap:192.168.1.0/255.255.255.0
</pre>

<p>
Następnie należy edytować plik <path>/etc/exports</path> węzła głównego, aby
wyeksportować strukturę katalogu roboczego (odpowiedni do tego jest np. /home).
</p>

<pre caption="/etc/exports">
/home/  *(rw)
</pre>

<p>
Skrypt nfs powinien znaleźć się na domyślnym poziomie uruchomieniowym:
</p>

<pre caption="Dodawanie NFS do domyślnego poziomu uruchamiania">
# <i>rc-update add nfs default</i>
</pre>

<p>
Aby móc montować wyeksportowany system plików nfs z głównego węzła należy
odpowiednio skonfigurować także węzły podrzędne. W tym celu do
<path>/etc/fstab</path> dodajemy taką linię:
</p>

<pre caption="/etc/fstab">
master:/home/  /home  nfs  rw,exec,noauto,nouser,async  0 0
</pre>

<p>
Potrzebujemy jeszcze ustawić wszystkie węzły, aby mogły montować system plików
nfs. Dokonuje się tego następującym poleceniem:
</p>

<pre caption="Dodanie nfsmount do domyślnego poziomu uruchamiania">
# <i>rc-update add nfsmount default</i>
</pre>

</body>
</section>
<section>
<title>RSH/SSH.</title>
<body>

<p>
SSH to protokół umożliwiające bezpieczne zdalne logowanie, oraz dostarczający
wielu innych usług sieciowych. OpenSSH zapewnia bezpieczną autoryzację poprzez
klucz publiczny i prywatny. Klucz publiczny jest współdzielony na wszystkich
systemach, prywatny znajduje się tylko w lokalnym komputerze. Aby z nich
korzystać należy najpierw skonfigurować klaster OpenSSH.
</p>

<p>
Proces wdrażania systemu kluczy w klastrze składa się z dwóch etapów:
</p>

<ul>
  <li>Generowania klucza publicznego i prywatnego,</li>
  <li>oraz kopiowania klucza publicznego na węzły podrzędne</li>
</ul>

<p>
Klucze generuje się w ten sposób:
</p>

<pre caption="Uwierzytelnianie klucza SSH">
# <i>ssh-keygen -t dsa</i>
Generating public/private dsa key pair.
Enter file in which to save the key (/root/.ssh/id_dsa): /root/.ssh/id_dsa
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_dsa.
Your public key has been saved in /root/.ssh/id_dsa.pub.
The key fingerprint is:
f1:45:15:40:fd:3c:2d:f7:9f:ea:55:df:76:2f:a4:1f root@master

<comment>UWAGA! Jeśli istnieje już plik o nazwie "authorized_keys" nie należy
korzystać z poniższego polecenia, tylko ręcznie dodać do niego odpowiedni wpis.</comment>

# <i>scp /root/.ssh/id_dsa.pub node01:/root/.ssh/authorized_keys</i>
root@master's password:
id_dsa.pub   100%  234     2.0MB/s   00:00

# <i>scp /root/.ssh/id_dsa.pub node02:/root/.ssh/authorized_keys</i>
root@master's password:
id_dsa.pub   100%  234     2.0MB/s   00:00
</pre>

<note>
Klucze hosta muszą mieć pustą frazę kodującą hasła. RSA jest wymagane dla
uwierzytelnienia hosta.
</note>

<p>
By umożliwić uwierzytelnianie hosta trzeba również edytować plik
<path>/etc/ssh/shosts.equiv</path>.
</p>

<pre caption="/etc/ssh/shosts.equiv">
node01.adelie
node02.adelie
master.adelie
</pre>

<p>
Oraz wykonać kilka zmian w pliku <path>/etc/ssh/sshd_config</path>:
</p>

<pre caption="Konfiguracja sshd">
# $OpenBSD: sshd_config,v 1.42 2001/09/20 20:57:51 mouring Exp $
# This sshd was compiled with PATH=/usr/bin:/bin:/usr/sbin:/sbin

# This is the sshd server system-wide configuration file.  See sshd(8)
# for more information.

# HostKeys for protocol version 2
HostKey /etc/ssh/ssh_host_rsa_key
</pre>

<p>
Jeśli program wymaga komunikacji RSH, należy zemergować net-misc/netkit-rsh i
sys-apps/xinetd.
</p>

<pre caption="Instalowanie potrzebnych programów">
# <i>emerge -p xinetd</i>
# <i>emerge xinetd</i>
# <i>emerge -p netkit-rsh</i>
# <i>emerge netkit-rsh</i>
</pre>

<p>
Potem konfigurujemy demona rsh. W tym celu edytujemy plik <path>/etc/xinet.d/rsh</path>.
</p>

<pre caption="rsh">
# Adelie Linux Research &amp; Development Center
# /etc/xinetd.d/rsh

service shell
{
        socket_type     = stream
        protocol        = tcp
        wait            = no
        user            = root
        group           = tty
        server          = /usr/sbin/in.rshd
        log_type        = FILE /var/log/rsh
        log_on_success  = PID HOST USERID EXIT DURATION
        log_on_failure  = USERID ATTEMPT
        disable         = no
}
</pre>

<p>
A następnie plik <path>/etc/hosts.allow</path> i zezwalamy na połączenia rsh.
</p>

<pre caption="hosts.allow">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.allow

in.rshd:192.168.1.0/255.255.255.0
</pre>

<p>
Lub po prostu możemy zaufać sieci LAN klastra:
</p>

<pre caption="hosts.allow">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.allow

ALL:192.168.1.0/255.255.255.0
</pre>

<p>
W końcu konfigurujemy uwierzytelnianie hosta w <path>/etc/hosts.equiv</path>.
</p>

<pre caption="hosts.equiv">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.equiv

master
node01
node02
</pre>

<p>
Potem należy dodać xinetd do domyślnego poziomu uruchamiania:
</p>

<pre caption="Dodawanie xinetd do domyślnego poziomu uruchamiania">
# <i>rc-update add xinetd default</i>
</pre>

</body>
</section>
<section>
<title>NTP.</title>
<body>

<p>
Network Time Protocol (NTP) jest używany do synchronizacji czasu
komputera klienta lub serwera do innego serwera lub innego odnośnika źródła
czasu, jak np. odbiornik radiowy, satelitarny lub modem. Dostarcza ono
dokładność rzędu milisekund w sieci LAN i do kilkudziesięciu milisekund w sieci
WAN w odniesieniu do czasu uniwersalnego (UTC), poprzez na przykład satelitarny
program lokalizujący GPS. Typowe ustawienia NTP wykorzystują wielorakie
rezerwowe serwery oraz różnorodne ścieżki sieciowe, co zapewnia wysoką
dokładność i rzetelność wyniku.
</p>

<p>
Należy wybrać najbliższy geograficznie serwer NTP ze strony <uri
link="http://www.eecis.udel.edu/~mills/ntp/servers.html">NTP
Time Servers</uri> oraz skonfigurować pliki <path>/etc/conf.d/ntp</path> i
<path>/etc/ntp.conf</path> na węźle głównym.
</p>

<pre caption="Główny /etc/conf.d/ntp">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License v2
# /etc/conf.d/ntpd

# NOTES:
#  - NTPDATE variables below are used if you wish to set your
#    clock when you start the ntp init.d script
#  - make sure that the NTPDATE_CMD will close by itself ...
#    the init.d script will not attempt to kill/stop it
#  - ntpd will be used to maintain synchronization with a time
#    server regardless of what NTPDATE is set to
#  - read each of the comments above each of the variable

# Comment this out if you dont want the init script to warn
# about not having ntpdate setup
NTPDATE_WARN="n"

# Command to run to set the clock initially
# Most people should just uncomment this line ...
# however, if you know what you're doing, and you
# want to use ntpd to set the clock, change this to 'ntpd'
NTPDATE_CMD="ntpdate"

# Options to pass to the above command
# Most people should just uncomment this variable and
# change 'someserver' to a valid hostname which you
# can aquire from the URL's below
NTPDATE_OPTS="-b ntp1.cmc.ec.gc.ca"

##
# A list of available servers is available here:
# http://www.eecis.udel.edu/~mills/ntp/servers.html
# Please follow the rules of engagement and use a
# Stratum 2 server (unless you qualify for Stratum 1)
##

# Options to pass to the ntpd process that will *always* be run
# Most people should not uncomment this line ...
# however, if you know what you're doing, feel free to tweak
#NTPD_OPTS=""

</pre>

<p>
Następnie edytujemy plik <path>/etc/ntp.conf</path> na węźle głównym, aby
ustawić zewnętrzne źródło synchronizacji:
</p>

<pre caption="Główny ntp.conf">
# Adelie Linux Research &amp; Development Center
# /etc/ntp.conf

# Synchronization source #1
server ntp1.cmc.ec.gc.ca
restrict ntp1.cmc.ec.gc.ca
# Synchronization source #2
server ntp2.cmc.ec.gc.ca
restrict ntp2.cmc.ec.gc.ca
stratum 10
driftfile /etc/ntp.drift.server
logfile  /var/log/ntp
broadcast 192.168.1.255
restrict default kod
restrict 127.0.0.1
restrict 192.168.1.0 mask 255.255.255.0
</pre>

<p>
Później na wszystkich węzłach podrzędnych ustawiamy źródło synchronizacji tak
jak na węźle głównym.
</p>

<pre caption="Węzeł /etc/conf.d/ntp">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License v2
# /etc/conf.d/ntpd

NTPDATE_WARN="n"
NTPDATE_CMD="ntpdate"
NTPDATE_OPTS="-b master"
</pre>

<pre caption="Węzeł ntp.conf">
# Adelie Linux Research &amp; Development Center
# /etc/ntp.conf

# Synchronization source #1
server master
restrict master
stratum 11
driftfile /etc/ntp.drift.server
logfile  /var/log/ntp
restrict default kod
restrict 127.0.0.1
</pre>

<p>
Następnie na wszystkich węzłach należy dodać demon ntpd do domyślnego poziomu
uruchamiania:
</p>

<pre caption="Dodawanie demona ntpd do domyślnego poziomu uruchamiania">
# <i>rc-update add ntpd default</i>
</pre>

<note>
NTP nie zaktualizuje zegara lokalnego jeśli różnica czasu pomiędzy źródłem
synchronizacji, a czasem lokalnym jest zbyt duża.
</note>

</body>
</section>
<section>
<title>Iptables.</title>
<body>

<p>
Do ustawienia zapory ogniowej w klastrze będziemy potrzebować iptables.
</p>

<pre caption="Instalowanie iptables">
# <i>emerge -p iptables</i>
# <i>emerge iptables</i>
</pre>

<p>
Wymagana konfiguracja jądra:
</p>

<pre caption="Konfigurowanie jądra dla iptables">
CONFIG_NETFILTER=y
CONFIG_IP_NF_CONNTRACK=y
CONFIG_IP_NF_IPTABLES=y
CONFIG_IP_NF_MATCH_STATE=y
CONFIG_IP_NF_FILTER=y
CONFIG_IP_NF_TARGET_REJECT=y
CONFIG_IP_NF_NAT=y
CONFIG_IP_NF_NAT_NEEDED=y
CONFIG_IP_NF_TARGET_MASQUERADE=y
CONFIG_IP_NF_TARGET_LOG=y
</pre>

<p>
A to reguły wymagane dla naszej zapory ogniowej:
</p>

<pre caption="rule-save">
# Adelie Linux Research &amp; Development Center
# /var/lib/iptables/rule-save

*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -s 192.168.1.0/255.255.255.0 -i eth1 -j ACCEPT
-A INPUT -s 127.0.0.1 -i lo -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -j LOG
-A INPUT -j REJECT --reject-with icmp-port-unreachable
COMMIT
*nat
:PREROUTING ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A POSTROUTING -s 192.168.1.0/255.255.255.0 -j MASQUERADE
COMMIT
</pre>

<p>
Następnie na wszystkich węzłach należy dodać iptables do domyślnego poziomu
uruchamiania:
</p>

<pre caption="Dodawanie iptables do domyślnego poziomu uruchamiania">
# <i>rc-update add iptables default</i>
</pre>

</body>
</section>
</chapter>

<chapter>
<title>Narzędzia HPC.</title>
<section>
<title>OpenPBS.</title>
<body>

<p>
Portable Batch System (PBS) jest elastycznym systemem nadzorującym
kolejkowanie prac wsadowych oraz obciążenie pracą, który pierwotnie został
opracowany dla NASA. Operuje ono na sieciowych wieloplatformowych środowiskach
UNIX, włącznie z niejednorodnymi klastrami stacji roboczych, superkomputerów
oraz w operacyjnych komputerach masywnie równoległych (massive parallel
systems). PBS jest rozwijany przez Altair Grid Technologies.
</p>

<pre caption="Instalowanie openpbs">
# <i>emerge -p openpbs</i>
</pre>

<note>
Ebuild OpenPBS w chwili obecnej nie ustawia poprawnie uprawnień w
katalogach var używanych przez OpenPBS.
</note>

<p>
Przed rozpoczęciem użytkowania OpenPBS wymagane są zatem pewne zmiany w
konfiguracji. Pliki, których ustawienia będziemy musieli dostosować to:
</p>

<ul>
<li>/etc/pbs_environment</li>
<li>/var/spool/PBS/server_name</li>
<li>/var/spool/PBS/server_priv/nodes</li>
<li>/var/spool/PBS/mom_priv/config</li>
<li>/var/spool/PBS/sched_priv/sched_config</li>
</ul>

<p>
Przykładowy sched_config:
</p>

<pre caption="/var/spool/PBS/sched_priv/sched_config">
#
# Create queues and set their attributes.
#
#
# Create and define queue upto4nodes
#
create queue upto4nodes
set queue upto4nodes queue_type = Execution
set queue upto4nodes Priority = 100
set queue upto4nodes resources_max.nodect = 4
set queue upto4nodes resources_min.nodect = 1
set queue upto4nodes enabled = True
set queue upto4nodes started = True
#
# Create and define queue default
#
create queue default
set queue default queue_type = Route
set queue default route_destinations = upto4nodes
set queue default enabled = True
set queue default started = True
#
# Set server attributes.
#
set server scheduling = True
set server acl_host_enable = True
set server default_queue = default
set server log_events = 511
set server mail_from = adm
set server query_other_jobs = True
set server resources_default.neednodes = 1
set server resources_default.nodect = 1
set server resources_default.nodes = 1
set server scheduler_iteration = 60
</pre>

<p>
Aby przedłożyć zadanie OpenPBS należy użyć polecenia <c>qsub</c> z
opcjonalnymi parametrami. W przykładzie poniżej, "-l" pozwala określić wymagane
zasoby, "-j" zapewnia przekierowanie wyjścia standardowego oraz standardowego
raportowania błędów, a "-m" wysyła e-mail do użytkownika przy rozpoczęciu (b),
zakończeniu (e) i anulowaniu (a) zadania.
</p>

<pre caption="Przedkładanie zadania">
<comment>(przedłożenie i żądanie aby OpenPBS uruchomił myscript na 2 węzłach)</comment>
# <i>qsub -l nodes=2 -j oe -m abe myscript</i>
</pre>

<p>
Zwykle zadania dla OpenPBS są przedkładane w formie skryptów. Istnieje również
funkcja ręcznego wykonywania poleceń dla celów testowych. Aby zażądać
interaktywnej powłoki od OpenPBS należy podać parametr "-I".
</p>

<pre caption="Żądanie interaktywnej powłoki">
# <i>qsub -I</i>
</pre>

<p>
Aby sprawdzić stan zadań należy wykonać polecenie qstat:
</p>

<pre caption="Sprawdzanie stanu zadań">
# <i>qstat</i>
Job id  Name  User   Time Use S Queue
------  ----  ----   -------- - -----
2.geist STDIN adelie 0        R upto1nodes
</pre>

</body>
</section>
<section>
<title>MPICH.</title>
<body>

<p>
Przesyłanie komunikatów jest paradygmatem używanym szeroko w pewnych klasach
maszyn pracujących równolegle, szczególnie na tych z pamięcią rozproszoną. MPICH
jest darmową, przenośną implementacją MPI, czyli standardu do bibliotek
przesyłania komunikatów.
</p>

<p>
Stworzony przez Adelie Linux ebuild mpich pozwala na użycie 2 flag USE:
<e>doc</e> i <e>crypt</e>.  <e>doc</e> spowoduje zainstalowanie dokumentacji, a
<e>crypt</e> skonfiguruje MPICH, aby używał <c>ssh</c> zamiast <c>rsh</c>.
</p>

<pre caption="Instalowanie programu mpich">
# <i>emerge -p mpich</i>
# <i>emerge mpich</i>
</pre>

<p>
Może być również konieczne wyeksportowanie katalogu roboczego mpich na wszystkie
węzły podrzędne w <path>/etc/exports</path>:
</p>

<pre caption="/etc/exports">
/home  *(rw)
</pre>

<p>
Większość operacyjnych komputerów masywnie równoległych (ang. massively parallel
processors) daje możliwość uruchomienia programu na żądanej przez nas liczbie
procesorów. <c>mpirun</c> robi użytek z odpowiedniego polecenia kiedykolwiek
jest to możliwe. Z drugiej strony klastry złożone ze stacji roboczych wymagają
aby każdy proces w równoległym zadaniu został uruchomiony oddzielnie, pomimo, że
istnieją programy, które pomagają start tym procesom. Ponieważ klastry stacji
roboczych nie są jeszcze tak zorganizowane jak MPP, więc dodatkowa informacja
jest wymagana aby zrobić z nich użytek. Mpich powinno być zainstalowane z listą
uczestniczących stacji roboczych w pliku <path>machines.LINUX</path>, który się
znajduje w katalogu <path>/usr/share/mpich/</path>. Ten plik jest używany przez
<c>mpirun</c> do wybierania procesorów na których będzie dane zadanie
wykonywane.
</p>

<p>
Należy edytować ten plik, tak by odpowiadał on konfiguracji sieci klastra:
</p>

<pre caption="/usr/share/mpich/machines.LINUX">
# Change this file to contain the machines that you want to use
# to run MPI jobs on.  The format is one host name per line, with either
#    hostname
# or
#    hostname:n
# where n is the number of processors in an SMP.  The hostname should
# be the same as the result from the command "hostname"
master
node01
node02
# node03
# node04
# ...
</pre>

<p>
Później należy użyć skryptu <c>tstmachines</c> w <path>/usr/sbin/</path> aby
się upewnić, że możemy używać wszystkich maszyn, które wcześniej zostały wpisane
do pliku machines.LINUX. Ten skrypt wykonuje <c>rsh</c> i wyświetla krótką
zawartość katalogu; te testy sprawdzają, czy obie maszyny mają dostęp do węzła i czy
program w katalogu bieżącym jest widoczny na węźle zdalnym. Jeśli są
jakiekolwiek problemy, to zostaną one wyświetlone. Te problemy muszą być
naprawione zanim przystąpimy do nastepnego kroku.
</p>

<p>
Jedynym parametrem dla <c>tstmachines</c> jest nazwa architektury, która jest
taka sama jak rozszerzenie w pliku machines. Dla przykładu, następujące
polecenie sprawdza czy program z katalogu bieżącego może być uruchomiony przez
wszystkie maszyny, które są na liście maszyn LINUX.
</p>

<pre caption="Uruchamianie testu">
# <i>/usr/local/mpich/sbin/tstmachines LINUX</i>
</pre>

<note>
Ten program pracuje w trybie dyskretnym; jeśli chcemy zobaczyć co on wykonuje,
możemy użyć argumentu -v (tryb szczegółowego informowania o wszystkim):
</note>

<pre caption="Uruchamianie testu ze szczogółowymi informacjami">
# <i>/usr/local/mpich/sbin/tstmachines -v LINUX</i>
</pre>

<p>
Wynik tego polecenia może wyglądać tak jak poniżej:
</p>

<pre caption="Wynik powyższego polecenia">
Trying true on host1.uoffoo.edu ...
Trying true on host2.uoffoo.edu ...
Trying ls on host1.uoffoo.edu ...
Trying ls on host2.uoffoo.edu ...
Trying user program on host1.uoffoo.edu ...
Trying user program on host2.uoffoo.edu ...
</pre>

<p>
Jeśli <c>tstmachines</c> znajdzie problem, zasugeruje on możliwe przyczyny i
rozwiązania tego problemu. W skrócie wykonuje on 3 testy:
</p>

<ul>
<li>
<e>Czy procesy mogą być wykonywane na zdalnych komputerach?</e> tstmachines
próbuje uruchomić polecenie powłoki true na każdej maszynie, która jest
na liście.
</li>
<li>
<e>Czy bieżący katalog jest dostępny dla wszystkich komputerów?</e> Spróbuje
poleceniem ls wyświetlić plik, który wcześniej utworzy.
</li>
<li>
<e>Czy programy użytkownika mogą być uruchamiane na zdalnych systemach?</e>
sprawdza czy biblioteki współdzielone i inne komponenty są poprawnie
zaintalowane na wszystkich komputerach.
</li>
</ul>

<p>
Wymagany test dla każdego narzędzia programistycznego:
</p>

<pre caption="Testowanie narzędzia programistycznego">
# <i>cd ~</i>
# <i>cp /usr/share/mpich/examples1/hello++.c ~</i>
# <i>make hello++</i>
# <i>mpirun -machinefile /usr/share/mpich/machines.LINUX -np 1 hello++</i>
</pre>

<p>
Aby uzyskać więcej informacji o MPICH należy zajrzeć do dokumentacji na <uri
link="http://www-unix.mcs.anl.gov/mpi/mpich/docs/mpichman-chp4/mpichman-chp4.
htm">http://www-unix.mcs.anl.gov/mpi/mpich/docs/mpichman-chp4/mpichman-chp4.htm</uri>.
</p>

</body>
</section>
<section>
<title>LAM.</title>
<body>

<p>
(Wkrótce)
</p>

</body>
</section>
<section>
<title>OMNI.</title>
<body>

<p>
(Wkrótce)
</p>

</body>
</section>
</chapter>

<chapter>
<title>Bibliografia</title>
<section>
<body>

<p>
Oryginalny dokument został opublikowany na stronie <uri
link="http://www.adelielinux.com">Adelie Linux R&amp;D Centre</uri>,
oraz jest powielony tu za zgodą autorów i <uri
link="http://www.cyberlogic.ca">Cyberlogic</uri>'s Adelie Linux R&amp;D
Centre.
</p>

<ul>
  <li><uri>http://www.gentoo.org</uri>, Gentoo Technologies, Inc.</li>
  <li>
    <uri link="http://www.adelielinux.com">http://www.adelielinux.com</uri>,
    Adelie Linux Research and Development Centre
  </li>
  <li>
    <uri link="http://nfs.sourceforge.net/">http://nfs.sourceforge.net</uri>,
    Linux NFS Project
  </li>
  <li>
    <uri link="http://www-unix.mcs.anl.gov/mpi/mpich/">http://www-unix.mcs.anl.gov/mpi/mpich/</uri>,
    Mathematics and Computer Science Division, Argonne National Laboratory
  </li>
  <li>
    <uri link="http://www.ntp.org/">http://ntp.org</uri>
  </li>
  <li>
    <uri link="http://www.eecis.udel.edu/~mills/">http://www.eecis.udel.edu/~mills/</uri>,
    David L. Mills, University of Delaware
  </li>
  <li>
    <uri link="http://www.ietf.org/html.charters/secsh-charter.html">http://www.ietf.org/html.charters/secsh-charter.html</uri>,
    Secure Shell Working Group, IETF, Internet Society
  </li>
  <li>
    <uri link="http://www.linuxsecurity.com/">http://www.linuxsecurity.com/</uri>,
    Guardian Digital
  </li>
  <li>
    <uri link="http://www.openpbs.org/">http://www.openpbs.org/</uri>,
    Altair Grid Technologies, LLC.
  </li>
</ul>

</body>
</section>
</chapter>

</guide>
