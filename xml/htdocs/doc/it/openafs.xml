<?xml version='1.0' encoding="UTF-8"?>
<!DOCTYPE guide SYSTEM "/dtd/guide.dtd">
<!-- $Header: /var/cvsroot/gentoo/xml/htdocs/doc/it/openafs.xml,v 1.8 2007/06/11 20:51:01 scen Exp $ -->

<guide link="/doc/it/openafs.xml" lang="it">
<title>Guida ad OpenAFS in Gentoo Linux</title>

<author title="Redazione">
  <mail link="darks@gentoo.org">Holger Brueckner</mail>
</author>
<author title="Redazione">
  <mail link="bennyc@gentoo.org">Benny Chuang</mail>
</author>
<author title="Redazione">
  <mail link="blubber@gentoo.org">Tiemo Kieft</mail>
</author>
<author title="Redazione">
  <mail link="fnjordy@gmail.com">Steven McCoy</mail>
</author>
<author title="Redazione">
  <mail link="stefaan@gentoo.org">Stefaan De Roeck</mail>
</author>
<author title="Redazione">
  <mail link="fox2mike@gentoo.org">Shyam Mani</mail>
</author>
<author title="Traduzione">
  <mail link="frgrieco@aliceposta.it">Francesco Grieco</mail>
</author>

<abstract>
Questa guida mostra come installare un server OpenAFS ed un client su Gentoo
Linux
</abstract>

<!-- The content of this document is licensed under the CC-BY-SA license -->
<!-- See http://creativecommons.org/licenses/by-sa/2.5 -->
<license/>

<version>1.1</version>
<date>2005-11-10</date>

<chapter>
<title>Introduzione</title>
<section>
<title>Premessa</title>
<body>

<p>
Questo documento descrive tutti i passi necessari all'installazione di un server
OpenAFS su Gentoo Linux. Parti di questo documento sono tratte dalle FAQ AFS e
dalla guida IBM's Quick Beginnings su AFS.
</p>

</body>
</section>
<section>
<title>Cos'è AFS?</title>
<body>

<p>
AFS è un filesystem distribuito che permette ad host cooperanti (client e
server) di condividere in maniera efficiente un filesystem attraverso una rete
locale (LAN) o estesa (WAN). I Client mantengono una cache per gli oggetti
(file) maggiormente utilizzati, per averne un accesso più rapido.
</p>

<p>
AFS è basato su un filesystem distribuito chiamato "Andrew File System",
originariamente sviluppato all'Information Technology Center della
Carnegie-Mellon University. "Andrew" era il nome del progetto di ricerca alla
CMU, in ricordo del fondatore dell'Università. Fondata la Transarc e divenuto
AFS un prodotto, si decise di eliminare il nome "Andrew" al fine di indicare
che ormai si era andati oltre il progetto di ricerca universitario. AFS era
divenuto un filesystem robusto e di qualità. Tuttavia, essendo ancora esistenti
dei nodi (cell) con filesystem del tipo /asf, e, nota la difficoltà dell'epoca
nel cambiare il volume root del filesystem, il nome AFS rimase tale.
</p>

</body>
</section>
<section>
<title>Cos'è un nodo (cell) AFS ?</title>
<body>


<p>
Un nodo (cell) AFS è un insieme di server collegati tra loro che presentano un
singolo filesystem. Di solito, un nodo AFS è costituito da host (postazioni) che
usano lo stesso dominio Internet ("domain name"), ad esempio gentoo.org. Gli
utenti si collegano ai client workstation, i quali richiedono dati e file dai
server del nodo per gli utenti stessi. Gli utenti non sapranno su quale server è
situato il file a cui stanno accedendo, poichè ogni volume può essere copiato e
spostato su di un altro server senza che se ne accorgano. I file sono sempre
accessibili. È una sorta di NFS potenziato.
</p>

</body>
</section>
<section>
<title>Che benefici si ottengono con l'utilizzo di AFS?</title>
<body>

<p>
I punti di forza di AFS sono: abilità di caching (da 100M a 1GB lato client),
caratteristiche di sicurezza (basato su Kerberos 4, con access control list),
semplicità di addressing (si ha in pratica un solo filesystem), scalabilità
(ulteriore aggiunta di server al nodo in caso di necessità), protocolli di
comunicazione.
</p>

</body>
</section>
<section>
<title>Dove si possono trovare maggiori informazioni?</title>
<body>

<p>
Leggere le <uri link="http://www.angelfire.com/hi/plutonic/afs-faq.html">FAQ
ASF</uri>.
</p>

<p>
Il sito ufficiale di OpenAFS è <uri
link="http://www.openafs.org">www.openafs.org</uri>.
</p>

<p>
AFS è stato originariamente sviluppato dalla Transarc che ora è di proprietà di
IBM. Si possono trovare maggiori informazioni riguardo AFS sul sito della <uri
link="http://www.transarc.ibm.com/Product/EFS/AFS/index.html">Transarc</uri>.
</p>

</body>
</section>
<section>
<title>Come posso effettuare un debug dei problemi?</title>
<body>

<p>
OpenAFS ha un ottimo supporto al logging. Naturalmente, in modo predefinito, i
messaggi di log vengono memorizzati nei propri file di log invece che nei
normali file log di sistema presenti nella propria macchina. Per redirigere i
messaggi di log nel proprio logger di sistema, utilizzare l'opzione
<c>-syslog</c> per tutti i comandi <c>bos</c>.
</p>

</body>
</section>
</chapter>

<chapter>
<title>Aggiornamento da versioni precedenti</title>
<section>
<title>Introduzione</title>
<body>

<p>
Questa sezione è di aiuto nel processo di aggiornamento da una versione OpenAFS
esistente e già installata ad una versione di OpenAFS versione 1.4.0 o superiore
(necessaria per i molti bug risolti, per il supporto al kernel 2.6 e ai file di
grandi dimensioni).
</p>

<p>
Se si è installata una versione "pulita" di OpenAFS uguale o superiore alla 1.4,
si può tranquillamente saltare questo capitolo. Se, però, si sta aggiornando da
una versione precedente, si raccomanda caldamente di seguire le istruzioni delle
prossime sezioni. Lo script di transizione nell'ebuild è stato creato in modo da
garantire un aggiornamento rapido e, a seguire, un riavvio. Notare che (per
ragioni di sicurezza) tale script non cancella i vecchi file di configurazione,
i vecchi script di avvio e non cambia la configurazione di boot al fine di
utilizzare i nuovi script. Inoltre notare ancora che l'utilizzo di un vecchio
modulo kernel OpenAFS in un sistema con binari aggiornati, causerà quasi
sicuramente un blocco del kernel.
</p>

<note>
Questo capitolo è stato scritto pensando alle differenti configurazioni di
sistema possibili. Tuttavia, è ancora possibile che, a causa di particolari
modifiche che un utente potrebbe aver effettuato, la sua situazione specifica
non sia qui considerata. Un utente con abilità tali da modificare autonomamente
il proprio sistema dovrebbe avere l'esperienza necessaria ad applicare le
modifiche quando necessarie. Viceversa, un utente che abbia semplicemente
installato l'ebuild precedente, può tranquillamente saltare la maggior parte
degli avvertimenti che seguiranno.
</note>

</body>
</section>
<section>
<title>Differenze dalle versioni precedenti</title>
<body>

 <p>
Tradizionalmente, OpenAFS ha sempre usato la stessa convenzione dei laboratori
IBM TransArc per i percorsi (path). Comprensibilmente, le vecchie installazioni
di AFS utilizzano queste convenzioni. Installazioni più recenti sono però
conformi all'FSH nell'utilizzo di locazioni standard (come si vede in molte
distribuzioni Linux). La tabella seguente è un elenco di script di
configurazione e di README che accompagnano un pacchetto OpenAFS:
</p>

<table>
<tr>
  <th>Directory</th>
  <th>Scopo</th>
  <th>Percorso Transarc</th>
  <th>Percorso Predefinito</th>
  <th>in Gentoo</th>
</tr>
<tr>
  <ti>viceetcdir</ti>
  <ti>Configurazione Client</ti>
  <ti>/usr/vice/etc</ti>
  <ti>$(sysconfdir)/openafs</ti>
  <ti>/etc/openafs</ti>
</tr>
<tr>
  <ti>unnamed</ti>
  <ti>Binari Client</ti>
  <ti>non specificato</ti>
  <ti>$(bindir)</ti>
  <ti>/usr/bin</ti>
</tr>
<tr>
  <ti>afsconfdir</ti>
  <ti>Configurazione Server</ti>
  <ti>/usr/afs/etc</ti>
  <ti>$(sysconfdir)/openafs/server</ti>
  <ti>/etc/openafs/server</ti>
</tr>
<tr>
  <ti>afssrvdir</ti>
  <ti>Binari Internal Server</ti>
  <ti>/usr/afs/bin (server)</ti>
  <ti>$(libexecdir)/openafs</ti>
  <ti>/usr/libexec/openafs</ti>
</tr>
<tr>
  <ti>afslocaldir</ti>
  <ti>Stato Server</ti>
  <ti>/usr/afs/local</ti>
  <ti>$(localstatedir)/openafs</ti>
  <ti>/var/lib/openafs</ti>
</tr>
<tr>
  <ti>afsdbdir</ti>
  <ti>Database Auth/serverlist/...</ti>
  <ti>/usr/afs/db</ti>
  <ti>$(localstatedir)/openafs/db</ti>
  <ti>/var/lib/openafs/db</ti>
</tr>
<tr>
  <ti>afslogdir</ti>
  <ti>File di Log</ti>
  <ti>/usr/afs/logs</ti>
  <ti>$(localstatedir)/openafs/logs</ti>
  <ti>/var/lib/openafs/logs</ti>
</tr>
<tr>
  <ti>afsbosconfig</ti>
  <ti>Configurazione Overseer</ti>
  <ti>$(afslocaldir)/BosConfig</ti>
  <ti>$(afsconfdir)/BosConfig</ti>
  <ti>/etc/openafs/BosConfig</ti>
</tr>
</table>

 <p>
Ci sono altre particolarità nei percorsi Transarc, come binari in
<path>/usr/vice/etc</path>, ma questa lista non ha come scopo l'essere
esauriente. Vuole, tuttavia, fare da riferimento per problemi di transizione
dei file di configurazione.
</p>

<p>
Risultato dei cambi di percorsi è che la locazione predefinita della cache di
disco passa da <path>/usr/vice/cache</path> a <path>/var/cache/openafs</path>.
</p>

<p>
Inoltre, gli script di init sono stati separati in client e server. Se prima si
aveva come script di avvio <path>/etc/init.d/afs</path>, ora si avranno
<path>/etc/init.d/openafs-client</path> e
<path>/etc/init.d/openafs-server</path>. Di conseguenza, il file di
configurazione <path>/etc/conf.d/afs</path> è stato diviso in
<path>/etc/conf.d/openafs-client</path> e
<path>/etc/conf.d/openafs-server</path>. Le opzioni in
<path>/etc/conf.d/afs</path> per attivare o disattivate il client o il server
diventano obsolete.
</p>

<p>
Un altro cambiamento relativo allo script di avvio è che ora non viene più
controllato il setup della cache del disco. Il vecchio codice richiedeva che
una partizione separata di tipo ext2 fosse montata in
<path>/usr/vice/cache</path>. Questo creava alcuni problemi:
</p>

<ul>
  <li>
    Nonostante sia una configurazione logica, la propria cache non necessita di
    una partizione separata. Fino a quando si è certi che lo spazio specificato
    in <path>/etc/openafs/cacheinfo</path> sia realmente disponibile per la
    cache del disco, va tutto bene. Quindi non ci sono particolari problemi
    nell'avere la cache sulla partizione di root.
  </li>
  <li>
    Alcuni utilizzano collegamenti simbolici per puntare alla locazione reale
    della cache del disco. Questo non va bene per lo script di avvio, perchè
    questa locazione della cache non viene risolta in <path>/proc/mounts</path>.
  </li>
  <li>
    Molti preferisco ext3 a ext2. Entrambi i filesystem sono validi per
    l'utilizzo della cache del disco. Qualsiasi altro filesystem non è
    supportato (quindi: non provare reiserfs, si va incontro a blocchi certi).
  </li>
</ul>

</body>
</section>
<section>
<title>Passaggio ai nuovi percorsi (path)</title>
<body>

<p>
Prima di tutto, effettuando un emerge di una nuova versione di OpenAFS non si
perderanno i file di configurazione relativi alle vecchie versioni. Lo script è
stato fatto in modo da non modificare nessun file presente sul proprio sistema.
Quindi anche se si ha una configurazione totalmente caotica con un mix di
vecchie e nuove locazioni, lo script non dovrebbe causare nessun tipo di
problema. Inoltre, se dovesse venir rilevato un server OpenAFS in esecuzione,
l'installazione terminerà, evitando possibili corruzioni del database.
</p>

<p>
Un ulteriore avvertimento: in internet circolano alcuni ebuild che disabilitano
parzialmente la protezione che Gentoo pone sulla directory <path>/etc</path>.
Questi ebuild non sono mai stati distribuiti da Gentoo. Per questo motivo si
dovrebbe controllare la variabile <c>CONFIG_PROTECT_MASK</c> con il comando
seguente:
</p>

<pre caption="Controllo della variabile CONFIG_PROTECT_MASK">
# <i>emerge info | grep "CONFIG_PROTECT_MASK</i>
CONFIG_PROTECT_MASK="/etc/gconf /etc/terminfo /etc/texmf/web2c /etc/env.d"
</pre>

<p>
Benchè niente in questo ebuild modifichi file in <path>/etc/afs</path>,
l'aggiormanento provvederà alla rimozione della vecchia installazione di
OpenAFS. I file in <c>CONFIG_PROTECT_MASK</c> appartenenti alla vecchia
installazione saranno rimossi.
</p>

<p>
Dovrebbe essere chiaro agli utenti esperti che, nel caso si fossero aggiunti
manualmente collegamenti simbolici nel proprio sistema (ad esempio da
<path>/usr/afs/etc</path> a <path>/etc/openafs</path>), la nuova installazione
potrebbe ugualmente andare a buon fine anche usando i vecchi file di
configurazione. In questo caso, non ci sarà stata nessuna reale transazione, e
la rimozione della vecchia installazione porterà ad una configurazione di
OpenAFS non funzionante.
</p>

<p>
Venuti a conoscenza di quello che non dovrebbe accadere, vediamo ora cosa ci si
aspetta che accada:
</p>

<ul>
 <li>
   <path>/usr/afs/etc</path> viene copiato in <path>/etc/openafs/server</path>
 </li>
 <li>
   <path>/usr/vice/etc</path> viene copiato in <path>/etc/openafs</path>
 </li>
 <li>
   <path>/usr/afs/local</path> viene copiato in <path>/var/lib/openafs</path>
 </li>
 <li>
   <path>/usr/afs/local/BosConfig</path> viene copiato in
   <path>/etc/openafs/BosConfig</path>, mentre le occorrenze di
   <path>/usr/afs/bin/</path> vengono sostituite con
   <path>/usr/libexec/openafs</path>, <path>/usr/afs/etc</path> con
   <path>/etc/openafs/server</path> e <path>/usr/afs/bin</path>
   (senza / e non come precedentemente) con
   <path>/usr/bin</path>
 </li>
 <li>
   <path>/usr/afs/db</path> viene copiato in <path>/var/lib/openafs/db</path>
 </li>
 <li>
   Il file di configurazione <path>/etc/conf.d/afs</path> viene copiato in
   <path>/etc/conf.d/openafs-client</path>, visto che tutte le vecchie opzioni
   erano destinate ad un utilizzo esclusivamente lato client.
 </li>
</ul>

</body>
</section>
<section>
<title>L'aggiornamento</title>
<body>

<p>
Se si ha un server in esecuzione, lo si dovrebbe bloccare ora.
</p>

<pre caption="Fermare OpenAFS (nel caso si abbia un server)">
# <i>/etc/init.d/afs stop</i>
</pre>

<p>
E ora l'aggiornamento!
</p>

<pre caption="Aggiornamento">
# <i>emerge -u openafs</i>
</pre>

</body>
</section>
<section>
<title>Riavviare OpenAFS</title>
<body>

<p>
Se si avesse ancora un server OpenAFS in esecuzione, ora è d'obbligo bloccarlo.
</p>

<pre caption="Fermare il client OpenAFS dopo l'aggiornamento">
# <i>/etc/init.d/afs stop</i>
</pre>

<p>
Poichè, naturalmente, si deve mantenere al minimo il tempo di non funzionamento,
ora è possibile riavviare il server OpenAFS nella maniera corretta.
</p>

<pre caption="Riavvio del server OpenAFS dopo l'aggiornamento">
# <i>/etc/init.d/openafs-server start</i>
</pre>

<p>
Per controllare che il server sia in esecuzione:
</p>

<pre caption="Controllo dello stato del server OpenAFS">
# <i>/usr/bin/bos status localhost -localauth</i>
</pre>

<p>
Prima di avviare il client OpenAFS, è opportuno controllare la configurazione
della cache attraverso il file <path>/etc/openafs/cacheinfo</path>. Per
riavviare il client OpenAFS:
</p>

<pre caption="Riavvio del client OpenAFS dopo l'aggiornamento">
# <i>/etc/init.d/openafs-client start</i>
</pre>

</body>
</section>
<section>
<title>Pulizia finale</title>
<body>

<p>
Prima di passare alla cancellazione di file obsoleti, assicurarsi che tutto
funzioni alla perfezione e che si siano riavviati correttamente i servizi
(altrimenti, potrebbe essere in esecuzione ancora la vecchia installazione).
</p>

<impo>
È importante verificare che non si stia utilizzando <path>/usr/vice/cache</path>
per la cache del disco quando si cancella <path>/usr/vice</path>!!
</impo>

<p>
Le directory seguenti possono essere rimosse dal sistema:
</p>

<ul>
  <li><path>/etc/afs</path></li>
  <li><path>/usr/vice</path></li>
  <li><path>/usr/afs</path></li>
  <li><path>/usr/afsws</path></li>
</ul>

<p>
Anche i file seguenti non sono più necessari:
</p>

<ul>
  <li><path>/etc/init.d/afs</path></li>
  <li><path>/etc/conf.d/afs</path></li>
</ul>

<pre caption="Cancellazione dei vecchi file">
# <i>tar czf /root/oldafs-backup.tgz /etc/afs /usr/vice /usr/afs /usr/afsws</i>
# <i>rm -R /etc/afs /usr/vice /usr/afs /usr/afsws</i>
# <i>rm /etc/init.d/afs /etc/conf.d/afs</i>
</pre>

<p>
Nel caso si avesse utilizzato precedentemente una delle seguenti due versioni
=openafs-1.2.13 oppure =openafs-1.3.85, si potrebbero avere altri file non
più necessari:
</p>

<ul>
  <li><path>/etc/init.d/afs-client</path></li>
  <li><path>/etc/init.d/afs-server</path></li>
  <li><path>/etc/conf.d/afs-client</path></li>
  <li><path>/etc/conf.d/afs-server</path></li>
</ul>

</body>
</section>
<section>
<title>Cambiamenti nello Script di Avvio</title>
<body>

<p>
A questo punto la maggior parte degli utenti potrebbe volere che il proprio
sistema sia configurato in maniera tale da avviare automaticamente il client e
il server OpenAFS all'avvio. Quelli che non lo desiderano possono saltare questa
sezione. Invece se, precedentemente, il proprio sistema era già configurato per
avviare in automatico i servizi, sarà necessario ora riaggiungerli manualmente,
poichè i nomi degli script di avvio sono cambiati.
</p>

<pre caption="Reinserimento dell'avvio di OpenAFS nella sequenza di boot">
# <i>rc-update del afs default</i>
# <i>rc-update add openafs-client default</i>
# <i>rc-update add openafs-server default</i>
</pre>

<p>
Se si avesse installato <c>=openafs-1.2.13</c> oppure <c>=openafs-1.3.85</c>,
bisognerà rimuovere <path>afs-client</path> e <path>afs-server</path> dal
runlevel default, invece di <path>afs</path>.
</p>

</body>
</section>
<section>
<title>Risoluzione problemi: cosa fare se l'aggiornamento automatico non va a
buon fine</title>
<body>

<p>
Non si dovrebbe avere perso nessun dato e nessun file di configurazione. Quindi
non resta altro da fare che analizzare la situazione. Bisogna effettuare una
segnalazione di bug all'indirizzo <uri
link="http://bugs.gentoo.org">bugs.gentoo.org</uri> con, preferibilmente, quante
più informazioni possibili allegate.
</p>

<p>
Se ci fossero problemi nell'avvio del client, questo dovrebbe aiutare nella
diagnosi del problema:
</p>

<ul>
  <li>
    Digitare da terminale <c>dmesg</c>. Il client normalmente invia eventuali
    messaggi di errore qui.
  </li>
  <li>
    Controllare <path>/etc/openafs/cacheinfo</path>. Dovrebbe essere nella
    forma: /afs:{percorso della cache del disco}:{numero di blocchi per la cache
    del disco}. Normalmente, la propria cache del disco è in
    <path>/var/cache/openafs</path>.
  </li>
  <li>
    Controllare cosa restituisce il comando <c>lsmod</c>. Una delle linee deve
    iniziare con la parola openafs.
  </li>
  <li>
    Il comando <c>pgrep afsd</c> sarà utile per controllare se afsd sia in
    esecuzione o meno.
  </li>
  <li>
    <c>cat /proc/mounts</c> mostra se <path>/afs</path> è stato montato.
  </li>
</ul>

<p>
Se ci fossero problemi nell'avvio del server, questi suggerimenti potrebbero
risultare utili:
</p>

<ul>
  <li>
    <c>pgrep bosserver</c> mostra se l'overseer è in esecuzione o meno. Se si ha
    più di un overseer in esecuzione, c'è sicuramente qualcosa che non va. In
    questo caso, si dovrebbe provare uno shutdown (spegnimento) del server
    OpenAFS con <c>bos shutdown localhost -localauth -wait</c>, controllare il
    risultato con <c>bos status localhost -localauth</c>, chiudere tutti i
    restanti processi overseer e infine controllare se ci sono processi server
    in esecuzione (<c>ls /usr/libexec/openafs</c> per averne una lista). Infine,
    eseguire <c>/etc/init.d/openafs-server zap</c> per resettare lo stato del
    server e <c>/etc/init.d/openafs-server start</c> per provare a lanciarlo
    nuovamente.
  </li>
  <li>
    Se si sta utilizzando il sistema di log interno ad OpenAFS (configurazione
    predefinita), controllare <path>/var/lib/openafs/logs/*</path>. Se, invece,
    si utilizzano i log di sistema, controllare questi ultimi per ottenere
    informazioni utili.
  </li>
</ul>

</body>
</section>
</chapter>

<chapter>
<title>Documentazione</title>
<section>
<title>Consultare la Documentazione AFS</title>
<body>

<p>
Si può facilmente consultare la Documentazione originale AFS di IBM. È scritta
molto bene e, se si deve amministrare un Server AFS, diventa una lettura
obbligata.
</p>

<pre caption="Installazione di afsdoc">
# <i>emerge app-doc/afsdoc</i>
</pre>

<p>
È anche possibile consultare la documentazione allegata ad OpenAFS. Viene
installata con la flag USE <c>doc</c> quando si effettua l'emerge del pacchetto
OpenAFS. Il percorso in cui trovarla è <path>/usr/share/doc/openafs-*/</path>.
Al momento della stesura di questa guida, la documentazione è ancora in pieno
sviluppo. Può, tuttavia, contenere informazioni relative a nuove caratteristiche
di OpenAFS non trattate nella documentazione ufficiale IBM AFS.
</p>

</body>
</section>
</chapter>

<chapter>
<title>Installazione sul Client</title>
<section>
<title>Compilare il Client</title>
<body>

<note>
Tutti i comandi devono essere digitati su una sola linea. Per facilitarne la
lettura, in questo documento alcune volte sono riportati su più linee.
</note>

<pre caption="Installazione di openafs">
# <i>emerge net-fs/openafs</i>
</pre>

<p>
Terminata la compilazione, si è pronti per proseguire.
</p>

</body>
</section>
<section>
<title>Installazione del client per una semplice "consultazione"</title>
<body>

<p>
Se non si fa parte di una specifica cella OpenAFS alla quale si vuole accedere,
e si vogliono semplicemente sfogliare le condivisioni OpenAFS disponibili,
allora si può installare OpenAFS e, senza toccare nessuna configurazione,
avviare <path>/etc/init.d/openafs-client</path>.
</p>

</body>
</section>
<section>
<title>Accedere ad una specifica cella OpenAFS</title>
<body>

<p>
Se si necessita di accedere ad una cella specifica, ad esempio la cella della
propria università o della propria compagnia, bisogna apportare alcune
modifiche.
</p>

<p>
Prima di tutto, è necessario aggiornare <path>/etc/openafs/CellServDB</path> con
i server del database della propria cella. Questa informazione viene solitamente
fornita dal proprio amministratore.
</p>

<p>
In seguito, per accedere alla cella OpenAFS, bisogna specificarne il nome in
<path>/etc/openafs/ThisCell</path>.
</p>

<pre caption="Modifica di CellServDB e ThisCell">
 CellServDB:
 >netlabs        #Cell name
 10.0.0.1        #storage

 ThisCell:
 netlabs
</pre>

<warn>
All'interno del file <path>CellServDB</path> devono essere usati solamente
spazi. Nel caso di uso dei TAB, il client andrà sicuramente incontro a problemi.
</warn>

<p>
Per un avvio rapido, eseguire <path>/etc/init.d/openafs/client</path> seguito
da <c>klog</c> per autenticarsi e cominciare. Per autenticazioni automatiche
alla propria cella, si può leggere la sezione che segue.
</p>

</body>
</section>
<section>
<title>Configurare la cache</title>
<body>

<note>
Sfortunatamente il client AFS richiede un filesystem di tipo ext2/3 per la
propria cache per funzionare correttamente; con reiserfs ci sono problemi.
</note>

<p>
Si può allocare la propria cache su un filesystem esistente (sempre di tipo
ext2/3), ma alcuni potrebbero voler creare una partizione separata a tale scopo.
La locazione predefinita della cache è <path>/var/cache/openafs</path>, ma è
possibile cambiarla modificando <path>/etc/openafs/cacheinfo</path>. La
dimensione predefinita per la propria cache è 200MB, meglio se maggiore.
</p>

</body>
</section>
<section>
<title>Modifica delle configurazioni di accesso alla cella</title>
<body>

<p>
Se si vuole fare altro oltre che sfogliare in sola lettura le celle AFS, si
dovranno modificare i due file CellServDB e ThisCell. Si trovano in
<path>/etc/openafs</path>.
</p>

<pre caption="Modifica di CellServDB e ThisCell">
CellServDB:
&gt;netlabs        #Cell name
10.0.0.1        #storage

ThisCell:
netlabs
</pre>

<warn>
Utilizzare solo spazi all'interno del file <path>CellServDB</path>. L'utilizzo
dei TAB provocherà errori nel client.
</warn>

<p>
CellServDB indica al client quale o quali server ha bisogno di contattare per
una specifica cella. ThisCell dovrebbe sembrare ovvio. Normalmente si utilizza
un nome che è unico per la propria organizzazione. Il proprio dominio
(ufficiale) potrebbe essere una buona scelta.
</p>

</body>
</section>
<section>
<title>Esecuzione di AFS all'avvio</title>
<body>

<p>
I comandi seguenti creano i collegamenti appropriati per avviare il client afs
durante il boot del sistema.
</p>

<warn>
Si dovrebbe sempre avere un server afs in esecuzione all'interno del proprio
dominio quando si avvia un client afs. Se il server AFS non è attivo (o è down),
il sistema non si avvierà e attenderà prima di proseguire, per un tempo
relativamente lungo (fino al timeout).
</warn>

<pre caption="Aggiunta di AFS server al runlevel default">
# <i>rc-update add openafs-server default</i>
</pre>

</body>
</section>
</chapter>

<chapter>
<title>Installazione sul Server</title>
<section>
<title>Compilazione lato Server</title>
<body>

<p>
I comandi seguenti installano tutti i binari necessari alla configurazione di
un Server AFS <e>e</e> di un Client.
</p>

<pre caption="Installazione di openafs">
# <i>emerge net-fs/openafs</i>
</pre>

</body>
</section>
<section>
<title>Avvio del Server AFS</title>
<body>

<p>
È necessario prima di tutto cancellare i file di esempio CellServDB e
ThisCell.
</p>

<pre caption="Rimuovere file di esempio">
# <i>rm /usr/vice/etc/ThisCell</i>
# <i>rm /usr/vice/etc/CellServDB</i>
</pre>

<p>
In seguito si deve eseguire il comando <c>bosserver</c> per inizializzare il
Basic OverSeer (BOS) Server, il quale ha il compito di monitorare e controllare
gli altri processi del server AFS. Si pensi al BOS come ad un init del sistema.
La flag <c>-noauth</c> va inclusa per disabilitare il controllo delle
autorizzazioni, poichè non si è ancora aggiunto l'user admin.
</p>

<warn>
Disabilitando il controllo delle autorizzazioni, la sicurezza del nodo (cell)
viene, temporaneamente, compromessa. Tutti questi sottopassaggi vanno completati
in un unica ininterrotta sessione e il sistema non deve essere lasciato
incustodito fino al successivo riavvio del Server BOS con il controllo delle
autorizzazioni attivato.
</warn>

<pre caption="Inizializzazione del Basic OverSeer Server">
# <i>bosserver -noauth &amp;</i>
</pre>

<p>
Controllare che il Server BOS abbia creato i file
<path>/usr/vice/etc/CellServDB</path> e <path>/usr/vice/etc/ThisCell</path>
</p>

<pre caption="Controllo della presenza di CellServDB e ThisCell">
# <i>ls -al /usr/vice/etc/</i>
-rw-r--r--    1 root     root           41 Jun  4 22:21 CellServDB
-rw-r--r--    1 root     root            7 Jun  4 22:21 ThisCell
</pre>

</body>
</section>
<section>
<title>Assegnazione del Nome del Nodo (cell) e del Gruppo (Membership) per i
Processi del Server</title>
<body>

<p>
A questo punto assegnare il nome al nodo.
</p>

<impo>
Ci sono alcune restrizioni nel formato del nome. Non possono essere usate
lettere in maiuscolo e non si possono superare i 64 caratteri. Il nome del
proprio nodo (cell) apparirà nel percorso <path>/afs</path>, quindi conviene
usare nomi brevi.
</impo>

<note>
D'ora in avanti sostituire alla stringa &lt;nome server&gt; l'hostname completo
(ad esempio <b>afs.gentoo.org</b>) della macchina sulla quale state installando
il tutto. Al posto di &lt;nome cella&gt; va inserito il nome completo del
proprio nodo (ad esempio <b>gentoo</b>).
</note>

<p>
Eseguire il comando <c>bos setcellname</c> per assegnare un nome al nodo:
</p>

<pre caption="Impostazione del nome della cella">
# <i>bos setcellname &lt;nome server&gt; &lt;nome cella&gt; -noauth</i>
</pre>

</body>
</section>
<section>
<title>Avvio del Database Server Process</title>
<body>

<p>
Il comando <c>bos create</c> crea le configurazioni dei quattro processi
database server nel file <path>/etc/openafs/BosConfig</path>. I quattro processi
funzionano solamente sui database server.
</p>

<table>
<tr>
  <ti>kaserver</ti>
  <ti>
    L'Authentication Server gestisce l'Authentication Database.
    Può essere sostituito con un demone Kerberos 5 (in questa guida non
    documentato).
  </ti>
</tr>
<tr>
  <ti>buserver</ti>
  <ti>Il Backup Server gestisce il Backup Database</ti>
</tr>
<tr>
  <ti>ptserver</ti>
  <ti>Il Protection Server gestisce il Protection Database</ti>
</tr>
<tr>
  <ti>vlserver</ti>
  <ti>
    Il Volume Location Server gestisce il Volume Location Database (VLDB).
    È il più importante
  </ti>
</tr>
</table>

<pre caption="Creazione configurazioni dei processi database">
# <i>bos create &lt;nome server&gt; kaserver simple
/usr/libexec/openafs/kaserver -cell &lt;nome cella&gt; -noauth</i>
# <i>bos create &lt;nome server&gt; buserver simple
/usr/libexec/openafs/buserver -cell &lt;nome cella&gt; -noauth</i>
# <i>bos create &lt;nome server&gt; ptserver simple
/usr/libexec/openafs/ptserver -cell &lt;nome cella&gt; -noauth</i>
# <i>bos create &lt;nome server&gt; vlserver simple
/usr/libexec/openafs/vlserver -cell &lt;nome cella&gt; -noauth</i>
</pre>

<p>
È possibile verificare che tutti i server siano in esecuzione con il comando
<c>bos status</c>:
</p>

<pre caption="Controllo del corretto funzionamento dei server">
# <i>bos status &lt;nome server&gt; -noauth</i>
Instance kaserver, currently running normally.
Instance buserver, currently running normally.
Instance ptserver, currently running normally.
Instance vlserver, currently running normally.
</pre>

</body>
</section>
<section>
<title>Sicurezza del Nodo (cell)</title>
<body>

<p>
Verranno ora esaminati i meccanismi di sicurezza del nodo. Si creeranno due
profili iniziali nell'Authentication Database: l'account di amministratore,
chiamato per convenzione <b>admin</b> e un account per i processi del server
AFS, chiamato <c>afs</c>. Nessun utente si può connettere usando l'user
<b>afs</b>, ma il modulo Authentication Server's Ticket Granting Service (TGS)
usa questo account per criptare i ticket del server concessi ai client AFS.
</p>

<p>
Passare alla modalità interattiva <c>kas</c>
</p>

<pre caption="Modalità interattiva">
# <i>kas -cell &lt;nome cella&gt; -noauth</i>
ka&gt; <i>create afs</i>
initial_password:
Verifying, please re-enter initial_password:
ka&gt; <i>create admin</i>
initial_password:
Verifying, please re-enter initial_password:
ka&gt; <i>examine afs</i>

User data for afs
  key (0) cksum is 2651715259, last cpw: Mon Jun  4 20:49:30 2001
  password will never expire.
  An unlimited number of unsuccessful authentications is permitted.
  entry never expires.  Max ticket lifetime 100.00 hours.
  last mod on Mon Jun  4 20:49:30 2001 by &lt;none&gt;
  permit password reuse
ka&gt; <i>setfields admin -flags admin</i>
ka&gt; <i>examine admin</i>

User data for admin (ADMIN)
  key (0) cksum is 2651715259, last cpw: Mon Jun  4 20:49:59 2001
  password will never expire.
  An unlimited number of unsuccessful authentications is permitted.
  entry never expires.  Max ticket lifetime 25.00 hours.
  last mod on Mon Jun  4 20:51:10 2001 by &lt;none&gt;
  permit password reuse
ka&gt;
</pre>

<p>
Il comando <c>bos adduser</c> aggiunge l'utente <b>admin</b> (amministratore) a
<path>/etc/openafs/server/UserList</path>.
</p>

<pre caption="Aggiunta dell'utente admin alla UserList">
# <i>bos adduser &lt;nome server&gt; admin -cell &lt;nome cella&gt; -noauth</i>
</pre>

<p>
Il comando <c>bos addkey</c> definisce la chiave di crittografia (encryption
key) del Server AFS in <path>/etc/openafs/server/KeyFile</path>
</p>

<note>
Nel caso di richiesta di una password ("input key"), sarà necessario digitare
quella usata nella creazione dell'user afs nella modalità interattiva
<c>kas</c>.
</note>

<pre caption="Inserimento password">
# <i>bos addkey  &lt;nome server&gt; -kvno 0 -cell &lt;nome cella&gt; -noauth</i>
input key:
Retype input key:
</pre>

<p>
Il comando <c>pts createuser</c> crea un profilo Protection Database per
l'utente amministratore.
</p>

<note>
il Protection Server assegna in modo predefinito all'utente <b>admin</b> un AFS
UID 1, perchè è il primo profilo creato. Se nel file delle password locale
(<path>/etc/passwd</path> o uno equivalente) è già presente un <b>admin</b> con
UID differente, la flag <c>-id</c> verrà utilizzata per creare UID equivalenti.
</note>

<pre caption="Creazione del profilo Protection Database per l'utente del database">
# <i>pts createuser -name admin -cell &lt;nome cella&gt; [-id &lt;AFS UID&gt;] -noauth</i>
</pre>

<p>
Il comando <c>pts adduser</c> fà dell'utente <b>admin</b> un membro del gruppo
"system:administrators" e il comando <c>pts membership</c> è utile per
verificare che ciò sia avvenuto correttamente.
</p>

<pre caption="Fare amministratore un membro del gruppo administrators e verifica">
# <i>pts adduser admin system:administrators -cell &lt;nome cella&gt; -noauth</i>
# <i>pts membership admin -cell &lt;nome cella&gt; -noauth</i>
Groups admin (id: 1) is a member of:
system:administrators
</pre>

<p>
Riavviare tutti i processi del Server AFS.
</p>

<pre caption="Riavvio di tutti i processi del server AFS">
# <i>bos restart &lt;nome server&gt; -all -cell &lt;nome cella&gt; -noauth</i>
</pre>

</body>
</section>
<section>
<title>Avvio del File Server, Volume Server e Salvager</title>
<body>

<p>
Avviare il processo <c>fs</c>, costituito da File Server, da Volume Server e da
Salvager (processi fileserver, volserver e salvager).
</p>

<pre caption="Avvio del processo fs">
# <i>bos create &lt;nome server&gt; fs fs /usr/libexec/openafs/fileserver /usr/libexec/openafs/volserver /usr/libexec/openafs/salvager -cell &lt;cell
name&gt; -noauth</i>
</pre>

<p>
Controllare che tutti i processi siano in esecuzione
</p>

<pre caption="Controllo della corretta esecuzione di tutti i processi">
# <i>bos status &lt;nome server&gt; -long -noauth</i>
Instance kaserver, (type is simple) currently running normally.
Process last started at Mon Jun  4 21:07:17 2001 (2 proc starts)
Last exit at Mon Jun  4 21:07:17 2001
Command 1 is '/usr/libexec/openafs/kaserver'

Instance buserver, (type is simple) currently running normally.
Process last started at Mon Jun  4 21:07:17 2001 (2 proc starts)
Last exit at Mon Jun  4 21:07:17 2001
Command 1 is '/usr/libexec/openafs/buserver'

Instance ptserver, (type is simple) currently running normally.
Process last started at Mon Jun  4 21:07:17 2001 (2 proc starts)
Last exit at Mon Jun  4 21:07:17 2001
Command 1 is '/usr/libexec/openafs/ptserver'

Instance vlserver, (type is simple) currently running normally.
Process last started at Mon Jun  4 21:07:17 2001 (2 proc starts)
Last exit at Mon Jun  4 21:07:17 2001
Command 1 is '/usr/libexec/openafs/vlserver'

Instance fs, (type is fs) currently running normally.
Auxiliary status is: file server running.
Process last started at Mon Jun  4 21:09:30 2001 (2 proc starts)
Command 1 is '/usr/libexec/openafs/fileserver'
Command 2 is '/usr/libexec/openafs/volserver'
Command 3 is '/usr/libexec/openafs/salvager'
</pre>

<p>
Il passo successivo è in relazione al fatto se si è o meno mai eseguito un file
server AFS nel nodo.
</p>

<p>
Se si sta installando il Server AFS per la prima volta nel nodo, è necessario
creare il primo volume AFS, <b>root.afs</b>
</p>

<note>
Al posto di "nome partizione" va inserito il nome di una partizione AFS Server
della postazione. Per convenzione queste partizioni sono chiamate
<path>/vicepx</path>, dove la x può essere compresa tra a e z.
</note>

<pre caption="Creazione del volume root.afs">
# <i>vos create &lt;nome server&gt; &lt;nome partizione&gt; root.afs -cell &lt;nome cella&gt; -noauth</i>
</pre>

<p>
Se nel nodo sono già presenti postazioni con file server AFS e volumi, verranno
utilizzati i comandi <c>vos sncvldb</c> e <c>vos syncserv</c> per sincronizzare
il VLDB (Volume Location Database) con la stato attuale dei volumi sulla
postazione locale. Tutti i dati necessari saranno copiati sul proprio nuovo
server.
</p>

<p>
Se il comando dovesse restituire il messaggio "partition /vicepa does not exist
on the server" (la partizione /vicepa non esiste sul server), bisognerà
assicurarsi che la partizione sia stata montata prima dell'esecuzione del server
OpenAFS, oppure bisognerà montare la directory e riavviare i processi con il
comando <c>bos restart &lt;nome server&gt; -all -cell &lt;nome cella&gt;
-noauth</c>.
</p>

<pre caption ="Sincronizzazione del VLDB">
# <i>vos syncvldb &lt;nome server&gt; -cell &lt;nome cella&gt; -verbose -noauth</i>
# <i>vos syncserv &lt;nome server&gt; -cell &lt;nome cella&gt; -verbose -noauth</i>
</pre>

</body>
</section>
<section>
<title>Avvio del Server Portion dell'Update Server</title>
<body>

<pre caption="Avvio del server update">
# <i>bos create &lt;nome server&gt;
upserver simple "/usr/libexec/openafs/upserver
-crypt /etc/openafs/server -clear /usr/libexec/openafs"
-cell &lt;nome cella&gt; -noauth</i>
</pre>

</body>
</section>
<section>
<title>Configurazione del Top Level del Filespace AFS</title>
<body>

<p>
C'è bisogno, a questo punto, di configurare le ACL ("access control list",
ovvero "Liste di controllo accesso") in modo che ogni utente possa accedere al
percorso <path>/afs</path>.
</p>

<pre caption="Configurazione delle ACL">
# <i>fs setacl /afs system:anyuser rl</i>
</pre>

<p>
Si deve ora creare il volume root, montarlo in modalità solo lettura su
<path>/afs/&lt;nome cella&gt;</path> e in modalità lettura/scrittura su
<path>/afs/.&lt;nome cella&gt;</path>
</p>

<pre caption="Configurazione del volume root">
# <i>vos create &lt;nome server&gt;&lt;partition name&gt; root.cell</i>
# <i>fs mkmount /afs/&lt;nome cella&gt; root.cell </i>
# <i>fs setacl /afs/&lt;nome cella&gt; system:anyuser rl</i>
# <i>fs mkmount /afs/.&lt;nome cella&gt; root.cell -rw</i>
</pre>

<p>
Ora è tutto pronto! Si dovrebbe avere un file server AFS funzionante all'interno
della propria rete.
</p>

<note>
Per il corretto funzionamento del server AFS è essenziale che tutti gli orologi
di sistema siano sincronizzati. La soluzione più conveniente consiste nella
installazione di un server ntp su di una postazione (ad esempio sul server AFS)
per sincronizzare tutti gli orologi dei client attraverso il client ntp. Questo
può, naturalmente, essere fatto anche su di un client AFS.
</note>

</body>
</section>
</chapter>

<chapter>
<title>Amministrazione di Base</title>
<section>
<title>Avviso</title>
<body>

<p>
OpenAFS è una tecnologia complessa e ampia. Si consiglia la lettura della
documentazione AFS per maggiori e ulteriori informazioni. In questo capitolo
verranno illustrate solo alcune operazioni di amministrazione.
</p>

</body>
</section>
<section>
<title>Configurazione di PAM per l'acquisizione di un Token AFS al Login</title>
<body>

<p>
Per utilizzare AFS è necessario autenticarsi sul Server KA se si sta utilizzando
una implementazione di AFS Kerberos 4, o sul Kerberos 5 KDC se si sta
utilizzando MIT, Heimdal, oppure ShiShi Kerberos 5. Tuttavia per loggarsi ad una
macchina sarà necessario anche un account utente, situato solitamente in
<path>/etc/passwd</path>, NIS, LDAP (OpenLDAP), oppure in un database Hesiod.
PAM permette a Gentoo di legare l'autenticazione AFS e di login all'account
utente.
</p>

<p>
Sarà necessario aggiornare <path>/etc/pam.d/system-auth</path>, utilizzato anche
da altre configurazioni. "use_first_pass" sta ad indicare il controllo prima sul
login dell'utente, e "ignore_root" blocca il controllo sul superuser locale in
modo da permettere il login se AFS o il network non si avviano.
</p>

<pre caption="/etc/pam.d/system-auth">
auth       required     pam_env.so
auth       sufficient   pam_unix.so likeauth nullok
auth       sufficient   pam_afs.so.1 use_first_pass ignore_root
auth       required     pam_deny.so

account    required     pam_unix.so

password   required     pam_cracklib.so retry=3
password   sufficient   pam_unix.so nullok md5 shadow use_authtok
password   required     pam_deny.so

session    required     pam_limits.so
session    required     pam_unix.so
</pre>

<p>
Per permettere a <c>sudo</c> di mantenere i token degli utenti reali e per
evitare agli utenti locali di ottenere un accesso AFS cambiare
<path>/etc/pam.d/su</path> come evidenziato di seguito:
</p>

<pre caption="/etc/pam.d/su">
<comment># Qui, utenti con uid &gt; 100 sono considerati appartenenti ad AFS e utenti con
# uid &lt;= 100 sono ignorati da pam_afs.</comment>
auth       sufficient   pam_afs.so.1 ignore_uid 100

auth       sufficient   pam_rootok.so

<comment># Se si vuole restringere il numero degli utenti ammessi all'utilizzo di 'su',
# creare il file /etc/security/suauth.allow, modificabile solo da root, ed
# aggiungere gli utenti a cui è permesso effettuare 'su' (un utente per ogni
# linea del suddetto file)
#auth       required     pam_listfile.so item=ruser \
#       sense=allow onerr=fail file=/etc/security/suauth.allow

# Decommentando questa linea si permette agli utenti del gruppo wheel
# di eseguire 'su' senza inserire una password.
#auth       sufficient   pam_wheel.so use_uid trust

# In alternativa, si può implementare una lista di utenti che non hanno
# bisogno di fornire una password
#auth       sufficient   pam_listfile.so item=ruser \
#       sense=allow onerr=fail file=/etc/security/suauth.nopass

# Commentando questa linea si permette a qualsiasi utente, anche a quelli
# non presenti nel gruppo wheel, l'esecuzione di 'su'</comment>
auth       required     pam_wheel.so use_uid

auth       required     pam_stack.so service=system-auth

account    required     pam_stack.so service=system-auth

password   required     pam_stack.so service=system-auth

session    required     pam_stack.so service=system-auth
session    optional     pam_xauth.so

<comment># Qui si previene la perdita dell'id token dell'utente reale</comment>
session    optional     pam_afs.so.1 no_unlog
</pre>

</body>
</section>
</chapter>
</guide>
