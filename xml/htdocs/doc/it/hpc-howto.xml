<?xml version='1.0' encoding="UTF-8"?>

<!-- $Header: /var/cvsroot/gentoo/xml/htdocs/doc/it/hpc-howto.xml,v 1.1 2005/04/09 15:29:42 so Exp $ -->

<!DOCTYPE guide SYSTEM "/dtd/guide.dtd">
<guide link="/doc/it/hpc-howto.xml" lang="it">

<title>High Performance Computing on Gentoo Linux</title>

<author title="Autore">
  <mail link="marc@adelielinux.com">Marc St-Pierre</mail>
</author>
<author title="Autore">
  <mail link="benoit@adelielinux.com">Benoit Morin</mail>
</author>
<author title="Assistente/Ricercatore">
  <mail link="jean-francois@adelielinux.com">Jean-Francois Richard</mail>
</author>
<author title="Assistente/Ricercatore">
  <mail link="olivier@adelielinux.com">Olivier Crete</mail>
</author>
<author title="Revisore">
  <mail link="spyderous@gentoo.org">Donnie Berkholz</mail>
</author>
<author title="Traduttore">
  <mail link="luca_guglielmetti@ticino.edu">Luca Guglielmetti</mail>
</author>

<!-- No licensing information; this document has been written by a third-party
     organisation without additional licensing information.

     In other words, this is copyright adelielinux R&D; Gentoo only has
     permission to distribute this document as-is and update it when appropriate
     as long as the adelie linux R&D notice stays
-->

<abstract>
Questo documento è stato scritto dagli sviluppatori dell'Adelie Linux R&amp;D
Center &lt;http://www.adelielinux.com&gt; come guida all'installazione di Gentoo
Linux in un sistema di computing ad alta performance (HPC).
</abstract>

<version>1.0</version>
<date>2003-08-01</date>

<chapter>
<title>Introduzione</title>
<section>
<body>

<p>
Gentoo Linux è una speciale distribuzione di Linux che può essere facilmente
ottimizzata e personalizzata per qualsiasi applicazione o necessità. L'estrema
velocità, la grande configurabilità e l'ottima collaborazione fra gli sviluppatori
e gli utenti sono i grandi punti di forza di Gentoo.
</p>

<p>
Grazie a portage, Gentoo Linux può diventare un server sicuro, una workstation
per lo sviluppo, una postazione desktop professionale, una piattaforma di gioco,
un sistema embedded o - come spiega questa guida - un sistema di computing ad
alta performance. Le varie personalizzazioni di Gentoo Linux sono
quasi infinite, perciò il sistema viene anche definito una metadistribuzione.
</p>

<p>
Questa guida spiega come installare un sistema basato su Gentoo in un sistema di
computing ad alta performance. Passo dopo passo spiega quali pacchetti sono necessari
e come configurarli correttamente.
</p>

<p>
Se non sei ancora in possesso di Gentoo Linux puoi procurartene una copia dal sito web
<uri>http://www.gentoo.org</uri> e puoi aiutarti durante l'installazione con la
<uri link="/doc/it/">documentazione</uri>.
</p>

</body>
</section>
</chapter>

<chapter>
<title>Configurazione di Gentoo Linux per un Cluster</title>
<section>
<title>Ottimizzazîoni raccomandate</title>
<body>

<note>
In questa sezione ci sono riferimenti al
<uri link="/doc/it/handbook/">manuale di Gentoo Linux</uri>
</note>

<p>
Durante l'installazione di Gentoo bisogna attivare alcune variabili USE in
<path>/etc/make.conf</path>. Noi raccomandiamo di disattivare tutte le variabili
predefinite (confronta <path>/etc/make.profile/make.defaults</path>) mettendo un
segno meno in <path>/etc/make.conf</path>. Malgrado questo potreste avere bisogno di
attivare USE variabili come x86, 3dnow, gpm, mmx, sse, ncurses, pam e tcpd. Per
maggiori informazioni vi consigliamo perciò la documentazione sulle USE flags.
</p>

<pre caption="USE Flags">
# Copyright 2000-2003 Daniel Robbins, Gentoo Technologies, Inc.
# Contains local system settings for Portage system

# Please review 'man make.conf' for more information.

USE="-oss 3dnow -apm -arts -avi -berkdb -crypt -cups -encode -gdbm
-gif gpm -gtk -imlib -java -jpeg -kde -gnome -libg++ -libwww -mikmod
mmx -motif -mpeg ncurses -nls -oggvorbis -opengl pam -pdflib -png
-python -qt -qtmt -quicktime -readline -sdl -slang -spell -ssl
-svga tcpd -truetype -X -xml2 -xmms -xv -zlib"
</pre>

<p>
o più semplicemente
</p>

<pre caption="USE Flags - versione semplificata">
# Copyright 2000-2003 Daniel Robbins, Gentoo Technologies, Inc.
# Contains local system settings for Portage system

# Please review 'man make.conf' for more information.

USE="-* 3dnow gpm mmx ncurses pam sse tcpd"
</pre>

<note>
La FLAG <e>tcpd</e> aumenta la sicurezza di pacchetti come xinetd.
</note>

<p>
Nelle sezioni 7 e 9 (Configurazione del Kernel e Installazione degli strumenti di
sistema) per ragioni di stabilità raccomandiamo l'installazione di vanilla-sources,
la versione ufficiale del kernel (quella di <uri>http://www.kernel.org/</uri>),
anche se necessitate di feautures speciali, come xfs.
</p>

<pre caption="Installazione di vanilla-sources">
# <i>emerge -p syslog-ng vanilla-sources</i>
</pre>

<p>
Quando in seguito installerete altri pacchetti vi consigliamo di emergere pure questi:
</p>

<pre caption="Installazione di pacchetti necessari">
# <i>emerge -p nfs-utils portmap tcpdump ssmtp iptables xinetd</i>
</pre>

</body>
</section>
<section>
<title>Communication Layer (TCP/IP Network)</title>
<body>

<p>
Un cluster necessita di un communication layer ("strato per le comunicazione") per
connettere i nodi principali ai nodi secondari. Normalmente delle schede di rete come
FastEthernet o GigaEthernet hanno un buon rapporto qualità/prezzo. Un'altra
possibilità è l'uso di prodotti come
<uri link="http://www.myricom.com/">Myrinet</uri>,
<uri link="http://quadrics.com/">QsNet</uri> o altri ancora.
</p>

<p>
Un cluster è composto da due tipi di nodi: master (primari) o slave (secondari).
Normalmente da un solo nodo master e da molti nodi slaves.
</p>

<p>
Il nodo master è il server del cluster e il suo compito è di coordinare i nodi slaves
dicendogli che cosa fare. Normalmente ha quindi dei demoni come dhcpd, nfs, pbs
server, e pbs-sched. Il nodo master deve quindi garantire sessioni interattive e
accettare esecuzioni di processi.
</p>

<p>
I nodi slaves invece attendono di ricevere istruzioni (ad esempio via ssh/rsh) dal
nodo master. Il loro unico compito è di elaborare le istruzioni ricevute, quindi non
dovrebbero avere installati servizi inutili.
</p>

<p>
Ogni nodo dovrebbe avere nel file host (<path>/etc/hosts</path>) gli indirizzi di
tutti i nodi del cluster.
</p>

<pre caption="/etc/hosts">
# Adelie Linux Research &amp; Development Center
# /etc/hosts

127.0.0.1	localhost

192.168.1.100	master.adelie master

192.168.1.1	node01.adelie node01
192.168.1.2	node02.adelie node02
</pre>

<p>
Per configurare la LAN dedicata del vostro cluster editate il file
<path>/etc/conf.d/net</path> del server.
</p>

<pre caption="/etc/conf.d/net">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License, v2 or later

# Global config file for net.* rc-scripts

# This is basically the ifconfig argument without the ifconfig $iface
#

iface_eth0="192.168.1.100 broadcast 192.168.1.255 netmask 255.255.255.0"
# Network Connection to the outside world using dhcp -- configure as required for you network
iface_eth1="dhcp"
</pre>


<p>
Infine configurate il demone DHCP del vostro server, in maniera da poter evitare
di dover mantenere manualmente la configurazione della rete su ogni nodo slave.
</p>

<pre caption="/etc/dhcp/dhcpd.conf">
# Adelie Linux Research &amp; Development Center
# /etc/dhcp/dhcpd.conf

log-facility local7;
ddns-update-style none;
use-host-decl-names on;

subnet 192.168.1.0 netmask 255.255.255.0 {
        option domain-name "adelie";
        range 192.168.1.10 192.168.1.99;
        option routers 192.168.1.100;

        host node01.adelie {
		# MAC address of network card on node 01
                hardware ethernet 00:07:e9:0f:e2:d4;
                fixed-address 192.168.1.1;
        }
        host node02.adelie {
		# MAC address of network card on node 02
                hardware ethernet 00:07:e9:0f:e2:6b;
                fixed-address 192.168.1.2;
        }
}
</pre>

</body>
</section>
<section>
<title>NFS/NIS</title>
<body>

<p>
Il Network File System (NFS) è stato sviluppato per permettere a delle macchine di
montare una partizione del disco su una macchina in rete, come se fosse realmente un
suo disco. Ciò permette quindi di condividere velocemente e simultaneamente dei files
nella rete.
</p>

<p>
Ci sono anche altri sistemi che hanno funzionalità simili a NFS e che possono essere
usati in un cluster. Il
<uri link="http://www.transarc.com/Product/EFS/AFS/index.html">
File System Andrew dell' IBM</uri>, recentemente rilasciato open-source, ha un
meccanismo di file-sharing con alcuni miglioramenti per la sicurezza e le prestazioni.
Il <uri link="http://www.coda.cs.cmu.edu/">File System Coda</uri> è ancora in fase di
sviluppo, ma è stato progettato per lavorare ottimamente con clients disconnessi.
Molte delle feautures del file system Coda saranno probabilmente introdotte nella
prossima release di <uri link="http://www.nfsv4.org">NFS (Version 4)</uri>.
Attualmente NFS ha i vantaggi di essere maturo, standardizzato, molto conosciuto e
supportato molto bene da diverse piattaforme.
</p>

<pre caption="Ebuilds per il supporto di NFS">
# <i>emerge -p nfs-utils portmap</i>
# <i>emerge nfs-utils portmap</i>
</pre>

<p>
Configura e installa un kernel che supporti NFS v3 in tutti i nodi:
</p>

<pre caption="Configurazione del kernel per il supporto di NFS">
CONFIG_NFS_FS=y
CONFIG_NFSD=y
CONFIG_SUNRPC=y
CONFIG_LOCKD=y
CONFIG_NFSD_V3=y
CONFIG_LOCKD_V4=y
</pre>

<p>
Nel server modifica il file <path>/etc/hosts.allow</path> per accettare le
connessioni dai nodi slaves. Se la LAN del tuo cluster è su 192.168.1.0/24 il file
<path>hosts.allow</path> dovrebbe essere simile a questo:
</p>

<pre caption="hosts.allow">
portmap:192.168.1.0/255.255.255.0
</pre>

<p>
Modifica il file <path>/etc/exports</path> del server per esportare una directory di
lavoro (/home va molto bene in questo caso).
</p>

<pre caption="/etc/exports">
/home/	*(rw)
</pre>

<p>
Aggiungi nfs al runlevel come default sul server:
</p>

<pre caption="Aggiunta di NFS al runlevel">
# <i>rc-update add nfs default</i>
</pre>

<p>
Per montare sui nodi slaves il filesystem nfs del nodo master dovete modificare
il file <path>/etc/fstab</path> dei vostri nodi slaves. Dovete quindi aggiungere
una linea simile a questa:
</p>

<pre caption="/etc/fstab">
master:/home/	/home	nfs	rw,exec,noauto,nouser,async	0 0
</pre>

<p>
Dovete anche fare in modo che i nodi slaves montino il filesystem ntfs:
</p>

<pre caption="Aggiunta di nfsmount al runlevel">
# <i>rc-update add nfsmount default</i>
</pre>

</body>
</section>
<section>
<title>RSH/SSH</title>
<body>

<p>
SSH è un protocollo per effettuare login remoti e altre operazioni in una rete
insicura, ma maniera sicura. Infatti OpenSSH usa la crittografia a chiave pubblica per
garantire un'autenticazione e una comunicazione sicura. Prima di configurare ssh
bisogna generare una chiave pubblica, che sarà poi condivisa con il sistema remoto,
e una chiave privata che sarà conosciuta solo dal sistema locale.
</p>

<p>
Per un uso pulito di ssh devono essere usate le chiavi pubbliche/private. Ciò va
fatto con due passaggi:
</p>

<ul>
  <li>Generare la chiave pubblica e quella privata</li>
  <li>Copiare la chiave pubblica su tutti i nodi slaves</li>
</ul>

<p>
Per un'autenticazione basata sugli utenti, bisogna eseguire qualcosa del genere:
</p>

<pre caption="Autorizzazione con chiavi">
# <i>ssh-keygen -t dsa</i>
Generating public/private dsa key pair.
Enter file in which to save the key (/root/.ssh/id_dsa): /root/.ssh/id_dsa
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_dsa.
Your public key has been saved in /root/.ssh/id_dsa.pub.
The key fingerprint is:
f1:45:15:40:fd:3c:2d:f7:9f:ea:55:df:76:2f:a4:1f root@master

<comment>
ATTENZIONE! Se hai già un file "authorized_keys" usa quello, non dare il seguente
comando.
</comment>

# <i>scp /root/.ssh/id_dsa.pub node01:/root/.ssh/authorized_keys</i>
root@master's password:
id_dsa.pub   100%  234     2.0MB/s   00:00

# <i>scp /root/.ssh/id_dsa.pub node02:/root/.ssh/authorized_keys</i>
root@master's password:
id_dsa.pub   100%  234     2.0MB/s   00:00
</pre>

<note>
Host keys deve avere una password vuota. RSA è richiesto solamente per
l'autenticazione basata sugli hosts.
</note>

<p>
Per l'autenticazione basata sugli hosts devi modificare il file
<path>/etc/ssh/shosts.equiv</path>.

</p>

<pre caption="/etc/ssh/shosts.equiv">
node01.adelie
node02.adelie
master.adelie
</pre>

<p>
Sono necessarie anche alcune modifiche al file <path>/etc/ssh/sshd_config</path>.
</p>

<pre caption="Configurazione di sshd">
# $OpenBSD: sshd_config,v 1.42 2001/09/20 20:57:51 mouring Exp $
# This sshd was compiled with PATH=/usr/bin:/bin:/usr/sbin:/sbin

# This is the sshd server system-wide configuration file.  See sshd(8)
# for more information.

# HostKeys for protocol version 2
HostKey /etc/ssh/ssh_host_rsa_key
</pre>

<p>
Se la tua applicazione richiede comunicazioni via RSH è necessario emergere
net-misc/netkit-rsh e sys-apps/xinetd.
</p>

<pre caption="Installazione dei programmi necessari">
# <i>emerge -p xinetd</i>
# <i>emerge xinetd</i>
# <i>emerge -p netkit-rsh</i>
# <i>emerge netkit-rsh</i>
</pre>

<p>
Adesso configura il demone rsh editando il file <path>/etc/xinet.d/rsh</path>.
</p>

<pre caption="rsh">
# Adelie Linux Research &amp; Development Center
# /etc/xinetd.d/rsh

service shell
{
        socket_type     = stream
        protocol        = tcp
        wait            = no
        user            = root
        group           = tty
        server          = /usr/sbin/in.rshd
        log_type        = FILE /var/log/rsh
        log_on_success  = PID HOST USERID EXIT DURATION
        log_on_failure  = USERID ATTEMPT
        disable         = no
}
</pre>

<p>
Modifica anche il file <path>/etc/hosts.allow</path> per autorizzare le connessioni rsh.
</p>

<pre caption="hosts.allow">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.allow

in.rshd:192.168.1.0/255.255.255.0
</pre>

<p>
O potete semplicemente avere fiducia nella vostra LAN:
</p>

<pre caption="hosts.allow">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.allow     

ALL:192.168.1.0/255.255.255.0
</pre>

<p>
Per finire configura l'autenticazione degli host editando il file <path>/etc/hosts.equiv</path>.
</p>

<pre caption="hosts.equiv">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.equiv

master
node01
node02
</pre>

<p>
E imposta il runlevel di xinetd su default:
</p>

<pre caption="Aggiunta di xinetd al runlevel">
# <i>rc-update add xinetd default</i>
</pre>

</body>
</section>
<section>
<title>NTP</title>
<body>

<p>
Il Network Time Protocol (NTP) è usato per sincronizzare l'ora di un computer
(client o server) con quella di un altro server o di un altra macchina che ha
un orologio, ad esempio una radio, un ricevitore satellitare o un modem.
Generalmente la precisione in una rete LAN è di un millisecondo e in una rete
WAN è di una decina di millisecondi utilizzando ad esempio UTC (Coordinated Universal
Time) con un GPS (Global Positioning Service). Solitamente le configurazioni
di NTP utilizzano molti servers ridondanti e diversi percorsi della rete per
raggiungere un'ottima precisione e un'ottima attendibilità.
</p>

<p>
Scegli un server NTP geograficamente abbastanza vicino a te da
<uri link="http://www.eecis.udel.edu/~mills/ntp/servers.html"> questa lista di
servers NTP pubblici</uri> e configura i tuoi files <path>/etc/conf.d/ntp</path> e
<path>/etc/ntp.conf</path> del tuo nodo master.
</p>

<pre caption="/etc/conf.d/ntp (del nodo master)">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License v2
# /etc/conf.d/ntpd

# NOTES:
#  - NTPDATE variables below are used if you wish to set your
#    clock when you start the ntp init.d script
#  - make sure that the NTPDATE_CMD will close by itself ...
#    the init.d script will not attempt to kill/stop it
#  - ntpd will be used to maintain synchronization with a time
#    server regardless of what NTPDATE is set to
#  - read each of the comments above each of the variable

# Comment this out if you dont want the init script to warn
# about not having ntpdate setup
NTPDATE_WARN="n"

# Command to run to set the clock initially
# Most people should just uncomment this line ...
# however, if you know what you're doing, and you
# want to use ntpd to set the clock, change this to 'ntpd'
NTPDATE_CMD="ntpdate"

# Options to pass to the above command
# Most people should just uncomment this variable and
# change 'someserver' to a valid hostname which you
# can aquire from the URL's below
NTPDATE_OPTS="-b ntp1.cmc.ec.gc.ca"

##
# A list of available servers is available here:
# http://www.eecis.udel.edu/~mills/ntp/servers.html
# Please follow the rules of engagement and use a
# Stratum 2 server (unless you qualify for Stratum 1)
##

# Options to pass to the ntpd process that will *always* be run
# Most people should not uncomment this line ...
# however, if you know what you're doing, feel free to tweak
#NTPD_OPTS=""

</pre>

<p>
Modifica il file <path>/etc/ntp.conf</path> del nodo master in maniera
da impostare una fonte esterna per la sincronizzazione:
</p>

<pre caption="ntp.conf (del nodo master)">
# Adelie Linux Research &amp; Development Center
# /etc/ntp.conf

# Synchronization source #1
server ntp1.cmc.ec.gc.ca
restrict ntp1.cmc.ec.gc.ca
# Synchronization source #2
server ntp2.cmc.ec.gc.ca
restrict ntp2.cmc.ec.gc.ca
stratum 10
driftfile /etc/ntp.drift.server
logfile  /var/log/ntp
broadcast 192.168.1.255
restrict default kod
restrict 127.0.0.1
restrict 192.168.1.0 mask 255.255.255.0
</pre>

<p>
E imposta in tutti i nodi slaves, come fonte per la sincronizzazione, il tuo nodo master.
</p>

<pre caption="/etc/conf.d/ntp">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License v2
# /etc/conf.d/ntpd

NTPDATE_WARN="n"
NTPDATE_CMD="ntpdate"
NTPDATE_OPTS="-b master"
</pre>

<pre caption="ntp.conf">
# Adelie Linux Research &amp; Development Center
# /etc/ntp.conf

# Synchronization source #1
server master
restrict master
stratum 11
driftfile /etc/ntp.drift.server
logfile  /var/log/ntp
restrict default kod
restrict 127.0.0.1
</pre>

<p>
Per finire aggiungi ntpd al runlevel di tutti i nodi:
</p>

<pre caption="Aggiunta di ntpd al runlevel">
# <i>rc-update add ntpd default</i>
</pre>

<note>
Ora quando la differenza fra l'orologio della fonte e quello locale sarà troppo grande
NTP sincronizzerà l'orologio locale.
</note>

</body>
</section>
<section>
<title>IPTABLES</title>
<body>

<p>
Per configurare un firewall sul vostro cluster dovete installare iptables.
</p>

<pre caption="Installazione di iptables">
# <i>emerge -p iptables</i>
# <i>emerge iptables</i>
</pre>

<p>
Configurazione del kernel richiesta:
</p>

<pre caption="Configurazione del kernel per iptables">
CONFIG_NETFILTER=y
CONFIG_IP_NF_CONNTRACK=y
CONFIG_IP_NF_IPTABLES=y
CONFIG_IP_NF_MATCH_STATE=y
CONFIG_IP_NF_FILTER=y
CONFIG_IP_NF_TARGET_REJECT=y
CONFIG_IP_NF_NAT=y
CONFIG_IP_NF_NAT_NEEDED=y
CONFIG_IP_NF_TARGET_MASQUERADE=y
CONFIG_IP_NF_TARGET_LOG=y
</pre>

<p>
Le regole che questo tipo di firewall necessita sono:
</p>

<pre caption="rule-save">
# Adelie Linux Research &amp; Development Center
# /var/lib/iptbles/rule-save

*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -s 192.168.1.0/255.255.255.0 -i eth1 -j ACCEPT
-A INPUT -s 127.0.0.1 -i lo -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -j LOG
-A INPUT -j REJECT --reject-with icmp-port-unreachable
COMMIT
*nat
:PREROUTING ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A POSTROUTING -s 192.168.1.0/255.255.255.0 -j MASQUERADE
COMMIT
</pre>

<p>
Infine aggiungi anche iptables al runlevel di tutti i nodi:
</p>

<pre caption="Aggiunta di iptables al runlevel">
# <i>rc-update add iptables default</i>
</pre>

</body>
</section>
</chapter>

<chapter>
<title>HPC Tools</title>
<section>
<title>OpenPBS</title>
<body>

<p>
Il Portable Batch System (PBS) è un sistema flessibile per la gestione delle code di
batch; originariamente è stato sviluppato dalla NASA. Funziona bene con networked e
sistemi UNIX multi-piattaforma, come clusters, workstations, supercomputers e molti
sistemi paralleli. Attualmente è la Altair Grid Technologies che continua lo sviluppo
di PBS.
</p>

<pre caption="Installazione di openpbs">
# <i>emerge -p openpbs</i>
</pre>

<note>
L'ebuild di OpenPBS al momento non imposta correttamente i permessi nelle directories
var usate da OpenPBS.
</note>

<p>
Prima di iniziare a usare OpenPBS sono necessarie alcune configurazioni particolari.
I files da personalizzare sono:
</p>

<ul>
	<li>/etc/pbs_environment</li>
	<li>/var/spool/PBS/server_name</li>
	<li>/var/spool/PBS/server_priv/nodes</li>
	<li>/var/spool/PBS/mom_priv/config</li>
	<li>/var/spool/PBS/sched_priv/sched_config</li>
</ul>

<p>
Questo è un esempio di sched_config:
</p>

<pre caption="/var/spool/PBS/sched_priv/sched_config">
#
# Create queues and set their attributes.
#
#
# Create and define queue upto4nodes
#
create queue upto4nodes
set queue upto4nodes queue_type = Execution
set queue upto4nodes Priority = 100
set queue upto4nodes resources_max.nodect = 4
set queue upto4nodes resources_min.nodect = 1
set queue upto4nodes enabled = True
set queue upto4nodes started = True
#
# Create and define queue default
#
create queue default
set queue default queue_type = Route
set queue default route_destinations = upto4nodes
set queue default enabled = True
set queue default started = True
#
# Set server attributes.
#
set server scheduling = True
set server acl_host_enable = True
set server default_queue = default
set server log_events = 511
set server mail_from = adm
set server query_other_jobs = True
set server resources_default.neednodes = 1
set server resources_default.nodect = 1
set server resources_default.nodes = 1
set server scheduler_iteration = 60
</pre>

<p>
Per mandare un processo (job) a OpenPBS, il comando <c>qsub</c> è usato con alcuni
parametri opzionali. Ad esempio "-l" permette di specificare le richieste necessarie,
"-j" permette il redirect degli errori e degli standard out e "-m" manda un e-mail
all'inizio (b), alla fine (e) e all'interruzione (a) del lavoro.
</p>

<pre caption="Mandare un'istruzione">
<comment>
manda la richiesta a OpenpPBS di eseguire myscript in 2 nodi
</comment>
# <i>qsub -l nodes=2 -j oe -m abe myscript</i>
</pre>

<p>
Normalmente le istruzioni sono mandate a OpenPBS sotto forma di scripts. Se qualche
volta aveste bisogno di eseguire delle operazioni manualmente potete richiedere una
shell interattiva a OpenPBS, con il parametro "-I".
</p>

<pre caption="Richesta di una shell interattiva">
# <i>qsub -I</i>
</pre>

<p>
Per vedere lo stato dei vostri jobs potete usare il comando:
</p>

<pre caption="Controllo dello stato dei jobs">
# <i>qstat</i>
Job id  Name  User   Time Use S Queue
------  ----  ----   -------- - -----
2.geist STDIN adelie 0        R upto1nodes
</pre>

</body>
</section>
<section>
<title>MPICH</title>
<body>

<p>
Message passing (scambio di messaggi) è una libreria usata frequentemente in certi tipi
di parallel machines, specialmente nelle architetture a memoria distribuita.  MPICH è
un'implementazione di MPI (lo standard per lo scambio di messaggi) distribuita liberamente.
</p>

<p>
L'ebuild di mpich scritto da Adelie Linux ha due USE flags: <e>doc</e> e <e>crypt</e>.  <e>doc</e> installa anche la documentazione, mentre <e>crypt</e> configura MPICH in
modo da usare <c>ssh</c> invece di <c>rsh</c>.
</p>

<pre caption="Installazione di mpich">
# <i>emerge -p mpich</i>
# <i>emerge mpich</i>
</pre>

<p>
Potreste avere la necessità di esportare una directory di lavoro per mpich in tutti
i vostri nodi slaves in <path>/etc/exports</path>:
</p>

<pre caption="/etc/exports">
/home	*(rw)
</pre>

<p>
Most massively parallel processors (MPPs) mette a disposizione un modo per fare
partire un programma su un determinato numero di processori; quando è possibile
<c>mpirun</c> usa questo comando.
Al contrario, i clusters di workstations necessitano che ogni processo in un
parallel job sia fatto partire individualmente, malgrado esistano programmi
che aiutano a fare partire questi processi. Ad ogni modo per usare i cluster
di workstations bisogna avere ancora alcune informazioni, perchè non sono
organizzati come un MPP.
Mpich dovrebbe essere installato con una lista di workstations nel file
<path>machines.LINUX</path> nella directory <path>/usr/share/MPICH/</path>.
Questo file è usato da <c>mpirun</c> per scegliere il processore su qui fare
partire il processo.
</p>

<p>
Modifica questo file per specificare le macchine del tuo cluster:
</p>

<pre caption="/usr/share/mpich/machines.LINUX">
# Change this file to contain the machines that you want to use
# to run MPI jobs on.  The format is one host name per line, with either
#    hostname
# or
#    hostname:n
# where n is the number of processors in an SMP.  The hostname should
# be the same as the result from the command "hostname"
master
node01
node02
# node03
# node04
# ...
</pre>

<p>
Puoi usare lo script <c>tstmachines</c> (in <path>/usr/sbin</path>) per
assicurarti che puoi fare uso correttamente di tutte le macchine specificate
precedentemente. Lo script chiama <c>rsh</c> e fa una breve lista della
directory; se il test funziona vuole dire che l'accesso al nodo funziona
correttamente e che la directory corrente è visibile nel nodo remoto. Se ci
sono dei problemi il test darà degli errori, e sarà necessario rimediarvi
prima di procedere con le prossime istruzioni.
</p>

<p>
L'unico argomento di <c>tstmachines</c> è il nome dell'architettura, che è lo
stesso nome dell'estensione del file machines. L'esempio seguente testa che il
programma nella directory corrente può essere eseguito da tutte le macchine
della lista LINUX.
</p>

<pre caption="Esecuzione del test">
# <i>/usr/local/mpich/sbin/tstmachines LINUX</i>
</pre>

<note>
Se il test va a buon fine non ci sarà nessun optput; se si vuole vedere come
procede il test si può usare l'argomento -v (verbose):
</note>

<pre caption="Esecuzione del test con l'opzione verbose">
# <i>/usr/local/mpich/sbin/tstmachines -v LINUX</i>
</pre>

<p>
L'output di questo comando sarà qualcosa del genere:
</p>

<pre caption="Output del comando precedente">
Trying true on host1.uoffoo.edu ...
Trying true on host2.uoffoo.edu ...
Trying ls on host1.uoffoo.edu ...
Trying ls on host2.uoffoo.edu ...
Trying user program on host1.uoffoo.edu ...
Trying user program on host2.uoffoo.edu ...
</pre>

<p>
Se <c>tstmachines</c> trova un problema suggerisce delle possibili ragioni e
soluzioni. Si possono vedere ad esempio questi test seguenti:
</p>

<ul>
	<li>
	 <e>Si possono fare partire dei processi nelle macchine remote?</e>tstmachines
	 lancia il comando "true" su ogni macchina del file machines usando la shell
	 remota.
  </li>
	<li>
	 <e>La directory di lavoro corrente è disponibile correttamente per tutte
	 le macchine?</e> Consiste nel listare il file che tstmachines crea lanciando
	 ls tramite la shell remota.
  </li>
	<li>
	 <e>I programmi degli utenti funzionano correttamente nel sistema remoto?</e>
	 Controlla che le shared libraries e tutti i componenti necessari siano stati
	 correttamente installati in tutte le macchine.
  </li>
</ul>

<p>
Anche questo test è necessario per tutti i tools di sviluppo:
</p>

<pre caption="Test di un tool di sviluppo">
# <i>cd ~</i>
# <i>cp /usr/share/mpich/examples1/hello++.c ~</i>
# <i>make hello++</i>
# <i>mpirun -machinefile /usr/share/mpich/machines.LINUX -np 1 hello++</i>
</pre>

<p>
Per ulteriori informazioni su MPICH consulta la documentazione sul sito
<uri>http://www-unix.mcs.anl.gov/mpi/mpich/docs/mpichman-chp4/mpichman-chp4.htm</uri>
</p>

</body>
</section>
<section>
<title>LAM</title>
<body>

<p>
(in arrivo)
</p>

</body>
</section>
<section>
<title>OMNI</title>
<body>

<p>
(in arrivo)
</p>

</body>
</section>
</chapter>

<chapter>
<title>Bibliografia</title>
<section>
<body>

<p>
Il documento originale (in inglese) è pubblicato sul sito<uri 
link="http://www.adelielinux.com">Adelie Linux R&amp;D Centre</uri>, 
ed è stato pubblicato qui grazie al permesso degli autori e del
<uri link="http://www.cyberlogic.ca">Cyberlogic</uri>'s Adelie Linux R&amp;D
Centre.
</p>

<ul>
	<li>
	  <uri>http://www.gentoo.org</uri>, Gentoo Technologies, Inc.
	</li>
	<li>
    <uri>http://www.adelielinux.com</uri>, 
    Adelie Linux Research and Development Centre
   </li>
	<li>
    <uri>http://nfs.sourceforge.net</uri>, 
    Linux NFS Project
   </li>
	<li>
    <uri>http://www-unix.mcs.anl.gov/mpi/mpich/</uri>, 
    Mathematics and Computer Science Division, Argonne National Laboratory
   </li>
	<li>
    <uri>http://ntp.org</uri>
   </li>
	<li>
    <uri>http://www.eecis.udel.edu/~mills/</uri>, 
    David L. Mills, University of Delaware
   </li>
	<li>
    <uri>http://www.ietf.org/html.charters/secsh-charter.html</uri>, 
    Secure Shell Working Group, IETF, Internet Society
   </li>
	<li>
    <uri>http://www.linuxsecurity.com/</uri>,
    Guardian Digital
   </li>
	<li>
    <uri>http://www.openpbs.org/</uri>
    Altair Grid Technologies, LLC.
  </li>
</ul>

</body>
</section>
</chapter>
	
</guide>
