<?xml version='1.0' encoding="UTF-8"?>
<!DOCTYPE guide SYSTEM "/dtd/guide.dtd">
<!-- $Header: /var/cvsroot/gentoo/xml/htdocs/doc/it/multipath.xml,v 1.1 2009/01/08 22:27:47 scen Exp $ -->

<guide lang="it">
<title>Multipathing in Gentoo</title>

<author title="Autore">
  <mail link="tsunam"/>
</author>
<author title="Autore">
  <mail link="matthew.summers@liquidustech.com">Matthew Summers</mail>
</author>
<author title="Autore">
  <mail link="richard.anderson@liquidustech.com">Richard Anderson</mail>
</author>
<author title="Autore">
  <mail link="steve.rucker@liquidustech.com">Steve Rucker</mail>
</author>
<author title="Redazione">
  <mail link="nightmorph"/>
</author>
<author title="Traduzione">
  <mail link="veni78@email.it">Andrea Venino</mail>
</author>

<abstract>
Questo documento illustra come configurare un servizio multipath per lo storage
dei dati.
</abstract>

<!-- The content of this document is licensed under the CC-BY-SA license -->
<!-- See http://creativecommons.org/licenses/by-sa/2.5 -->
<license/>

<version>1</version>
<date>2008-09-10</date>

<chapter>
<title>Introduzione</title>
<section>
<body>

<p>
I servizi di multipath, tipicamente presenti negli ambienti enterprise,
forniscono uno strumento di memorizzazione dei dati in grado di garantire alte
prestazioni, bilanciamento di carico e tolleranza d'errore sia localmente che su
uno storage di rete (storage area network, SAN). Il multipath permette in
maniera trasparente di accedere ad un singolo dispositivo di storage da uno o
più percorsi. Per esempio, se fossero presenti due connessioni dall'HBA (Host
Bus Adapter, la scheda che interfaccia un host con una SAN) di un server a due
switch in fibra ottica e quindi ad una SAN, il modulo HBA durante il caricamento
effettuerebbe una scansione del bus e riconoscerebbe quattro percorsi
disponibili per la SAN: due percorsi per raggiungere lo switch ed altrettanti
dallo switch alla SAN. Sfruttando questa situazione, Multipath permette di
utilizzare ogni percorso contemporaneamente o indipendentemente, assicurando una
connessione costante ed affidabile ai dati nello storage. Nel momento in cui
viene a mancare un percorso, Multipath agisce da failover, rendendo i dati
critici sempre disponibili grazie alla ridondanza nella progettazione e nella
implementazione del sistema.
</p>

<p>
Nello stadio più elementare, Multipath si compone di due parti distinte:
<c>device-mapper</c> e <c>multipath-tools</c>. <b>Device Mapper</b> è il primo
elemento chiave dell'applicazione. Agli amministratori probabilmente
risulteranno familiari Device Mapper come LVM, EVMS, dm-crypt o, in questo caso,
Multipath. In breve, lavorando nel kernel space, il Device Mapper rimappa un
dispositivo a blocchi come <path>/dev/sda</path> (tutte le esportazioni delle
SAN risultano in qualche modo periferiche SCSI) in un altro dispositivo.
</p>

<p>
Entrando maggiormente nel dettaglio, Device Mapper crea un dispositivo a blocchi
virtuale che riconosce tutti i comandi di un dispositivo a blocchi regolare, ma
inoltra i dati al dispositivo a blocchi reali. Come anticipato, il processo di
rimappatura viene interamente gestito nello spazio kernel: nulla viene eseguito
nello spazio utente.
</p>

<p>
<b>Multipath Tools</b> comprende un insieme di applicazioni che lavorano in
spazio utente le quali interagiscono con il Device Mapper e creano le strutture
per gestire il device, implementando l'I/O attraverso più percorsi a livello di
sistema operativo. In un tipico ambiente SAN, saranno presenti più percorsi per
uno stesso dispositivo di storage: una scheda (o più) in fibra ottica che
collega il server ad uno switch, collegato a sua volta allo storage vero e
proprio (come lo scenario presentato precedentemente). In questo caso gli
amministratori potrebbero raggiungere lo stesso device da una a quattro volte in
base alle situazioni: ogni scheda vedrebbe la LUN (Logical Unit Number, un
numero che identifica partizioni differenti di un medesimo RAID set) due volte,
una per ogni percorso disponibile. Così, un singolo dispositivo potrebbe venire
riconosciuto come <path>sda</path>, <path>sdb</path>, <path>sdc</path>, e
<path>sdd</path>. Per esempio, se si volesse montare <path>/dev/sda</path> su
<path>/san1</path>, si andrebbe ad utilizzare solamente un percorso singolo
attraverso una scheda in fibra connessa ad uno switch e quindi ad una porta sul
dispositivo di storage stesso: se uno qualsiasi di questi tre punti fallisse, lo
storage device verrebbe perso e occorrerebbe smontarlo per sostituirlo con
un'altro device (<path>sdb</path>).
</p>

<p>
Di conseguenza, utilizzare solamente uno dei quattro percorsi disponibili non
risulta la soluzione ideale, soprattutto in ambienti enterprise. In questi casi,
la soluzione si ottiene combinando Multipath Tools e Device Mapper.
Quest'ultimo, come già spiegato, provvede a creare un dispositivo a blocchi
virtuali e quindi a passare le informazioni al dispositivi reale.
</p>

</body>
</section>
</chapter>

<chapter>
<title>Installazione e Configurazione</title>
<section>
<title>Installazione dei Multipath Tools</title>
<body>

<p>
Anzitutto, è necessario emergere <c>multipath-tools</c> e <c>sg3_utils</c>.
Quindi è necessario individuare il <c>wwid</c> (World Wide Identification) del
disco: a tal fine è possibile utilizzare <c>sq_vpd</c> (fornito da
<c>sg3_utils</c>).
</p>

<pre caption="Installazione di multipath-tools e configurazione iniziale">
# <i>emerge multipath-tools sg3_utils</i>
<comment>(Si sostituisca /dev/DEVICE con il proprio disco per identificare il suo wwid)</comment>
# <i>/usr/bin/sq_vpd ?page=di /dev/DEVICE</i>
</pre>

<p>
Se DEVICE è il dispositivo scsi, il codice identificativo restituito inizierà
con <c>0x6</c>. Sostituendo <c>0x</c> con <c>3</c>, si otterrà l'ID corretto da
inserire all'interno del <c>multipath wwid</c> in
<path>/etc/multipath.conf</path>. Informazioni più approfondite verranno
specificate nel prossimo capitolo.
</p>

</body>
</section>
<section>
<title>Preparare Gentoo per il multipath</title>
<body>

<p>
Per utilizzare multipath in Gentoo, occorre abilitare i seguenti parametri nel
kernel:
</p>

<pre caption="Aggiungere il supporto a multipath">
Device Drivers  --->
  SCSI device support  --->
    &lt;*&gt; SCSI target support
    &lt;*&gt; SCSI disk support
    [*] Probe all LUNs on each SCSI device
  [*] Multiple devices driver support (RAID and LVM)  --->
    &lt;*&gt;  Multipath I/O support
    &lt;*&gt;  Device mapper support
    &lt;*&gt;    Multipath target
        <comment>(Si selezioni il/i proprio/i dispositivo/i dall'elenco)</comment>)
    &lt;*&gt;      EMC CX/AX multipath support
    &lt;*&gt;      LSI/Engenio RDAC multipath support
    &lt;*&gt;      HP MSA multipath support
</pre>

<note>
Lo <c>scsi_id</c> viene generato dai dispositivi. I drive IDE hanno due
tipologie di connessione: un amministratore ha la possibilità di impostare un
dispositivo come master e un altro come slave oppure lasciarlo decidere al
sistema modificando i ponticelli. scsi_id si comporta in modo simile: ogni drive
o Logical Unit Number (LUN) contiene un id univoco da 0 a 254. Un dispositivo
con ID 0 verrà identificato prima di un dispositivo con -per esempio- ID 120,
dal momento che il sistema effettua una scansione del bus per individuare quali
device rispondono (LIP) in maniera incrementale partendo da zero.
</note>

<p>
All'interno del menù di configurazione del kernel, ci si assicuri di impostare
il parametro CONFIG_SCSI_MULTI_LUN=y per dare al sottosistema SCSI l'abilità di
testare tutte le LUN (raccomandato nelle situazioni in cui ci si trova ad avere
un dispositivo con ID <c>0</c> e <c>2</c> ma nessun <c>1</c>: semplicemente si
troverebbe il device con ID <c>0</c> ma non quello con <c>2</c>). Si verifichi
inoltre di aver abilitato tutti i device hardware necessari per il sistema SCSI,
come le schede QLogic 2400, che non si trovano nel settore dei driver SCESI di
basso livello.
</p>

<p>
Per apprendere meglio quanto detto, si considerino i seguenti scenari:
</p>

<p>
Prendiamo ad esempio un sistema con tre dispositivi con ID 0,1 e 2. Senza
l'opzione "probe all LUNs", ritroveremmo gli ID 0,1,2 come sda,sdb,sdc: tutti
i dispositivi verrebbero correttamente riconosciuti. A questo punto eliminando
il drive con ID 1, quelli con ID 0 e 2 dovrebbero continuare ad essere visibili:
sarebbe logico pensare che adesso siano visibili sda ed sdb (sdc dovrebbe
essersi spostato su sdb, dal momento che non è più presente il dispositivo
precedente). A questo punto ci si potrebbe trovare in una delle seguenti
situazioni:
</p>

<p>
Scenario 1: Senza aver impostato l'opzione "probe all LUNs", all'avvio della
scansione verrebbe riconosciuto il dispositivo con ID 0 ed assegnato al disco
sda: a questo punto non individuando nessun dispositivo con ID 1, la scansione
verrebbe terminata e considerata completa sebbene ci sia un device con ID 2 o
superiore.
</p>

<p>
Scenario 2: Con l'opzione "probe all LUNs" impostata, all'avvio della scansione
verrebbe nuovamente individuato il dispositivo ID 0 ed assegnato al dispositivo
sda. A questo punto però la ricerca procederebbe con l'ID 1 (non presente) ma
- a differenza dallo scenario precedente - proseguirebbe alla ricerca di altri
dispositivi, assegnando la lun 2 al dispositivo sdb. La scansione verrebbe
considerata conclusa solo se non si riuscisse ad individuare nessun altro
dispositivo con ID superiore.
</p>

<note>
Sebbene possa sembrare irrealistico o anche non necessario avere device
distanziati tra loro da multe LUN, per ricoprire il più ampio ventaglio di
possibilità è preferibile effettuare una scansione completa di tutte le LUN. Un
amministratore potrebbe avere le più svariate ragioni per implementare una
simile configurazione. Pertanto, il secondo scenario presentato risulterebbe
ottimale per assicurare che tutti i dispositivi vengano riconosciuti ed
assegnati durante il processo di configurazione di multipath.
</note>

<p>
Dopo aver testato tutte le LUN, tutti i dispositivi sono stati riconosciuti ed
assegnati ad un ID di Multipath.
</p>

</body>
</section>
</chapter>

<chapter>
<title>Uno sguardo all'architettura</title>
<section>
<body>

<p>
Essendo parte di Multipath Tools, i dispositivi menzionati precedentemente
verranno riuniti in gruppi. Dopo aver configurato ed avviato
<c>multipath-tools</c> tramite il comando <c>/etc/init.d/multipath start</c>,
sarà possibile elencare i gruppi di dispositivi tramite <c>multipath -l</c>.
Il risultato ottenuto sarà simile al seguente:
</p>

<pre caption="output di multipath -l">
EVA_SAN (3600508b4001044ee00013000031e0000)
[size=300 GB][features="1 queue_if_no_path"][hwhandler="0"]
\_ round-robin 0 [active]
\_ 0:0:0:1 sda 8:0  [active]
\_ round-robin 0 [enabled]
\_ 0:0:1:1 sdb 8:16 [active]

EVA_SAN2 (3600508b4001044ee0001300003880000)
[size=300 GB][features="1 queue_if_no_path"][hwhandler="0"]
\_ round-robin 0 [active]
\_ 0:0:0:2 sdc 8:32 [active]
\_ round-robin 0 [enabled]
\_ 0:0:1:2 sdd 8:48 [active]
</pre>

<p>
Come impostazione predefinita, verrà preso in considerazione il primo gruppo
disponibile (per esempio il primo round-robin per il san
EVA_SAN2,<path>sdc</path>). In questa specifica situazione, dal momento che è
stato impostato una politica round-robin, i due percorsi verranno utilizzati
alternativamente; tuttavia nel momento in cui uno dei due percorsi fallisse,
tutte le informazioni verrebbero inviate tramite l'altro percorso ed il
dispositivo continuerebbe ad essere disponibile. Solamente se tutti i percorsi
al device fallissero contemporaneamente, il dispositivo risulterebbe
irraggiungibile e si dovrebbe usufruire del gruppo secondario.
</p>

</body>
</section>
<section>
<title>Una configurazione tipica</title>
<body>

<p>
Vediamo ora un esempio di una tipica configurazione di multipath:
</p>

<pre caption="Un tipico /etc/multipath.conf">
defaults {
udev_dir                /dev
polling_interval        15
selector                "round-robin 0"
path_grouping_policy    group_by_prio
failback                5
path_checker            tur
prio_callout            "/sbin/mpath_prio_tpc /dev/%n"
rr_min_io               100
rr_weight               uniform
no_path_retry           queue
user_friendly_names     yes
}
blacklist {
devnode cciss
devnode fd
devnode hd
devnode md
devnode sr
devnode scd
devnode st
devnode ram
devnode raw
devnode loop
devnode sda
}

multipaths {
multipath {
wwid
<comment>(Per individuare il proprio wwid, si utilizzi /usr/bin/sq_vpd ?page=di
/dev/DEVICE. L'indirizzo dovrebbe iniziare con il prefisso 0x6: si rimuovano i
primi due caratteri 0x e li si sostituisca con 3.)</comment>
alias DB_SAN
}
devices {
device {
<comment>(Gli spazi bianchi sono importanti in questi elementi per ottenere una
esatta corrispondenza con le specifiche dei vendor.)</comment>
"IBM     "
"1815      FAStT "
}
}
}
</pre>

<impo>
Per quanto riguarda le proprie periferiche, è opportuno utilizzare <c>cat</c>
<path>/sys/block/sd(device)/device/model</path> e <c>cat</c>
<path>/sys/block/device/sd(device)/device/vendor</path> per popolare
direttamente la sezione corretta del file di configurazione escludendo errori
di battitura: in determinati casi, gli spazi bianchi potrebbero risultare non
visibili. Questa sezione è necessaria dal momento che non tutte le stringhe
identificative dei vendor rispettano le convenzioni del kernel, e di
conseguenza, non sempre vengono riconosciute correttamente.
</impo>

<p>
Una configurazione tipica di multipath che utilizza un EVA_SAN riconosciuto
correttamente dal kernel, potrebbe essere simile alla seguente:
</p>

<pre caption="Configurazione di EVA_SAN">
multipaths {
multipath {
wwid  3600508b4001044ee00013000031e0000
alias EVA_SAN
}
multipath {
wwid    3600508b4001044ee0001300003880000
alias   EVA_SAN2
}
}
</pre>

</body>
</section>
</chapter>

<chapter>
<title>Configurare il proprio sistema</title>
<section>
<body>

<p>
La configurazione di multipath è abbastanza semplice da realizzare dal momento
che l'unico file su cui occorre intervenire è <path>/etc/multipath.conf</path>.
</p>

<p>
Anzitutto, occorre impostare nel parametro <b>polling interview</b> la frequenza
(in secondi) con cui verrà effettuato un controllo per assicurarsi che il
percorso sia disponibile.
</p>

<p>
<b>selector</b> andrà impostato su <c>"round-robin 0"</c>.
</p>

<note>
Il settore round-robin sarà l'unico utilizzato in questa configurazione.
</note>

<p>
<b>prio_callout</b>: questo parametro è abbastanza importante. Sono disponibili
numerose differenti priorità adatte ai differenti dispositivi, come ad esempio:
</p>

<ul>
  <li>mpath_prio_alua</li>
  <li>mpath_prio_emc</li>
  <li>mpath_prio_hds_modular</li>
  <li>mpath_prio_netapp</li>
  <li>mpath_prio_tpc</li>
</ul>

<note>
Per la maggior parte delle configurazioni, <c>mpath_prio_tpc</c> sarà la
soluzione migliore. Altri dispositivi come <c>mpath_prio_netapp</c> offrono
funzionalità particolari come il raggruppamento per priorità, come netapps.
</note>

<p>
<b>path_grouping_policy</b> possiede alcune possibili opzioni: failover,
multibus, group_by_prio. <c>Failover</c> manterrà solamente un disco per ogni
gruppo di priorità. <c>Multibus</c> inserirà tutti i dispositivi in un gruppo.
<c>Group_by_prio</c> utilizza un "valore di priorità": percorsi con stesso
valore vengono raggruppati insieme. Tali valori vengono determinati alla
chiamata del programma.
</p>

<p>
<b>no_path_retry</b> è impostato a <c>queue</c> perchè nella maggior parte delle
situazioni in caso di fallimento di un percorso, si preferisce non inviare i
dati del tutto. Così, in presenza di un fallimento, il sistema operativo metterà
le operazioni di I/O in coda finchè il dispositivo non ritornerà ad essere
disponibile; a quel punto verrà inviato nuovamente l'intero blocco di dati. In
se al proprio traffico, questo comportamento può provocare problemi di
sovraccarico.
</p>

<p>
<b>rr_min_io</b> è il numero di operazioni che il sistema operativo deve
effettuare per ogni percorso prima di passare al successivo percorso all'interno
dello stesso gruppo. Se <path>sda</path> ed <path>sdb</path> fossero nello
stesso gruppo, rr_min_io permetterebbe di fare 100 operazioni di I/O su
<path>sda</path> e quindi 100 su <path>sdb</path>, continuando a passare da uno
all'altro. Questa impostazione va perfezionata e personalizzata su ogni istanza
per massimizzare le prestazioni dal momento che il carico delle operazioni di
I/O cambiano molto in base alle situazioni. Il valore predefinito è <c>1000</c>,
ma in alcune circostanze è preferibile un valore minore in modo da cambiare
porte più spesso, quando possibile.
</p>

<p>
<b>user_friendly_names</b> rende più facile individuare le periferiche con cui
si sta lavorando. In questo esempio, impostando user_friendly_names a <c>no</c>,
la SAN sarà identificata tramite il suo WWID piuttosto che EVA_SAN.
</p>

</body>
</section>
</chapter>
</guide>
