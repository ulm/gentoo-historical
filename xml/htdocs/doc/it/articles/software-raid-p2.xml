<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE guide SYSTEM "/dtd/guide.dtd">
<!-- $Header: /var/cvsroot/gentoo/xml/htdocs/doc/it/articles/software-raid-p2.xml,v 1.3 2010/03/07 13:30:50 scen Exp $ -->

<guide link="/doc/it/articles/software-raid-p2.xml" disclaimer="articles"
lang="it">
<title>Software RAID nel nuovo kernel Linux 2.4, Parte 2</title>

<author title="Autore">
  Daniel Robbins
</author>
<author title="Traduzione">
  <mail link="scen"/>
</author>
<!-- xmlified by: Joshua Saddler (nightmorph@gentoo.org) -->

<abstract>
In questa serie di due articoli, Daniel Robbins introduce al RAID Software di
Linux 2.4, una tecnologia usata per incrementare le prestazioni e
l'affidabilità dei dischi fissi mediante la distribuzione dei dati su più
dischi. In questo articolo, Daniel spiega cosa possono e non possono fare
RAID-1, 4 e 5 e come trattare l'implementazione di questi livelli RAID in un
ambiente di produzione. Nella seconda metà dell'articolo, Daniel accompagnerà i
lettori attraverso una simulazione di sostituzione di un disco in RAID-1 guasto.
</abstract>

<!-- The original version of this article was first published on IBM
developerWorks, and is property of Westtech Information Services. This
document is an updated version of the original article, and contains
various improvements made by the Gentoo Linux Documentation team -->

<version>1.1</version>
<date>2005-10-09</date>

<chapter>
<title>Impostare RAID-1 in un ambiente di produzione</title>
<section>
<title>RAID nel mondo reale</title>
<body>

<p>
Nell'<uri link="/doc/it/articles/software-raid-p1.xml">articolo
precedente</uri>, sono state introdotte ai lettori alle funzionalità del RAID
software di Linux 2.4, mostrando come impostare volumi lineari, RAID-0 e RAID-1.
In questo articolo, verrà analizzato cosa serve sapere per poter usare RAID-1 in
modo da incrementare la disponibilità dei dati in ambiente di produzione. Ciò
richiede maggiore comprensione e conoscenza rispetto alla semplice creazione di
un RAID-1 su un server di test o nel proprio pc casalingo; più precisamente,
bisogna sapere esattamente da quali cause esterne il RAID-1 dovrà proteggere
i dati, e come mantenere attivo e funzionante il proprio volume RAID nel caso si
guasti un disco. In questo articolo verranno coperti questi aspetti, iniziando
con una panoramica su cosa possono fare o non fare RAID-1, 4 e 5, e terminando
con una completa simulazione di test della sostituzione di un disco in RAID-1
guasto (qualcosa che si dovrebbe effettivamente fare, seguendo questo articolo
come guida, in base alle proprie possibilità). Dopo aver effettuato la
simulazione, si avrà acquisito tutta l'esperienza necessaria per gestire i
guasti di un RAID-1 in un ambiente reale.
</p>

</body>
</section>
<section>
<title>Cosa non può fare RAID</title>
<body>

<p>
Le caratteristiche di tolleranza ai guasti di RAID sono progettate per
proteggere i dati dell'utente dagli effetti negativi di uno spontaneo guasto
completo del disco, e questa è una buona cosa. Ma RAID non è una soluzione
perfetta per ogni tipo di problema riguardante l'affidabilità. Prima di
implementare una forma di RAID (1,4,5) tollerante ai guasti in un ambiente di
produzione, è estremamente importante conoscere esattamente cosa <b>NON</b>
potrà fare RAID per i dati dell'utente. Quando si è in una situazione nella
quale l'operatività dipende da RAID, non si vuole fare nessuna falsa
affermazione riguardo alle sue funzionalità. Cominciamo sfatando alcuni miti
comuni riguardo a RAID 1, 4 e 5.
</p>

<p>
Molte persone pensano che posizionare tutti i propri dati importanti su un
volume RAID 1/4/5 li esimi dall'eseguire backup regolari. Questa cosa è
completamente falsa, ed il perchè è presto detto. RAID 1/4/5 aiutano a
proteggere i dati da <e>interruzioni</e> non pianificate causate da guasti
casuali dei dischi. Tuttavia, non offrono protezioni contro <e>corruzioni dei
dati</e> casuali o intenzionali. Se viene digitato <c>cd /; rm -rf *</c> come
root su un volume RAID, si perderanno un sacco di dati molto importanti nel giro
di qualche secondo, ed il fatto di avere una configurazione RAID-5 su 10 dischi
sarà ininfluente. Inoltre, RAID non sarà d'aiuto se il server viene fisicamente
rubato o se c'è un incendio nell'edificio. E ovviamente, se non viene
implementata una strategia di backup, non si avrà un archivio dei dati passati:
se qualcuno nel proprio ufficio cancella alcuni dati importanti, non si
riuscirà a recuperarli. Solamente questo dovrebbe essere sufficiente a
convincere che, nella maggior parte dei casi, bisogna pianificare ed
implementare una strategia di backup <e>prima</e> di pensare di affrontare
l'implementazione di un potenziale RAID-1, 4 o 5.
</p>

<p>
Un altro errore è l'implementazione di un RAID software su un sistema composto
da hardware di bassa qualità. Se si sta assemblando un server che dovrà svolgere
qualcosa di importante, ha senso acquistare l'hardware migliore nel limite
delle proprie disponibilità economiche. Se il proprio sistema è instabile o non
adeguatamente raffreddato, si incorrerà in problemi che RAID non potrà
risolvere. Un'altro esempio simile è l'impossibilità di RAID nel fornire
continuità aggiuntiva al servizio in caso di mancanza di alimentazione. Se il
proprio server è adibito a svolgere un qualsiasi compito relativamente
importante, assicurarsi che sia equipaggiato con un gruppo di continuità (UPS).
</p>

<p>
Inoltre è importante focalizzarsi sulle problematiche inerenti ai filesystem.
Il filesystem esiste "al di sopra" del proprio volume RAID software. Ciò
significa che usare un RAID software non permette di evitare i problemi dovuti
al filesystem, per esempio a lunghi e potenzialmente problematici <c>fsck</c> se
capita di non utilizzare un filesystem non journaled o con la tendenza a
rovinarsi. Pertanto, il RAID software non rende il filesystem ext2 più
affidabile; ecco perché è così importante che la comunità Linux abbia a
disposizione ReiserFS, come anche JFS e XFS. RAID software e filesystem
journaled affidabili sono una combinazione efficace.
</p>

</body>
</section>
<section>
<title>RAID - implementazione intelligente</title>
<body>

<p>
Speriamo che la sezione precedente abbia sfatato qualunque mito riguardo a RAID
eventualmente a conoscenza del lettore. Quando si implementa RAID-1, 4 o 5, è
molto importante vedere la tecnologia come qualcosa che migliora
l'<e>uptime</e> (periodo di tempo nel quale la macchina è attiva e funzionante,
N.d.T.). Quando viene implementato uno di questi livelli RAID, si verrà
protetti da una situazione molto specifica, ovvero un completo guasto spontaneo
(singolo o multiplo) dei dischi. Se si sperimenta questa situazione, il RAID
software permetterà al sistema di continuare a funzionare, così come dare
l'opportunità all'utente di eseguire le operazioni necessarie per sostituire il
disco guasto con uno nuovo. In altre parole, se si implementa RAID 1,4, o 5,
verrà ridotto il rischio di avere una lunga interruzione di servizio non
pianificata, solamente il tempo necessario alla sostituzione del disco fuori
uso. Ovviamente, questo significa che se non è una priorità avere un sistema
altamente disponibile non sarà necessario implementare un RAID software, a meno
che non si stia progettando di usarlo principalmente come un modo per
incrementare le prestazioni di I/O su file.
</p>

<p>
Un amministratore di sistema intelligente usa il RAID software per uno scopo
preciso, ovvero migliorare l'affidabilità di un server già molto affidabile. Se
si è dei bravi amministratori di sistema, si avranno già affrontati gli aspetti
basilari. La propria organizzazione sarà protetta dalle catastrofi grazie
all'implementazione di un piano regolare di backup. Il proprio server sarà
alimentato da un UPS, e ci sarà in funzione un software di monitoraggio
dell'UPS in modo che il proprio server venga spento in caso di un prolungamento
di mancanza di corrente. Forse si starà utilizzando un filesystem journaled
come ReiserFS in modo da ridurre i tempi di esecuzione di <c>fsck</c> e
incrementare le prestazioni e l'affidabilità dello stesso. E si spera che il
proprio server sia adeguatamente raffreddato e sia composto da hardware di alta
qualità, e si abbia fatto grande attenzione ai problemi di sicurezza. Ora, e
solo ora, si dovrebbe considerare l'implementazione di un RAID-1, 4 o 5
software; facendolo, si aggiungerà potenzialmente qualche punto percentuale di
uptime al proprio server difendendolo da un guasto completo dei dischi. Il RAID
software aggiunge uno strato di protezione che rende ancora migliore un server
già robusto di per sé.
</p>

</body>
</section>
</chapter>

<chapter>
<title>Spiegazione dettagliata per RAID-1</title>
<section>
<body>

<p>
Ora che si è a conoscenza di cosa RAID può fare e non fare, chi scrive spera che
il lettore abbia aspirazioni ragionevoli ed un giusto atteggiamento nei
confronti dell'argomento. In questa sezione, si verrà accompagnati attraverso il
processo di simulazione di guasto di un disco, ed il riavvio del proprio volume
RAID in modalità degradata. Se si possiede l'abilità di impostare un volume
RAID-1 su una macchina di test e seguire i vari passaggi, è caldamente
consigliato farlo. Questo tipo di simulazione può risultare divertente. E
divertirsi un po' proprio adesso può aiutare ad assicurare che quando un disco si
guasterà veramente, si sarà calmi e padroni di sé, sapendo esattamente cosa
fare.
</p>

<impo>
Per eseguire questo test, è essenziale l'aver impostato un volume RAID-1 in
modo da poter avviare comunque il proprio sistema Linux con un disco fisso
scollegato, perché sarà il modo con il quale si andrà a simulare il guasto di un
disco.
</impo>

<p>
Il primo passo è impostare un volume RAID-1: fare riferimento all'<uri
link="/doc/it/articles/software-raid-p1.xml">articolo precedente</uri> se si ha
bisogno di rinfrescarsi la memoria a riguardo. Una volta impostato il volume, il
comando <c>cat /proc/mdstat</c> visualizzerà qualcosa di simile a questo:
</p>

<pre caption="Esaminare il volume RAID">
# <i>cat /proc/mdstat</i>
Personalities : [linear] [raid0] [raid1] [raid5]
read_ahead 1024 sectors
md0 : active raid1 ide/host2/bus0/target0/lun0/part1[1] ide/host0/bus0/target0/lun0/part5[0]
      4610496 blocks [2/2] [UU]
      [======&gt;..............]  resync = 34.8% (1606276/4610496) finish=3.2min speed=15382K/sec

unused devices: &lt;none&gt;
</pre>

<p>
Notare che chi scrive sta utilizzando devfs, che è anche il motivo per cui si
vedono dei nomi di dispositivo estremamente lunghi nel codice appena esposto.
L'autore in verità sta utilizzando <path>/dev/hda5</path> e
<path>/dev/hde1</path> come dischi RAID-1. In questo momento, il codice del
software RAID nel kernel sta sincronizzando i dischi in modo che ciascuno sia
l'esatta copia dell'altro. Se il proprio volume RAID-1 è arrivato a questo
punto, si può proseguire e creare un filesystem sul volume, e montarlo da
qualche parte. Copiarci dentro qualche file ed impostare il proprio
<path>/etc/fstab</path> in modo che il volume (<path>/dev/md0</path>) venga
montato all'avvio del sistema. Questa è la linea che l'autore del documento ha
aggiunto al proprio fstab; quella dell'utente potrebbe differire leggermente:
</p>

<pre caption="informazioni in fstab">
/dev/md0       /mnt/raid1              reiserfs        defaults            0 0
</pre>

<p>
Si è quasi pronti per simulare un guasto del disco, ma manca ancora una cosa.
Primo, eseguire nuovamente <c>cat /proc/mdstat</c>, ed attendere fino a che
tutti i volumi dei dischi saranno sincronizzati. Dopodiché, il proprio
<path>/proc/mdstat</path> dovrebbe apparire così:
</p>

<pre caption="Riesaminare il volume RAID">
# cat /proc/mdstat
Personalities : [linear] [raid0] [raid1] [raid5]
read_ahead 1024 sectors
md0 : active raid1 ide/host2/bus0/target0/lun0/part1[1] ide/host0/bus0/target0/lun0/part5[0]
      4610496 blocks [2/2] [UU]

unused devices: &lt;none&gt;
</pre>

</body>
</section>
<section>
<title>Inizia la simulazione</title>
<body>

<p>
Ora che la risincronizzazione è completata, si è pronti per la simulazione.
Proseguire spegnendo completamente la propria macchina. Successivamente aprirla
e scollegare uno dei dischi fissi che compongono l'array RAID-1. Ovviamente, non
bisognerà scollegare il disco che contiene la propria partizione root di Linux,
in quanto si avrà bisogno di avviare nuovamente Linux! Ora che il disco fisso è
scollegato, riaccendere la macchina. Una volta effettuato il login, si potrà
constatare che <path>/dev/md0</path> è montato e che si potrà ancora usare il
volume. Eseguendo <c>cat /proc/mdstat</c> si potrà vedere il cambiamento
importante:
</p>

<pre caption="Mancanza di un disco">
# <i>cat /proc/mdstat</i>
Personalities : [linear] [raid0] [raid1] [raid5]
read_ahead 1024 sectors
md0 : active raid1 ide/host0/bus0/target0/lun0/part5[0]
      4610496 blocks [2/1] [U_]

unused devices: &lt;none&gt;
</pre>

<p>
È possibile vedere che il volume <path>/dev/md0</path> è in esecuzione in
modalità degradata. Il disco <path>/dev/hde</path> è scollegato, in tal modo
<path>/dev/hde1</path> non viene trovato durante la fase di boot del kernel e
l'avvio automatico dell'array. Fortunatamente, il kernel ha trovato
<path>/dev/hda5</path>, e <path>/dev/md0</path> è riuscito comunque ad avviarsi
in modalità degradata. Com'è possibile vedere, la partizione
<path>/dev/hde1</path> non viene elencata in <path>/proc/mdstat</path>, e uno
dei dischi RAID è marcato come "fuori uso" (<c>[U_]</c> invece di <c>[UU]</c>).
Ma siccome <path>/dev/md0</path> sta comunque funzionando, il RAID-1 software
sta facendo quello che per cui è progettato: mantenere i propri dati
disponibili.
</p>

</body>
</section>
<section>
<title>Recupero</title>
<body>

<p>
In questo momento si sta sperimentando un guasto simulato di un disco. Se il
disco che attualmente non è alimentato si guasta realmente mentre il sistema è
in esecuzione, è il tipo di situazione che si sta vivendo. Il proprio volume
RAID-1 verrà eseguito in modalità degradata, in altre parole esso sarà ancora
disponibile ma senza alcuna ridondanza. Al momento opportuno si dovrà spegnere
il sistema, sostituire il disco guasto, e riavviare nuovamente il sistema. A
questo punto il proprio volume RAID-1 partirà ancora in modalità degradata.
</p>

<p>
Una volta che il nuovo disco è nella macchina, creare una partizione RAID
autodetect (<c>FD</c>) di dimensioni adeguate sul nuovo disco. Un riavvio
addizionale potrebbe essere necessario in modo che Linux possa rileggere la
tabella delle partizioni del disco. Una volta che la nuova partizione è
visibile al sistema, si è pronti per ripristinare il proprio array RAID-1,
ottenendo nuovamente un po' di ridondanza.
</p>

<p>
Ovviamente, quella che si sta eseguendo è solamente una simulazione. Per fare
pratica nel riaggiungere una partizione nel proprio array RAID, è possibile
fare due cose, in base al tipo di scenario che si preferisce affrontare. Si può
o spegnere la macchina, collegare il disco, avviare il sistema, e aggiungere
nuovamente la vecchia partizione nell'array, o spegnere la macchina, collegare
il disco, avviare, cancellare il disco, creare una <e>nuova</e> partizione RAID
autodetect (<c>FD</c>) da aggiungere all'array (della dimensione corretta,
ovviamente, grande almeno quanto la partizione che si sta ripristinando),
dopodiché aggiungere questa partizione nuova di zecca all'array. La seconda
scelta probabilmente si avvicina di più a quello che potrebbe avvenire nel caso
un guasto reale del disco, mentre la prima simula qualcosa come situazione di
guasto al controller dei dischi o di un cavo danneggiato, dove uno dei dischi
del mirror è temporaneamente non disponibile, causando l'esecuzione in modalità
degradata di <path>/dev/md0</path>, e richiedendo la successiva aggiunta di una
delle partizioni dopo aver rimediato al problema. Qualsiasi simulazione si
scelga, la "correzione" è la medesima: dopo che la partizione è pronta, bisogna
reinserirla nel volume <path>/dev/md0</path>.
</p>

</body>
</section>
<section>
<title>Controllare dmesg</title>
<body>

<p>
Prima di reinserire la partizione nel proprio array, è buona cosa dare
un'occhiata ai messaggi di boot del proprio kernel. Digitando <c>dmesg |
more</c> sarà possibile visualizzarli. Si dovrebbe vedere una porzione di testo
simile al seguente:
</p>

<pre caption="Messaggi di boot del kernel">
linear personality registered
raid0 personality registered
raid1 personality registered
raid5 personality registered
raid5: measuring checksumming speed
   8regs     :  1291.209 MB/sec
   32regs    :  1195.197 MB/sec
   pII_mmx   :  2110.740 MB/sec
   p5_mmx    :  2652.522 MB/sec
raid5: using function: p5_mmx (2652.522 MB/sec)
md driver 0.90.0 MAX_MD_DEVS=256, MD_SB_DISKS=27
md.c: sizeof(mdp_super_t) = 4096
autodetecting RAID arrays
(read) ide/host0/bus0/target0/lun0/part5's sb offset: 4610560 [events: 00000004]
(read) ide/host2/bus0/target0/lun0/part1's sb offset: 4610496 [events: 00000002]
autorun ...
considering ide/host2/bus0/target0/lun0/part1 ...
  adding ide/host2/bus0/target0/lun0/part1 ...
  adding ide/host0/bus0/target0/lun0/part5 ...
created md0
bind&lt;ide/host0/bus0/target0/lun0/part5,1&gt;
bind&lt;ide/host2/bus0/target0/lun0/part1,2&gt;
running: &lt;ide/host2/bus0/target0/lun0/part1&gt;&lt;ide/host0/bus0/target0/lun0/part5&gt;
now!
ide/host2/bus0/target0/lun0/part1's event counter: 00000002
ide/host0/bus0/target0/lun0/part5's event counter: 00000004
md: superblock update time inconsistency -- using the most recent one
freshest: ide/host0/bus0/target0/lun0/part5
md: kicking non-fresh ide/host2/bus0/target0/lun0/part1 from array!
unbind&lt;ide/host2/bus0/target0/lun0/part1,1&gt;
export_rdev(ide/host2/bus0/target0/lun0/part1)
md0: max total readahead window set to 124k
md0: 1 data-disks, max readahead per data-disk: 124k
raid1: device ide/host0/bus0/target0/lun0/part5 operational as mirror 0
raid1: md0, not all disks are operational -- trying to recover array
raid1: raid set md0 active with 1 out of 2 mirrors
md: updating md0 RAID superblock on device
ide/host0/bus0/target0/lun0/part5 [events: 00000005](write) ide/host0/bus0/target0/lun0/part5's sb offset: 4610560
md: recovery thread got woken up ...
md0: no spare disk to reconstruct array! -- continuing in degraded mode
md: recovery thread finished ...
..
.... autorun DONE.
</pre>

<p>
Ora è consigliabile prendere un po' di tempo e leggere attentamente questi
messaggi, perché aiuteranno a capire il processo usato dal kernel per avviare
automaticamente <path>/dev/md0</path>, fornendo un altro visione preziosa dei
funzionamenti interni del software RAID di Linux. Se l'output del kernel
elencato sopra è stato letto, si vedrà che il kernel ha individuato
<path>/dev/hda5</path> e <path>/dev/hde1</path>, ma <path>hde1</path> non è
aggiornato rispetto a <path>hda5</path>. Pertanto il kernel avvia
<path>/dev/md0</path> in modalità degradata, usando <path>/dev/hda5</path> e non
toccando assolutamente <path>/dev/hde1</path>. A questo punto è il momento di
aggiungere la propria partizione originale (o creata ex-novo) al volume. Ecco
come.
</p>

</body>
</section>
<section>
<title>Il ripristino continua</title>
<body>

<p>
Per prima cosa, se la partizione sostitutiva ha un nuovo nome di dispositivo,
aggiornare <path>/etc/raidtab</path> in modo che rispecchi le nuove
informazioni. Successivamente aggiungere la nuova partizione al volume usando
il seguente comando, sostituendo <path>/dev/hde1</path> con il nome del
dispositivo della partizione che si andrà ad aggiungere:
</p>

<pre caption="Aggiungere il nuovo dispositivo">
# <i>raidhotadd /dev/md0 /dev/hde1</i>
</pre>

<p>
Le luci del proprio disco fisso dovrebbero cominciare a lampeggiare in quanto
comincerà la ricostruzione. Proseguire ed eseguire <c>cat /proc/mdstat</c> per
verificare lo stato di ricostruzione del RAID-1 che ora sarà in corso:
</p>

<pre caption="Verificare lo stato di ricostruzione del RAID-1">
# <i>cat /proc/mdstat</i>
Personalities : [linear] [raid0] [raid1] [raid5]
read_ahead 1024 sectors
md0 : active raid1 ide/host2/bus0/target0/lun0/part1[2] ide/host0/bus0/target0/lun0/part5[0]
      4610496 blocks [2/1] [U_]
      [&gt;....................]  recovery =  1.8% (84480/4610496) finish=3.5min speed=21120K/sec
unused devices: &lt;none&gt;
</pre>

<p>
Nel giro di qualche minuto, il proprio volume RAID-1 tornerà ad uno stato
normale:
</p>

<pre caption="Il volume RAID normale">
# <i>cat /proc/mdstat</i>
Personalities : [linear] [raid0] [raid1] [raid5]
read_ahead 1024 sectors
md0 : active raid1 ide/host2/bus0/target0/lun0/part1[1] ide/host0/bus0/target0/lun0/part5[0]
      4610496 blocks [2/2] [UU]

unused devices: &lt;none&gt;
</pre>

<p>
Voila! È stata appena effettuato un ripristino da un guasto di un disco
simulato, e si è pronti per usare RAID-1 in un ambiente di produzione. Ora sarà
possibile attaccare l'adesivo fatto in casa "certificato RAID-1" sulla propria
fronte ed iniziare a sbattere le braccia correndo in giro per l'ufficio per il
diletto dei propri colleghi. Ovviamente, forse non è una grande idea. Ci si
vede nel prossimo articolo.
</p>

</body>
</section>
<section>
<title>Risorse</title>
<body>

<ul>
  <li>
    Leggere la <uri
    link="/doc/it/articles/software-raid-p1.xml">Parte 1</uri> nella serie di
    Daniel sul RAID, dove introduce la funzionalità di RAID software di Linux
    2.4 e mostra come impostare volumi lineari, RAID-0 e RAID-1.
  </li>
  <li>
    Il <uri
    link="http://www.tldp.org/HOWTO/Software-RAID-HOWTO.html">
    Software-RAID HOWTO</uri> è un'altra risorsa eccellente per informazioni
    riguardo al Software RAID di Linux.
  </li>
  <li>
    Se si vuole saperne di più su come creare un filesystem di root su RAID è
    consigliabile dare un'occhiata al <uri
    link="http://www.tldp.org/HOWTO/Boot+Root+Raid+LILO.html">
    Boot+Root+RAID+Lilo Software RAID HOWTO</uri>
  </li>
  <li>
    Per versioni aggiornare di raidtools-0.90, tenere sotto controllo <uri
    link="http://people.redhat.com/mingo/raid-patches/">
    people.redhat.com</uri>.
  </li>
  <li>
    Trovare un <uri link="http://www.kernel.org">kernel recente</uri> negli
    Archivi del Kernel Linux.
  </li>
  <li>
    Leggere il <uri
    link="/doc/it/articles/linux-kernel-compiling.xml">tutorial</uri> di Daniel
    su come compilare ed installare un nuovo kernel a partire dai sorgenti.
  </li>
  <li>
    Trovare il <uri link="http://people.redhat.com/mingo/raidtools/">programma
    raidtools</uri>.
  </li>
  <li>
    Recuperare le <uri link="http://people.redhat.com/mingo/raid-patches">
    ultime versioni di raidtools</uri>.
  </li>
  <li>
    Dare uno sguardo a <uri link="http://linas.org/linux/raid.html">ulteriori
    consigli sulle soluzione Raid Software per Linux</uri>.
  </li>
</ul>

</body>
</section>
</chapter>
</guide>
