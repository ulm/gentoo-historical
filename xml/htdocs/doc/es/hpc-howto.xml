<?xml version='1.0' encoding="UTF-8"?>

<!-- $Header: /var/cvsroot/gentoo/xml/htdocs/doc/es/hpc-howto.xml,v 1.10 2012/07/24 22:24:39 nimiux Exp $ -->

<!DOCTYPE guide SYSTEM "/dtd/guide.dtd">
<guide lang="es">

<title>Computación de Alto Rendimiento con Gentoo Linux</title>

<author title="Autor">
  <mail link="marc@adelielinux.com">Marc St-Pierre</mail>
</author>
<author title="Autor">
  <mail link="benoit@adelielinux.com">Benoit Morin</mail>
</author>
<author title="Asistente">
  <mail link="jean-francois@adelielinux.com">Jean-Francois Richard</mail>
</author>
<author title="Asistente">
  <mail link="olivier@adelielinux.com">Olivier Crete</mail>
</author>
<author title="Revisor">
  <mail link="dberkholz@gentoo.org">Donnie Berkholz</mail>
</author>
<author title="Editor">
  <mail link="nightmorph"/>
</author>
<author title="Traductor">
    <mail link="chiguire@gentoo.org">John Christian Stoddart</mail>
</author>
<author title="Traductor">
    <mail link="yoswink@gentoo.org">José Luis Rivero</mail>
</author>
<author title="Traductor">
  <mail link="LinuxBlues@gentoo-es.org">Fernando M. Bueno</mail>
</author>
<author title="Traductor">
  <mail link="nimiux"/>
</author>

<!-- Sin información sobre la licencia; este documento ha sido escrito por
     terceras partes, sin información adicional sobre la licencia desde esta
     organización.
     En otras palabras, tiene copyright de adelielinux R&D; Gentoo solo
     tiene permiso para distribuir este documento tal cual y actualizarlo
     cuando sea apropiado de acuerdo con la notificación de adelie linux R&D.
-->

<abstract>
Este documento fue escrito por el personal del Centro Adelie Linux
R&amp;D &lt;http://www.adelielinux.com&gt; como una guía paso a paso
para hacer de Gentoo un sistema de computación de alto rendimiento
(HPC).
</abstract>

<version>2</version>
<date>2012-07-24</date>

<chapter>
<title>Introducción</title>
<section>
<body>

<p>
Gentoo Linux, es un sabor especial de Linux que puede ser
automáticamente optimizado para cualquier aplicación o
necesidad. Rendimiento extremo, configurabilidad y una comunidad de
usuarios y desarrolladores de la mayor calidad imaginable son
característicos de la experiencia Gentoo.
</p>

<p>
Gracias a una tecnología llamada Portage, Gentoo Linux puede llegar a
ser un servidor seguro ideal, una estación de trabajo, un escritorio
profesional, un sistema de juegos, una solución que las integre o
... un sistema de computación de alto rendimiento. Debido a su
adaptabilidad casi ilimitada, llamamos a Gentoo Linux una
metadistribución.
</p>

<p>
Este documento explica cómo hacer de Gentoo un sistema de computación
de alto rendimiento. Explica paso a paso los paquetes a instalar y
ayuda a configurarlos.
</p>

<p>
Gentoo Linux puede obtenerse desde <uri>http://www.gentoo.org</uri>, y
puede obtenerse también la <uri link="/doc/es/">documentación</uri>
para instalarlo.
</p>
</body>
</section>
</chapter>

<chapter>
<title>Configurando Gentoo Linux para Clustering</title>
<section>
<title>Optimizaciones Recomendadas</title>
<body>

<note>
Clustering: grupo de ordenadores realizando una misma función.
</note>

<note>
En esta sección nos referimos al <uri link="/doc/es/handbook/">
Manual Gentoo</uri>.
</note>

<p>
Durante el proceso de instalación, deben ajustarse los parámetros USE
en <path>/etc/portage/make.conf</path>. Recomendamos que se
desactiven los valores por defecto (véase
<path>/etc/portage/make.profile/make.defaults</path>) negándolos en
make.conf. De cualquier forma, pueden conservarse variables como
3dnow, gpm, mmx, nptl, nptlonly, sse, ncurses, pam y tcpd. Consulte la
documentación USE para más información.
</p>

<pre caption="Parámetros USE">
USE="-oss 3dnow -apm -avi -berkdb -crypt -cups -encode -gdbm -gif gpm -gtk
-imlib -java -jpeg -kde -gnome -libg++ -libwww -mikmod mmx -motif -mpeg ncurses
-nls nptl nptlonly -ogg -opengl pam -pdflib -png -python -qt4 -qtmt
-quicktime -readline -sdl -slang -spell -ssl -svga tcpd -truetype -vorbis -X
-xml2 -xv -zlib"
</pre>

<p>
O sencillamente:
</p>

<pre caption="versión simplificada - Parámetros USE">
# Copyright 2000-2003 Daniel Robbins, Gentoo Technologies, Inc.
# Contains local system settings for Portage system

# Please review 'man make.conf' for more information.

USE="-* 3dnow gpm mmx ncurses pam sse tcpd"
</pre>

<note>
El parámetro USE <e>tcpd</e> incrementa la seguridad de paquetes como
xinetd.
</note>

<p>
En el paso 15 ("Instalando el núcleo y los registros del sistema"),
por razones de estabilidad, recomendamos las fuentes vanilla
(vanilla-sources), el código fuente oficial del núcleo realizado en
<uri>http://www.kernel.org/</uri>, a menos que se requiera soporte
específico, como el de xfs.
</p>

<pre caption="Instalando las fuentes vanilla">
# <i>emerge -a syslog-ng vanilla-sources</i>
</pre>

<p>
Recomendamos instalar los siguientes paquetes:
</p>

<pre caption="Instalando los paquetes necesarios">
# <i>emerge -p nfs-utils portmap tcpdump ssmtp iptables xinetd</i>
</pre>
</body>
</section>

<section>
<title>Capa de Comunicación (TCP/IP Network)</title>
<body>

<p>
Un grupo de computadoras <e>(cluster)</e> requiere una capa de
comunicación que permita la comunicación entre los nodos esclavos y el
nodo maestro.  Normalmente, LAN: FastEthernet o GigaEthernet, pueden
ser usados, dado que tienen una buena relación
precio/rendimiento. Otras posibilidades incluyen el uso de productos
como <uri link="http://www.myricom.com/">Myrinet</uri>, <uri
link="http://quadrics.com/">QsNet</uri> u otros.
</p>

<p>
Un cluster se compone de dos tipos de nodo: maestro y
esclavo. Normalmente, un cluster dispone de un nodo maestro y varios
nodos esclavos.
</p>

<p>
El nodo maestro es el servidor del cluster. Es el responsable de
decirle a los nodos esclavo qué deben hacer. El servidor ejecutará
<e>demonios</e> como dhcpd, nfs, pbs-server, y pbs-sched. El nodo
maestro permitirá sesiones interactivas de usuarios y aceptará la
ejecución de trabajos.
</p>

<p>
Los nodos esclavo escuchan las instrucciones (quizá vía ssh/rsh) desde
el nodo maestro. Deben dedicarse a proporcionar resultados y, por
tanto, no deberían ejecutar ningún servicio innecesario.
</p>

<p>
El resto de este documento asume un cluster cuya configuración es la
que se proporciona en el siguiente archivo <e>hosts</e>. Debería
mantenerse cada nodo en el archivo <path>/etc/hosts</path> con
entradas para cada uno de los nodos que participe en el cluster.
</p>

<pre caption="/etc/hosts">
# Adelie Linux Research &amp; Development Center
# /etc/hosts

127.0.0.1       localhost

192.168.1.100   master.adelie master

192.168.1.1     node01.adelie node01
192.168.1.2     node02.adelie node02
</pre>

<p>
Para configurar la LAN dedicada al <e>cluster</e>, se edita el archivo
<path>/etc/conf.d/net</path> en el nodo maestro.
</p>

<pre caption="/etc/conf.d/net">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License, v2 or later

# Global config file for net.* rc-scripts

# This is basically the ifconfig argument without the ifconfig $iface
#

iface_eth0="192.168.1.100 broadcast 192.168.1.255 netmask 255.255.255.0"
# Network Connection to the outside world using dhcp -- configure as required for you network
iface_eth1="dhcp"
</pre>

<p>
Finalmente, se configura un servidor DHCP en el nodo maestro para
evitar tener que mantener una configuración de red en cada uno de los
nodos esclavo.
</p>

<pre caption="/etc/dhcp/dhcpd.conf">
# Adelie Linux Research &amp; Development Center
# /etc/dhcp/dhcpd.conf

log-facility local7;
ddns-update-style none;
use-host-decl-names on;

subnet 192.168.1.0 netmask 255.255.255.0 {
        option domain-name "adelie";
        range 192.168.1.10 192.168.1.99;
        option routers 192.168.1.100;

        host node01.adelie {
                # MAC address of network card on node 01
                hardware ethernet 00:07:e9:0f:e2:d4;
                fixed-address 192.168.1.1;
        }
        host node02.adelie {
                # MAC address of network card on node 02
                hardware ethernet 00:07:e9:0f:e2:6b;
                fixed-address 192.168.1.2;
        }
}
</pre>
</body>
</section>

<section>
<title>NFS/NIS</title>
<body>

<p>
El sistema de archivos de red (NFS) se desarrolló para permitir a las
máquinas montar la partición de un disco en una máquina remota como si
estuviese en un disco duro local. Esto permite compartir ficheros a
través de una red de forma rápida y eficaz.
</p>

<p>
Hay otros sistemas que proporcionan una funcionalidad similar a NFS
que pueden usarse en entornos <e>cluster</e>. El <uri
link="http://www.openafs.org">Andrew File System de IBM</uri>,
recientemente de código abierto, proporciona un mecanismo que permite
compartir archivos con características adicionales de seguridad y
rendimiento. El <uri link="http://www.coda.cs.cmu.edu/">Sistema de
archivos Coda</uri> se está desarrollando todavía, pero está diseñado
para trabajar bien, aún con clientes desconectados. Muchas de las
características de los sistemas de archivos Andrew y Coda se incluirán
en la siguiente versión de <uri link="http://www.nfsv4.org">NFS
(Versión 4)</uri>. La ventajas actuales de NFS son que es un sistema
maduro, estándar, bien entendido y soportado robustamente en una
extensa variedad de plataformas.
</p>

<pre caption="Ebuilds para el soporte NFS">
# <i>emerge -a nfs-utils portmap</i>
</pre>

<p>
Se ha de configurar e instalar un núcleo con soporte NFS V3 en todos
los nodos:
</p>

<pre caption="Configuración Requerida por el Núcleo (kernel) para NFS">
CONFIG_NFS_FS=y
CONFIG_NFSD=y
CONFIG_SUNRPC=y
CONFIG_LOCKD=y
CONFIG_NFSD_V3=y
CONFIG_LOCKD_V4=y
</pre>

<p>
En el nodo maestro, se edita el archivo <path>/etc/hosts.allow</path>
para permitir conexiones desde los nodos esclavos. Si la LAN del
<e>cluster</e> está en 192.168.1.0/24, el <path>hosts.allow</path>
debe ser:
</p>

<pre caption="hosts.allow">
portmap:192.168.1.0/255.255.255.0
</pre>

<p>
Se edita el archivo <path>/etc/exports</path> en el nodo maestro para
exportar una estructura de directorios (/home es adecuado para esto).
</p>

<pre caption="/etc/exports">
/home/ *(rw)
</pre>

<p>
Se añade nfs en el nivel de ejecución default del nodo maestro:
</p>

<pre caption="Añadiendo NFS al nivel de ejecución default">
# <i>rc-update add nfs default</i>
</pre>

<p>
Para montar el NFS exportado desde el nodo maestro, se tiene que
configurar <path>/etc/fstab</path> en los nodos esclavo. Ha de
añadirse la siguiente línea:
</p>

<pre caption="/etc/fstab">
master:/home/   /home   nfs     rw,exec,noauto,nouser,async     0 0
</pre>

<p>
También se han de configurar los nodos para que monten el sistema de
archivos nfs con el siguiente comando:
</p>

<pre caption="Añadiendo nfsmount al nivel de ejecución default">
# <i>rc-update add nfsmount default</i>
</pre>
</body>
</section>

<section>
<title>RSH/SSH</title>
<body>

<p>
SSH es un protocolo que proporciona servicios de red seguros, entre
ellos el acceso remoto seguro <e>(login)</e>, en una red
insegura. OpenSSH usa criptografía de una llave pública para asegurar
una autorización segura. Generar la llave pública, que es compartida
con los sistemas remotos, y la llave privada, que se mantiene en el
sistema local, debe hacerse antes de configurar OpenSSH en el
<e>cluster</e>.
</p>

<p>
Para hacer un uso transparente del <e>cluster</e>, pueden usarse las
llaves privadas/públicas. Este proceso se hace en dos pasos:
</p>

<ul>
  <li>Generar llaves públicas y privadas</li>
  <li>Copiar la llave pública en los nodos esclavo</li>
</ul>

<p>
Para autenticación basada en usuario, ha de hacerse lo siguiente:
</p>

<pre caption="Autenticación con llave SSH">
# <i>ssh-keygen -t dsa</i>
Generating public/private dsa key pair.
Enter file in which to save the key (/root/.ssh/id_dsa): /root/.ssh/id_dsa
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_dsa.
Your public key has been saved in /root/.ssh/id_dsa.pub.
The key fingerprint is:
f1:45:15:40:fd:3c:2d:f7:9f:ea:55:df:76:2f:a4:1f root@master

<comment>
¡AVISO! De tener un archivo "authorized_keys" ha de añadirse al mismo,
no usar el siguiente comando.
</comment>

# <i>scp /root/.ssh/id_dsa.pub node01:/root/.ssh/authorized_keys</i>
root@master's password:
id_dsa.pub   100%  234     2.0MB/s   00:00

# <i>scp /root/.ssh/id_dsa.pub node02:/root/.ssh/authorized_keys</i>
root@master's password:
id_dsa.pub   100%  234     2.0MB/s   00:00
</pre>

<note>
Las llaves del anfitrión deben tener una contraseña vacía. Se requiere
RSA para una autenticación basada en el anfitrión.
</note>

<p>
Para una autenticación basada en el anfitrión se necesita editar
también <path>/etc/ssh/shosts.equiv</path>.
</p>

<pre caption="/etc/ssh/shosts.equiv">
node01.adelie
node02.adelie
master.adelie
</pre>

<p>
Y algunas modificaciones al archivo <path>/etc/ssh/sshd_config</path>:
</p>

<pre caption="configuración de sshd">
# $OpenBSD: sshd_config,v 1.42 2001/09/20 20:57:51 mouring Exp $
# This sshd was compiled with PATH=/usr/bin:/bin:/usr/sbin:/sbin

# This is the sshd server system-wide configuration file.  See sshd(8)
# for more information.

# HostKeys for protocol version 2
HostKey /etc/ssh/ssh_host_rsa_key
</pre>

<p>
Si la aplicación requiere comunicaciones RSH, se necesita hacer un
emerge de <c>net-misc/netkit-rsh</c> y <c>sys-apps/xinetd</c>.
</p>

<pre caption="Instalando las aplicaciones necesarias">
# <i>emerge -a xinetd</i>
# <i>emerge -a netkit-rsh</i>
</pre>

<p>
Entonces se ha de configurar el <e>demonio</e> rsh. Editando el
archivo <path>/etc/xinet.d/rsh</path>.
</p>

<pre caption="rsh">
# Adelie Linux Research &amp; Development Center
# /etc/xinetd.d/rsh

service shell
{
        socket_type     = stream
        protocol        = tcp
        wait            = no
        user            = root
        group           = tty
        server          = /usr/sbin/in.rshd
        log_type        = FILE /var/log/rsh
        log_on_success  = PID HOST USERID EXIT DURATION
        log_on_failure  = USERID ATTEMPT
        disable         = no
}
</pre>

<p>
Se edita <path>/etc/hosts.allow</path> para permitir conexiones rsh:
</p>

<pre caption="hosts.allow">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.allow

in.rshd:192.168.1.0/255.255.255.0
</pre>

<p>
O se puede, sencillamente, confiar en la LAN del <e>cluster</e>:
</p>

<pre caption="hosts.allow">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.allow

ALL:192.168.1.0/255.255.255.0
</pre>

<p>
Finalmente, se configura la autenticación del anfitrión en
<path>/etc/hosts.equiv</path>.
</p>

<pre caption="hosts.equiv">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.equiv

master
node01
node02
</pre>

<p>
Y se añade xinetd al nivel de ejecución default:
</p>

<pre caption="Añadiendo xinetd al nivel de ejecución default">
# <i>rc-update add xinetd default</i>
</pre>
</body>
</section>

<section>
<title>NTP</title>
<body>

<p>
El Protocolo de Tiempo de Red (NTP) se usa para sincronizar el tiempo
de una computadora cliente o servidor con otro servidor como fuente de
referencia temporal, como un receptor radio o satélite o
módem. Proporciona ajustes con una diferencia de un milisegundo en una
LAN o de varias decenas de milisegundos en WANs relativas al Tiempo
Universal Coordinado (UTC) vía el receptor del Servicio de
Posicionamiento Global (GPS), por ejemplo. Las configuraciones típicas
de NTP utilizan múltiples servidores redundantes y diversas rutas de
red para alcanzar la máxima sincronización posible.
</p>

<p>
Ha de seleccionarse un servidor NTP geográficamente cercano en
<uri link="http://ntp.isc.org/bin/view/Servers/NTPPoolServers">
Servidores públicos de tiempo NTP</uri>, y configurar los archivos
<path>/etc/conf.d/ntp</path> y <path>/etc/ntp.conf</path> en el
nodo maestro.
</p>

<pre caption="/etc/conf.d/ntp en el nodo maestro">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License v2
# /etc/conf.d/ntpd

# NOTES:
#  - NTPDATE variables below are used if you wish to set your
#    clock when you start the ntp init.d script
#  - make sure that the NTPDATE_CMD will close by itself ...
#    the init.d script will not attempt to kill/stop it
#  - ntpd will be used to maintain synchronization with a time
#    server regardless of what NTPDATE is set to
#  - read each of the comments above each of the variable

# Comment this out if you dont want the init script to warn
# about not having ntpdate setup
NTPDATE_WARN="n"

# Command to run to set the clock initially
# Most people should just uncomment this line ...
# however, if you know what you're doing, and you
# want to use ntpd to set the clock, change this to 'ntpd'
NTPDATE_CMD="ntpdate"

# Options to pass to the above command
# Most people should just uncomment this variable and
# change 'someserver' to a valid hostname which you
# can aquire from the URL's below
NTPDATE_OPTS="-b ntp1.cmc.ec.gc.ca"

##
# A list of available servers is available here:
# http://ntp.isc.org/bin/view/Servers/NTPPoolServers
# Please follow the rules of engagement and use a
# Stratum 2 server (unless you qualify for Stratum 1)
##

# Options to pass to the ntpd process that will *always* be run
# Most people should not uncomment this line ...
# however, if you know what you're doing, feel free to tweak
#NTPD_OPTS=""

</pre>

<p>
Se edita el archivo <path>/etc/ntp.conf</path> en el nodo maestro para
configurar una fuente externa de sincronización:
</p>

<pre caption="ntp.conf en el nodo maestro">
# Adelie Linux Research &amp; Development Center
# /etc/ntp.conf

# Synchronization source #1
server ntp1.cmc.ec.gc.ca
restrict ntp1.cmc.ec.gc.ca
# Synchronization source #2
server ntp2.cmc.ec.gc.ca
restrict ntp2.cmc.ec.gc.ca
stratum 10
driftfile /etc/ntp.drift.server
logfile  /var/log/ntp
broadcast 192.168.1.255
restrict default kod
restrict 127.0.0.1
restrict 192.168.1.0 mask 255.255.255.0
</pre>

<p>
Y en el resto de nodos esclavo, se configura como fuente de
sincronización el nodo maestro.
</p>

<pre caption="/etc/conf.d/ntp en el resto de nodos">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License v2
# /etc/conf.d/ntpd

NTPDATE_WARN="n"
NTPDATE_CMD="ntpdate"
NTPDATE_OPTS="-b master"
</pre>

<pre caption="ntp.conf en un nodo">
# Adelie Linux Research &amp; Development Center
# /etc/ntp.conf

# Synchronization source #1
server master
restrict master
stratum 11
driftfile /etc/ntp.drift.server
logfile  /var/log/ntp
restrict default kod
restrict 127.0.0.1
</pre>

<p>
Después se añade ntpd al nivel de ejecución default en todos los
nodos:
</p>

<pre caption="Añadiendo ntpd al nivel de ejecución default">
# <i>rc-update add ntpd default</i>
</pre>

<note>
NTP no actualizará el reloj local si la diferencia de tiempo es muy
grande con respecto a la fuente de sincronización.
</note>
</body>
</section>

<section>
<title>IPTABLES</title>
<body>

<p>
Para configurar un <e>cortafuego</e> en el <e>cluster</e>, se necesita
iptables.
</p>

<pre caption="Instalando iptables">
# <i>emerge -a iptables</i>
</pre>

<p>
Configuración necesaria del núcleo (kernel):
</p>

<pre caption="Configuración IPtables en el núcleo">
CONFIG_NETFILTER=y
CONFIG_IP_NF_CONNTRACK=y
CONFIG_IP_NF_IPTABLES=y
CONFIG_IP_NF_MATCH_STATE=y
CONFIG_IP_NF_FILTER=y
CONFIG_IP_NF_TARGET_REJECT=y
CONFIG_IP_NF_NAT=y
CONFIG_IP_NF_NAT_NEEDED=y
CONFIG_IP_NF_TARGET_MASQUERADE=y
CONFIG_IP_NF_TARGET_LOG=y
</pre>

<p>
Y las reglas requeridas para este cortafuego:
</p>

<pre caption="rules-save">
# Adelie Linux Research &amp; Development Center
# /var/lib/iptables/rules-save

*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -s 192.168.1.0/255.255.255.0 -i eth1 -j ACCEPT
-A INPUT -s 127.0.0.1 -i lo -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -j LOG
-A INPUT -j REJECT --reject-with icmp-port-unreachable
COMMIT
*nat
:PREROUTING ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A POSTROUTING -s 192.168.1.0/255.255.255.0 -j MASQUERADE
COMMIT
</pre>

<p>
Después se añade ipables al nivel de ejecución default en todos los
nodos:
</p>

<pre caption="Añadiendo iptables al nivel de ejecución default">
# <i>rc-update add iptables default</i>
</pre>
</body>
</section>
</chapter>

<chapter>
<title>Herramientas HPC</title>
<section>
<title>OpenPBS</title>
<body>

<p>
El Sistema de Lotes Portables <e>Portable Batch System (PBS)</e> es un
sistema flexible de puesta en cola de lotes según la carga de trabajo,
originalmente desarrollado para la NASA. Opera en entornos
multi-plataforma UNIX, incluyendo clusters heterogéneos de estaciones
de trabajo, supercomputadoras, y sistemas paralelos masivos. El
desarrollo de PBS está a cargo de Altair Grid Technologies.
</p>

<pre caption="Instalando openpbs">
# <i>emerge -a openpbs</i>
</pre>

<note>
El ebuild OpenPBS no ajusta los permisos adecuadamente en los
directorios var usados por OpenPBS.
</note>

<p>
Antes de empezar a usar OpenPBS, es necesario configurar algunas
cosas. Los archivos necesarios para personalizar el sistema son:
</p>

<ul>
  <li>/etc/pbs_environment</li>
  <li>/var/spool/PBS/server_name</li>
  <li>/var/spool/PBS/server_priv/nodes</li>
  <li>/var/spool/PBS/mom_priv/config</li>
  <li>/var/spool/PBS/sched_priv/sched_config</li>
</ul>

<p>
Aquí tenemos una muestra de sched_config:
</p>

<pre caption="/var/spool/PBS/sched_priv/sched_config">
#
# Create queues and set their attributes.
#
#
# Create and define queue upto4nodes
#
create queue upto4nodes
set queue upto4nodes queue_type = Execution
set queue upto4nodes Priority = 100
set queue upto4nodes resources_max.nodect = 4
set queue upto4nodes resources_min.nodect = 1
set queue upto4nodes enabled = True
set queue upto4nodes started = True
#
# Create and define queue default
#
create queue default
set queue default queue_type = Route
set queue default route_destinations = upto4nodes
set queue default enabled = True
set queue default started = True
#
# Set server attributes.
#
set server scheduling = True
set server acl_host_enable = True
set server default_queue = default
set server log_events = 511
set server mail_from = adm
set server query_other_jobs = True
set server resources_default.neednodes = 1
set server resources_default.nodect = 1
set server resources_default.nodes = 1
set server scheduler_iteration = 60
</pre>

<p>
Para someter una tarea a OpenPBS, se usa el comando <c>qsub</c> con
algunos parámetros opcionales. En el siguiente ejemplo, "-l" permite
especificar los recursos que se requieren, "-j" permite la redirección
de las salidas de error estándar y la salida estándar, y "-m" enviará
un correo electrónico al usuario al principio (b), final (e) y al
cancelar (a) el trabajo.
</p>

<pre caption="Sometiendo una tarea">
<comment>(someter y solicitar que myscript sea ejecutado en 2 nodos)</comment>
# <i>qsub -l nodes=2 -j oe -m abe myscript</i>
</pre>

<p>
Normalmente, las tareas sometidas a OpenPBS están en forma de
macros. A veces, se puede intentar una tarea manualmente. Para
solicitar un intérprete de comandos interactivo desde OpenPBS, se usa
el parámetro "-I".
</p>

<pre caption="Solicitando un intérprete de comandos interactivo">
# <i>qsub -I</i>
</pre>

<p>
Para comprobar el estado de las tareas, se usa el comando qstat:
</p>

<pre caption="Comprobando el estado de las tareas">
# <i>qstat</i>
Job id  Name  User   Time Use S Queue
------  ----  ----   -------- - -----
2.geist STDIN adelie 0        R upto1nodes
</pre>
</body>
</section>

<section>
<title>MPICH</title>
<body>

<p>
Pasar Mensajes es un paradigma usado ampliamente en ciertas clases de
máquinas en paralelo, especialmente en aquellas con memoria
distribuida.  MPICH es una implementación portable de MPI disponible
libremente. El estándar para las librerías pasar-mensajes.
</p>

<p>
El ebuild de mpich proporcionado por Adelie Linux permite dos
parámetros USE: <e>doc</e> y <e>crypt</e>. <e>doc</e> hará que la
documentación sea instalada, mientras que <e>crypt</e> configurará
MPICH para usar <c>ssh</c> en lugar de <c>rsh</c>.
</p>

<pre caption="Instalando la aplicación mpich">
# <i>emerge -a mpich</i>
</pre>

<p>
Se debe exportar un directorio de trabajo mpich a todos los nodos
esclavo en <path>/etc/exports</path>:
</p>

<pre caption="/etc/exports">
/home *(rw)
</pre>

<p>
La mayoría de sistemas de procesamiento en paralelo masivo (MPPs)
proporcionan una forma de iniciar un programa en un número determinado
de procesadores; <c>mpirun</c> hace uso del comando adecuado siempre
que sea posible. En contraste, los <e>clusters</e> de estaciones de
trabajo requieren que cada proceso de un trabajo en paralelo sea
iniciado individualmente, aunque programas para iniciar estos procesos
existen. Debido a que los <e>clusters</e> con estaciones de trabajo no
están organizados como MPP, se requiere información adicional para
hacer uso de ellos. Mpich debe ser instalado con una lista de las
estaciones de trabajo en el archivo <path>machines.LINUX</path> del
directorio <path>/usr/share/mpich/</path>.  Este archivo es utilizado
por <c>mpirun</c> para elegir los procesadores en los que se ejecuta.
</p>

<p>
Se edita este archivo para reflejar la configuración del
<e>cluster-lan</e>:
</p>

<pre caption="/usr/share/mpich/machines.LINUX">
# Change this file to contain the machines that you want to use
# to run MPI jobs on.  The format is one host name per line, with either
#    hostname
# or
#    hostname:n
# where n is the number of processors in an SMP.  The hostname should
# be the same as the result from the command "hostname"
master
node01
node02
# node03
# node04
# ...
</pre>

<p>
Se usa el macro <c>tstmachines</c> en <path>/usr/sbin/</path> para
asegurarse de que se pueden usar todas las máquinas de la lista. Esta
macro realiza un <c>rsh</c> y un breve listado de directorio; con lo
cual se comprueba tanto que se tiene acceso el nodo como que el
programa en el directorio puede verse en el nodo remoto. De haber
algún problema, será comunicado. Estos problemas deben resolverse
antes de proceder.
</p>

<p>
El único argumento para <c>tstmachines</c> es el nombre de la
arquitectura; es el mismo nombre que el del archivo machines. Por
ejemplo, lo siguiente comprueba si un programa en el directorio actual
puede ser ejecutado por todas las máquinas en la lista LINUX.
</p>

<pre caption="Haciendo un test">
# <i>/usr/local/mpich/sbin/tstmachines LINUX</i>
</pre>

<note>
Este programa es silencioso si todo va bien; si quiere verse lo que
está haciendo, se ha de usar el argumento -v (modo detallado):
</note>

<pre caption="Haciendo un test en modo detallado">
# <i>/usr/local/mpich/sbin/tstmachines -v LINUX</i>
</pre>

<p>
La salida de este comando podría ser:
</p>

<pre caption="Mensajes del comando anterior">
Trying true on host1.uoffoo.edu ...
Trying true on host2.uoffoo.edu ...
Trying ls on host1.uoffoo.edu ...
Trying ls on host2.uoffoo.edu ...
Trying user program on host1.uoffoo.edu ...
Trying user program on host2.uoffoo.edu ...
</pre>

<p>
Si <c>tstmachines</c> encuentra un problema, sugerirá las posibles
razones y las soluciones. Resumidamente, he aquí tres mensajes:
</p>

<ul>
  <li>
    <e>¿Se pueden iniciar los procesos en las máquinas remotas?</e>
    tstmachines intenta ejecutar el comando de shel true en cada
    máquina que aparece en el archivo de máquinas usando un
    intérprete de comandos remoto.
  </li>
  <li>
    <e>¿Está disponible el directorio de trabajo para todas las
    máquinas?</e> Esto intenta hacer un ls de un archivo que crea
    tstmachines ejecutándolo desde un intérprete de comandos remoto.
  </li>
  <li>
    <e>¿Se pueden ejecutar programas de usuario en los sistemas
    remotos?</e> Esto comprueba que las librerías compartidas y otros
    componentes han sido correctamente instalados en todas las
    máquinas.
  </li>
</ul>

<p>
Y el test requerido para cada herramienta de desarrollo:
</p>

<pre caption="Comprobando una herramienta de desarrollo">
# <i>cd ~</i>
# <i>cp /usr/share/mpich/examples1/hello++.c ~</i>
# <i>make hello++</i>
# <i>mpirun -machinefile /usr/share/mpich/machines.LINUX -np 1 hello++</i>
</pre>

<p>
Para mayor información acerca de MPICH, puede consultarse la
documentación de <uri
link="http://www-unix.mcs.anl.gov/mpi/mpich/docs/mpichman-chp4/mpichman-chp4.htm">
http://www-unix.mcs.anl.gov/mpi/mpich/docs/mpichman-chp4/mpichman-chp4.htm
</uri>.
</p>
</body>
</section>

<section>
<title>LAM</title>
<body>

<p>
(¡Pronto Disponible!)
</p>
</body>
</section>

<section>
<title>OMNI</title>
<body>

<p>
(¡Pronto Disponible!)
</p>
</body>
</section>
</chapter>

<chapter>
<title>Bibliografía</title>
<section>
<body>

<p>
El documento original se publicó en la página del <uri
link="http://www.adelielinux.com">Centro Adelie Linux R&amp;D</uri>, y
se ha reproducido aquí con el permiso de los autores y del <uri
link="http://www.cyberlogic.ca">Centro Cyberlogic</uri> de Adelie
Linux R&amp;D.
</p>

<ul>
  <li><uri>http://www.gentoo.org</uri>, Gentoo Technologies,
  Inc.</li>
  <li>
    <uri link="http://www.adelielinux.com">http://www.adelielinux.com</uri>,
    Adelie Linux Research and Development Centre
  </li>
  <li>
    <uri
    link="http://nfs.sourceforge.net/">http://nfs.sourceforge.net</uri>,
    Linux NFS Project
  </li>
  <li>
    <uri
    link="http://www-unix.mcs.anl.gov/mpi/mpich/">
    http://www-unix.mcs.anl.gov/mpi/mpich/</uri>,
    Mathematics and Computer Science Division, Argonne National
    Laboratory
  </li>
  <li>
    <uri link="http://www.ntp.org/">http://ntp.org</uri>
  </li>
  <li>
    <uri
    link="http://www.eecis.udel.edu/~mills/">
    http://www.eecis.udel.edu/~mills/</uri>,
    David L. Mills, University of Delaware
  </li>
  <li>
    <uri
    link="http://www.ietf.org/html.charters/secsh-charter.html">
    http://www.ietf.org/html.charters/secsh-charter.html</uri>,
    Secure Shell Working Group, IETF, Internet Society
  </li>
  <li>
    <uri link="http://www.linuxsecurity.com/">
    http://www.linuxsecurity.com/</uri>,
    Guardian Digital
  </li>
  <li>
    <uri link="http://www.openpbs.org/">http://www.openpbs.org/</uri>,
    Altair Grid Technologies, LLC.
  </li>
</ul>
</body>
</section>
</chapter>
</guide>
