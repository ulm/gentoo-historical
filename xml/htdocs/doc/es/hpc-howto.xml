<?xml version='1.0' encoding="UTF-8"?>

<!-- $Header: /var/cvsroot/gentoo/xml/htdocs/doc/es/hpc-howto.xml,v 1.5 2005/10/07 20:45:45 chiguire Exp $ -->

<!DOCTYPE guide SYSTEM "/dtd/guide.dtd">
<guide link="/doc/es/hpc-howto.xml" lang="es">

<title>Computación de Alto Rendimiento con Gentoo Linux</title>

<author title="Autor">
  <mail link="marc@adelielinux.com">Marc St-Pierre</mail>
</author>
<author title="Autor">
  <mail link="benoit@adelielinux.com">Benoit Morin</mail>
</author>
<author title="Asistente">
  <mail link="jean-francois@adelielinux.com">Jean-Francois Richard</mail>
</author>
<author title="Asistente">
  <mail link="olivier@adelielinux.com">Olivier Crete</mail>
</author>
<author title="Revisor">
  <mail link="spyderous@gentoo.org">Donnie Berkholz</mail>
</author>
<author title="Editor Es">
  <mail link="bass@gentoo.org">José Alberto Suárez López</mail>
</author>
<author title="Editor Es">
    <mail link="chiguire@gentoo.org">John Christian Stoddart</mail>
</author>
<author title="Editor Es">
    <mail link="yoswink@gentoo.org">José Luis Rivero</mail>
</author>
<author title="Traductor">
  <mail link="LinuxBlues@gentoo-es.org">Fernando M. Bueno</mail>
</author>

<!-- Sin información sobre la licencia; este documento ha sido escrito por
     terceras partes, sin información adicional sobre la licencia desde esta
     organización.
     En otras palabras, tiene copyright de adelielinux R&D; Gentoo solo
     tiene permiso para distribuir este documento tal cual y actualizarlo
     cuando sea apropiado de acuerdo con la notificación de adelie linux R&D.
-->
     
<abstract>
Este documento fue escrito por el personal del Centro Adelie Linux R&amp;D
&lt;http://www.adelielinux.com&gt; como una guía paso a paso para hacer de
Gentoo un sistema de computación de alto rendimiento (HPC).
</abstract>

<version>1.2</version>
<date>2003-08-01</date>

<chapter>
<title>Introducción</title>
<section>
<body>

<p>
Gentoo Linux, es un sabor especial de Linux que puede ser automáticamente
optimizado para cualquier aplicación o necesidad. Rendimiento extremo,
configurabilidad y una comunidad de usuarios y desarrolladores de la mayor
calidad imaginable son característicos de la experiencia Gentoo.
</p>

<p>
Gracias a una tecnología llamada Portage, Gentoo Linux puede llegar a ser
un servidor seguro ideal, una estación de trabajo, un escritorio profesional,
un sistema de juegos, una solución que las integre o ... un sistema de
computación de alto rendimiento. Debido a su adaptabilidad casi ilimitada,
llamamos a Gentoo Linux una metadistribución.
</p>

<p>
Este documento explica cómo hacer de Gentoo un sistema de computación
de alto rendimiento. Explica paso a paso los paquetes a instalar y ayuda
a configurarlos.
</p>

<p>
Gentoo Linux puede obtenerse desde <uri>http://www.gentoo.org</uri>, y
puede obtenerse también la <uri link="/doc/es/">documentación</uri> para
instalarlo.
</p>
</body>
</section>
</chapter>

<chapter>
<title>Configurando Gentoo Linux para Clustering</title>
<section>
<title>Optimizaciones Recomendadas</title>
<body>

<note>
Clustering: grupo de ordenadores realizando una misma función.
</note>

<note>
En esta sección nos referimos al <uri link="/doc/es/handbook/">Manual Gentoo</uri>.
</note>

<p>
Durante el proceso de instalación, deben ajustarse los  parámetros USE en
<path>/etc/make.conf</path>. Recomendamos que se desactiven los valores
por defecto (vease <path>/etc/make.profile/make.defaults</path>) negándolos
en make.conf. De cualquier forma, pueden conservarse variables como x86, 3dnow, 
gpm, mmx, sse, ncurses, pam y tcpd. Consultar la documentación USE para
más información.
</p>

<pre caption="Parámetros USE">
# Copyright 2000-2003 Daniel Robbins, Gentoo Technologies, Inc.
# Contains local system settings for Portage system

# Please review 'man make.conf' for more information.

USE="-oss 3dnow -apm -arts -avi -berkdb -crypt -cups -encode -gdbm
-gif gpm -gtk -imlib -java -jpeg -kde -gnome -libg++ -libwww -mikmod
mmx -motif -mpeg ncurses -nls -oggvorbis -opengl pam -pdflib -png
-python -qt -qtmt -quicktime -readline -sdl -slang -spell -ssl
-svga tcpd -truetype -X -xml2 -xmms -xv -zlib"
</pre>

<p>
O sencillamente:
</p>

<pre caption="versión simplificada - Parámetros USE">
# Copyright 2000-2003 Daniel Robbins, Gentoo Technologies, Inc.
# Contains local system settings for Portage system

# Please review 'man make.conf' for more information.

USE="-* 3dnow gpm mmx ncurses pam sse tcpd"
</pre>

<note>
El parámetro USE <e>tcpd</e> incrementa la seguridad de paquetes como xinetd.
</note>

<p>
En el paso 15 ("Instalando el núcleo y los registros del sistema"), por
razones de estabilidad, recomendamos las fuentes vanilla (vanilla-sources),
el código fuente oficial del núcleo realizado en <uri>http://www.kernel.org/</uri>,
a menos que se requiera soporte específico, como el de xfs.
</p>

<pre caption="Instalando las fuentes vanilla">
# <i>emerge -p syslog-ng vanilla-sources</i>
</pre>

<p>
Recomendamos instalar los siguientes paquetes:
</p>

<pre caption="Instalando los paquetes necesarios">
# <i>emerge -p nfs-utils portmap tcpdump ssmtp iptables xinetd</i>
</pre>
</body>
</section>

<section>
<title>Capa de Comunicación (TCP/IP Network)</title>
<body>

<p>
Un grupo de computadoras <e>(cluster)</e> requiere una capa de comunicación que
permita la comunicación entre los nodos esclavos y el nodo maestro.
Normalmente, LAN: FastEthernet o GigaEthernet, pueden ser usados, dado que
tienen una buena relación precio/rendimiento. Otras posibilidades incluyen
el uso de productos como <uri link="http://www.myricom.com/">Myrinet</uri>, <uri 
link="http://quadrics.com/">QsNet</uri> u otros.
</p>

<p>
Un cluster se compone de dos tipos de nodo: maestro y esclavo. Normalmente,
un cluster dispone de un nodo maestro y varios nodos esclavos.
</p>

<p>
El nodo maestro es el servidor del cluster. Es el responsable de decirle a
los nodos esclavo qué deben hacer. El servidor ejecutará <e>demonios</e> como dhcpd,
nfs, pbs-server, y pbs-sched. El nodo maestro permitirá sesiones interactivas
de usuarios y aceptará la ejecución de trabajos.
</p>

<p>
Los nodos esclavo escuchan las instrucciones (quizá vía ssh/rsh) desde el nodo
maestro. Deben dedicarse a proporcionar resultados y, por tanto, no deberían
ejecutar ningún servicio innecesario.
</p>

<p>
El resto de este documento asume un cluster cuya configuración es la que se
proporciona en el siguiente archivo <e>hosts</e>. Debería mantenerse cada nodo en el
archivo <path>/etc/hosts</path> con entradas para cada uno de los nodos que
participe en el cluster.
</p>

<pre caption="/etc/hosts">
# Adelie Linux Research &amp; Development Center
# /etc/hosts

127.0.0.1	localhost

192.168.1.100	master.adelie master

192.168.1.1	node01.adelie node01
192.168.1.2	node02.adelie node02
</pre>

<p>
Para configurar la LAN dedicada al <e>cluster</e>, se edita el archivo
<path>/etc/conf.d/net</path> en el nodo maestro.
</p>

<pre caption="/etc/conf.d/net">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License, v2 or later

# Global config file for net.* rc-scripts

# This is basically the ifconfig argument without the ifconfig $iface
#

iface_eth0="192.168.1.100 broadcast 192.168.1.255 netmask 255.255.255.0"
# Network Connection to the outside world using dhcp -- configure as required for you network
iface_eth1="dhcp"
</pre>


<p>
Finalmente, se configura un servidor DHCP en el nodo maestro para evitar tener
que mantener una configuración de red en cada uno de los nodos esclavo.
</p>

<pre caption="/etc/dhcp/dhcpd.conf">
# Adelie Linux Research &amp; Development Center
# /etc/dhcp/dhcpd.conf

log-facility local7;
ddns-update-style none;
use-host-decl-names on;

subnet 192.168.1.0 netmask 255.255.255.0 {
        option domain-name "adelie";
        range 192.168.1.10 192.168.1.99;
        option routers 192.168.1.100;

        host node01.adelie {
		# MAC address of network card on node 01
                hardware ethernet 00:07:e9:0f:e2:d4;
                fixed-address 192.168.1.1;
        }
        host node02.adelie {
		# MAC address of network card on node 02
                hardware ethernet 00:07:e9:0f:e2:6b;
                fixed-address 192.168.1.2;
        }
}
</pre>
</body>
</section>

<section>
<title>NFS/NIS</title>
<body>

<p>
El sistema de archivos de red (NFS) se desarrolló para permitir a las máquinas
montar la partición de un disco en una máquina remota como si estuviese en un
disco duro local. Esto permite compartir ficheros a través de una red de forma
rápida y eficaz.
</p>

<p>
Hay otros sistemas que proporcionan una funcionalidad similar a NFS que pueden
usarse en entornos <e>cluster</e>. El <uri link="http://www.openafs.org">Andrew
File System de IBM</uri>, recientemente de código abierto, proporciona un
mecanismo que permite compartir archivos con características adicionales de
seguridad y rendimiento. El <uri 
link="http://www.coda.cs.cmu.edu/">Sistema de archivos Coda</uri> se está
desarrollando todavía, pero está diseñado para trabajar bien, aún con clientes
desconectados. Muchas de las características de los sistemas de archivos
Andrew y Coda se incluirán en la siguiente versión de <uri link="http://www.nfsv4.org">NFS (Versión 4)</uri>. La ventajas actuales de NFS son que es un sistema maduro,
estándar, bien entendido y soportado robustamente en una extensa variedad
de plataformas.
</p>

<pre caption="Ebuilds para el soporte NFS">
# <i>emerge -p nfs-utils portmap</i>
# <i>emerge nfs-utils portmap</i>
</pre>

<p>
Se ha de configurar e instalar un núcleo con soporte NFS V3 en todos los nodos:
</p>

<pre caption="Configuración Requerida por el Núcleo (kernel) para NFS">
CONFIG_NFS_FS=y
CONFIG_NFSD=y
CONFIG_SUNRPC=y
CONFIG_LOCKD=y
CONFIG_NFSD_V3=y
CONFIG_LOCKD_V4=y
</pre>

<p>
En el nodo maestro, se edita el archivo <path>/etc/hosts.allow</path> para permitir
conexiones desde los nodos esclavos. Si la LAN del <e>cluster</e> está en 192.168.1.0/24,
el <path>hosts.allow</path> debe ser:
</p>

<pre caption="hosts.allow">
portmap:192.168.1.0/255.255.255.0
</pre>

<p>
Se edita el archivo <path>/etc/exports</path> en el nodo maestro para exportar una
estructura de directorios (/home es adecuado para esto).
</p>

<pre caption="/etc/exports">
/home/	*(rw)
</pre>

<p>
Se añade nfs en el nivel de ejecución default del nodo maestro:
</p>

<pre caption="Añadiendo NFS al nivel de ejecución default">
# <i>rc-update add nfs default</i>
</pre>

<p>
Para montar el nfs exportado desde el nodo maestro, se tiene que configurar
<path>/etc/fstab</path> en los nodos esclavo. Ha de añadirse la siguiente línea:
</p>

<pre caption="/etc/fstab">
master:/home/	/home	nfs	rw,exec,noauto,nouser,async	0 0
</pre>

<p>
También se han de configurar los nodos para que monten el sistema de archivos
nfs con el siguiente comando:
</p>

<pre caption="Añadiendo nfsmount al nivel de ejecución default">
# <i>rc-update add nfsmount default</i>
</pre>
</body>
</section>

<section>
<title>RSH/SSH</title>
<body>

<p>
SSH es un protocolo que proporciona servicios de red seguros, entre ellos el
acceso remoto seguro <e>(login)</e>, en una red insegura. OpenSSH usa criptografía
de una llave pública para asegurar una autorización segura. Generar la llave
pública, que es compartida con los sistemas remotos, y la llave privada, que
se mantiene en el sistema local, debe hacerse antes de configurar OpenSSH en
el <e>cluster</e>.
</p>

<p>
Para hacer un uso transparente del <e>cluster</e>, pueden usarse las llaves
privadas/públicas. Este proceso se hace en dos pasos:
</p>

<ul>
  <li>Generar llaves públicas y privadas</li>
  <li>Copiar la llave pública en los nodos esclavo</li>
</ul>

<p>
Para autenticación basada en usuario, ha de hacerse lo siguiente:
</p>

<pre caption="Autenticación con llave SSH">
# <i>ssh-keygen -t dsa</i>
Generating public/private dsa key pair.
Enter file in which to save the key (/root/.ssh/id_dsa): /root/.ssh/id_dsa
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_dsa.
Your public key has been saved in /root/.ssh/id_dsa.pub.
The key fingerprint is:
f1:45:15:40:fd:3c:2d:f7:9f:ea:55:df:76:2f:a4:1f root@master

<comment>
¡AVISO! De tener un archivo "authorized_keys" ha de añadirse al mismo,
no usar el siguiente comando.
</comment>

# <i>scp /root/.ssh/id_dsa.pub node01:/root/.ssh/authorized_keys</i>
root@master's password:
id_dsa.pub   100%  234     2.0MB/s   00:00

# <i>scp /root/.ssh/id_dsa.pub node02:/root/.ssh/authorized_keys</i>
root@master's password:
id_dsa.pub   100%  234     2.0MB/s   00:00
</pre>

<note>
Las llaves del anfitrion deben tener una contraseña vacía. Se requiere
RSA para una autenticación basada en el anfitrion.
</note>

<p>
Para una autenticación basada en el anfitrión se necesita editar también
<path>/etc/ssh/shosts.equiv</path>.
</p>

<pre caption="/etc/ssh/shosts.equiv">
node01.adelie
node02.adelie
master.adelie
</pre>

<p>
Y algunas modificaciones al archivo <path>/etc/ssh/sshd_config</path>:
</p>

<pre caption="configuración de sshd">
# $OpenBSD: sshd_config,v 1.42 2001/09/20 20:57:51 mouring Exp $
# This sshd was compiled with PATH=/usr/bin:/bin:/usr/sbin:/sbin

# This is the sshd server system-wide configuration file.  See sshd(8)
# for more information.

# HostKeys for protocol version 2
HostKey /etc/ssh/ssh_host_rsa_key
</pre>

<p>
Si la aplicación requiere comunicaciones RSH, se necesita hacer un emerge
de net-misc/netkit-rsh y sys-apps/xinetd.
</p>

<pre caption="Instalando las aplicaciones necesarias">
# <i>emerge -p xinetd</i>
# <i>emerge xinetd</i>
# <i>emerge -p netkit-rsh</i>
# <i>emerge netkit-rsh</i>
</pre>

<p>
Entonces se ha de configurar el <e>demonio</e> rsh. Editando el archivo <path>/etc/xinet.d/rsh</path>.
</p>

<pre caption="rsh">
# Adelie Linux Research &amp; Development Center
# /etc/xinetd.d/rsh

service shell
{
        socket_type     = stream
        protocol        = tcp
        wait            = no
        user            = root
        group           = tty
        server          = /usr/sbin/in.rshd
        log_type        = FILE /var/log/rsh
        log_on_success  = PID HOST USERID EXIT DURATION
        log_on_failure  = USERID ATTEMPT
        disable         = no
}
</pre>

<p>
Se edita <path>/etc/hosts.allow</path> para permitir conexiones rsh:
</p>

<pre caption="hosts.allow">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.allow

in.rshd:192.168.1.0/255.255.255.0
</pre>

<p>
O se puede, sencillamente, confiar en la LAN del <e>cluster</e>:
</p>

<pre caption="hosts.allow">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.allow     

ALL:192.168.1.0/255.255.255.0
</pre>

<p>
Finalmente, se configura la autenticación del anfitrión en <path>/etc/hosts.equiv</path>.
</p>

<pre caption="hosts.equiv">
# Adelie Linux Research &amp; Development Center
# /etc/hosts.equiv

master
node01
node02
</pre>

<p>
Y se añade xinetd al nivel de ejecución default:
</p>

<pre caption="Añadiendo xinetd al nivel de ejecución default">
# <i>rc-update add xinetd default</i>
</pre>
</body>
</section>

<section>
<title>NTP</title>
<body>

<p>
El Protocolo de Tiempo de Red (NTP) se usa para sincronizar el tiempo de una
computadora cliente o servidor con otro servidor como fuente de referencia
temporal, como un receptor radio o satélite o modem. Proporciona ajustes con
una diferencia de un milisegundo en una LAN o de varias decenas de
milisegundos en WANs relativas al Tiempo Universal Coordinado (UTC) vía el
receptor del Servicio de Posicionamiento Global (GPS), por ejemplo. Las
configuraciones típicas de NTP utilizan múltiples servidores redundantes
y diversas rutas de red para alcanzar la máxima sincronización posible.
</p>

<p>
Ha de seleccionarse un servidor NTP geográficamente cercano en
<uri link="http://ntp.isc.org/bin/view/Servers/NTPPoolServers">Servidores
públicos de tiempo NTP</uri>, y configurar los archivos <path>/etc/conf.d/ntp</path>
y <path>/etc/ntp.conf</path> en el nodo maestro.
</p>

<pre caption="/etc/conf.d/ntp en el nodo maestro">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License v2
# /etc/conf.d/ntpd

# NOTES:
#  - NTPDATE variables below are used if you wish to set your
#    clock when you start the ntp init.d script
#  - make sure that the NTPDATE_CMD will close by itself ...
#    the init.d script will not attempt to kill/stop it
#  - ntpd will be used to maintain synchronization with a time
#    server regardless of what NTPDATE is set to
#  - read each of the comments above each of the variable

# Comment this out if you dont want the init script to warn
# about not having ntpdate setup
NTPDATE_WARN="n"

# Command to run to set the clock initially
# Most people should just uncomment this line ...
# however, if you know what you're doing, and you
# want to use ntpd to set the clock, change this to 'ntpd'
NTPDATE_CMD="ntpdate"

# Options to pass to the above command
# Most people should just uncomment this variable and
# change 'someserver' to a valid hostname which you
# can aquire from the URL's below
NTPDATE_OPTS="-b ntp1.cmc.ec.gc.ca"

##
# A list of available servers is available here:
# http://ntp.isc.org/bin/view/Servers/NTPPoolServers
# Please follow the rules of engagement and use a
# Stratum 2 server (unless you qualify for Stratum 1)
##

# Options to pass to the ntpd process that will *always* be run
# Most people should not uncomment this line ...
# however, if you know what you're doing, feel free to tweak
#NTPD_OPTS=""

</pre>

<p>
Se edita el archivo <path>/etc/ntp.conf</path> en el nodo maestro para configurar
una fuente externa de sincronización:
</p>

<pre caption="ntp.conf en el nodo maestro">
# Adelie Linux Research &amp; Development Center
# /etc/ntp.conf

# Synchronization source #1
server ntp1.cmc.ec.gc.ca
restrict ntp1.cmc.ec.gc.ca
# Synchronization source #2
server ntp2.cmc.ec.gc.ca
restrict ntp2.cmc.ec.gc.ca
stratum 10
driftfile /etc/ntp.drift.server
logfile  /var/log/ntp
broadcast 192.168.1.255
restrict default kod
restrict 127.0.0.1
restrict 192.168.1.0 mask 255.255.255.0
</pre>

<p>
Y en el resto de nodos esclavo, se configura como fuente de sincronización el
nodo maestro.
</p>

<pre caption="/etc/conf.d/ntp en el resto de nodos">
# Copyright 1999-2002 Gentoo Technologies, Inc.
# Distributed under the terms of the GNU General Public License v2
# /etc/conf.d/ntpd

NTPDATE_WARN="n"
NTPDATE_CMD="ntpdate"
NTPDATE_OPTS="-b master"
</pre>

<pre caption="ntp.conf en un nodo">
# Adelie Linux Research &amp; Development Center
# /etc/ntp.conf

# Synchronization source #1
server master
restrict master
stratum 11
driftfile /etc/ntp.drift.server
logfile  /var/log/ntp
restrict default kod
restrict 127.0.0.1
</pre>

<p>
Después se añade ntpd al nivel de ejecución default en todos los nodos:
</p>

<pre caption="Añadiendo ntpd al nivel de ejecución default">
# <i>rc-update add ntpd default</i>
</pre>

<note>
NTP no actualizará el reloj local si la diferencia de tiempo es muy grande
con respecto a la fuente de sincronización.
</note>
</body>
</section>

<section>
<title>IPTABLES</title>
<body>

<p>
Para configurar un <e>cortafuego</e> en el <e>cluster</e>, se necesita iptables.
</p>

<pre caption="Instalando iptables">
# <i>emerge -p iptables</i>
# <i>emerge iptables</i>
</pre>

<p>
Configuración necesaria del núcleo (kernel):
</p>

<pre caption="Configuración IPtables en el núcleo">
CONFIG_NETFILTER=y
CONFIG_IP_NF_CONNTRACK=y
CONFIG_IP_NF_IPTABLES=y
CONFIG_IP_NF_MATCH_STATE=y
CONFIG_IP_NF_FILTER=y
CONFIG_IP_NF_TARGET_REJECT=y
CONFIG_IP_NF_NAT=y
CONFIG_IP_NF_NAT_NEEDED=y
CONFIG_IP_NF_TARGET_MASQUERADE=y
CONFIG_IP_NF_TARGET_LOG=y
</pre>

<p>
Y las reglas requeridas para este cortafuego:
</p>

<pre caption="rules-save">
# Adelie Linux Research &amp; Development Center
# /var/lib/iptables/rules-save

*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -s 192.168.1.0/255.255.255.0 -i eth1 -j ACCEPT
-A INPUT -s 127.0.0.1 -i lo -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -j LOG
-A INPUT -j REJECT --reject-with icmp-port-unreachable
COMMIT
*nat
:PREROUTING ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A POSTROUTING -s 192.168.1.0/255.255.255.0 -j MASQUERADE
COMMIT
</pre>

<p>
Después se añade ipables al nivel de ejecución default en todos los nodos:
</p>

<pre caption="Añadiendo iptables al nivel de ejecución default">
# <i>rc-update add iptables default</i>
</pre>
</body>
</section>
</chapter>

<chapter>
<title>Herramientas HPC</title>
<section>
<title>OpenPBS</title>
<body>

<p>
El Sistema de Lotes Portables <e>Portable Batch System (PBS)</e> es un sistema flexible
de puesta en cola de lotes según la carga de trabajo, originalmente desarrollado para
la NASA. Opera en entornos multi-plataforma UNIX, incluyendo clusters heterogéneos
de estaciones de trabajo, supercomputadoras, y sistemas paralelos masivos. El desarrollo
de PBS está a cargo de Altair Grid Technologies.
</p>

<pre caption="Instalando openpbs">
# <i>emerge -p openpbs</i>
# <i>emerge openpbs</i>
</pre>

<note>
El ebuild OpenPBS no ajusta los permisos adecuadamente en los directorios var
usados por OpenPBS.
</note>

<p>
Antes de empezar a usar OpenPBS, es necesario configurar algunas cosas. Los
archivos necesarios para personalizar el sistema son:
</p>

<ul>
	<li>/etc/pbs_environment</li>
	<li>/var/spool/PBS/server_name</li>
	<li>/var/spool/PBS/server_priv/nodes</li>
	<li>/var/spool/PBS/mom_priv/config</li>
	<li>/var/spool/PBS/sched_priv/sched_config</li>
</ul>

<p>
Aquí tenemos una muestra de sched_config:
</p>

<pre caption="/var/spool/PBS/sched_priv/sched_config">
#
# Create queues and set their attributes.
#
#
# Create and define queue upto4nodes
#
create queue upto4nodes
set queue upto4nodes queue_type = Execution
set queue upto4nodes Priority = 100
set queue upto4nodes resources_max.nodect = 4
set queue upto4nodes resources_min.nodect = 1
set queue upto4nodes enabled = True
set queue upto4nodes started = True
#
# Create and define queue default
#
create queue default
set queue default queue_type = Route
set queue default route_destinations = upto4nodes
set queue default enabled = True
set queue default started = True
#
# Set server attributes.
#
set server scheduling = True
set server acl_host_enable = True
set server default_queue = default
set server log_events = 511
set server mail_from = adm
set server query_other_jobs = True
set server resources_default.neednodes = 1
set server resources_default.nodect = 1
set server resources_default.nodes = 1
set server scheduler_iteration = 60
</pre>

<p>
Para someter una tarea a OpenPBS, se usa el comando <c>qsub</c> con algunos
parámetros opcionales. En el siguiente ejemplo, "-l" permite especificar los
recursos que se requieren, "-j" permite la redirección de las salidas de error
estándar y la salida estándar, y "-m" enviará un correo electrónico al usuario
al principio (b), final (e) y al cancelar (a) el trabajo.
</p>

<pre caption="Sometiendo una tarea">
<comment>(someter y solicitar que myscript sea ejecutado en 2 nodos)</comment>
# <i>qsub -l nodes=2 -j oe -m abe myscript</i>
</pre>

<p>
Normalmente, las tareas sometidas a OpenPBS están en forma de macros. A veces,
se puede intentar una tarea manualmente. Para solicitar un intérprete de comandos
interactivo desde OpenPBS, se usa el parámetro "-I".
</p>

<pre caption="Solicitando un intérprete de comandos interactivo">
# <i>qsub -I</i>
</pre>

<p>
Para comprobar el estado de las tareas, se usa el comando qstat:
</p>

<pre caption="Comprobando el estado de las tareas">
# <i>qstat</i>
Job id  Name  User   Time Use S Queue
------  ----  ----   -------- - -----
2.geist STDIN adelie 0        R upto1nodes
</pre>
</body>
</section>

<section>
<title>MPICH</title>
<body>

<p>
Pasar Mensajes es un paradigma usado ampliamente en ciertas clases de
máquinas en paralelo, especialmente en aquellas con memoria distribuida.
MPICH es una implementación portable de MPI disponible libremente. El
estándar para las librerías pasar-mensajes.
</p>

<p>
El ebuild de mpich proporcionado por Adelie Linux permite dos parámetros USE:  
<e>doc</e> y <e>crypt</e>. <e>doc</e> hará que la documentación sea
instalada, mientras que <e>crypt</e> configurará MPICH para usar <c>ssh</c>
en lugar de <c>rsh</c>.
</p>

<pre caption="Instalando la aplicación mpich">
# <i>emerge -p mpich</i>
# <i>emerge mpich</i>
</pre>

<p>
Se debe exportar un directorio de trabajo mpich a todos los nodos esclavo
en <path>/etc/exports</path>:
</p>

<pre caption="/etc/exports">
/home	*(rw)
</pre>

<p>
La mayoría de sistemas de procesamiento en paralelo masivo (MPPs)
proporcionan una forma de iniciar un programa en un número determinado de
procesadores; <c>mpirun</c> hace uso del comando adecuado siempre que
sea posible. En contraste, los <e>clusters</e> de estaciones de trabajo requieren
que cada proceso de un trabajo en paralelo sea iniciado individualmente,
aunque programas para iniciar estos procesos existen. Debido a que los
<e>clusters</e> con estaciones de trabajo no están organizados como MPP, se
requiere información adicional para hacer uso de ellos. Mpich debe ser
instalado con una lista de las estaciones de trabajo en el archivo
<path>machines.LINUX</path> del directorio <path>/usr/share/mpich/</path>.
Este archivo es utilizado por <c>mpirun</c> para elegir los procesadores
en los que se ejecuta.
</p>

<p>
Se edita este archivo para reflejar la configuración del <e>cluster-lan</e>:
</p>

<pre caption="/usr/share/mpich/machines.LINUX">
# Change this file to contain the machines that you want to use
# to run MPI jobs on.  The format is one host name per line, with either
#    hostname
# or
#    hostname:n
# where n is the number of processors in an SMP.  The hostname should
# be the same as the result from the command "hostname"
master
node01
node02
# node03
# node04
# ...
</pre>

<p>
Se usa el macro <c>tstmachines</c> en <path>/usr/sbin/</path> para asegurarse
de que se pueden usar todas las máquinas de la lista. Esta macro realiza un
<c>rsh</c> y un breve listado de directorio; con lo cual se comprueba tanto
que se tiene acceso el nodo como que el programa en el directorio puede verse
en el nodo remoto. De haber algún problema, será comunicado. Estos problemas
deben resolverse antes de proceder.
</p>

<p>
El único argumento para <c>tstmachines</c> es el nombre de la arquitectura;
es el mismo nombre que el del archivo machines. Por ejemplo, lo siguiente
comprueba si un programa en el directorio actual puede ser ejecutado por todas
las máquinas en la lista LINUX.
</p>

<pre caption="Haciendo un test">
# <i>/usr/local/mpich/sbin/tstmachines LINUX</i>
</pre>

<note>
Este programa es silencioso si todo va bien; si quiere verse lo que está
haciendo, se ha de usar el argumento -v (modo detallado):
</note>

<pre caption="Haciendo un test en modo detallado">
# <i>/usr/local/mpich/sbin/tstmachines -v LINUX</i>
</pre>

<p>
La salida de este comando podría ser:
</p>

<pre caption="Mensajes del comando anterior">
Trying true on host1.uoffoo.edu ...
Trying true on host2.uoffoo.edu ...
Trying ls on host1.uoffoo.edu ...
Trying ls on host2.uoffoo.edu ...
Trying user program on host1.uoffoo.edu ...
Trying user program on host2.uoffoo.edu ...
</pre>

<p>
Si <c>tstmachines</c> encuentra un problema, sugerirá las posibles razones
y las soluciones. Resumidamente, he aquí tres mensajes:
</p>

<ul>
	<li>
    <e>¿Se pueden iniciar los procesos en las máquinas remotas?</e> tstmachines
    intenta ejecutar un intérprete de comandos remoto <e>true</e> en cada una de
    las máquinas listada en el archivo <e>machines.LINUX</e> usando un intérprete
    de comandos remoto.
  </li>
	<li>
    <e>¿Está disponible el directorio de trabajo para todas las máquinas?</e> Esto 
    intenta hacer un ls de un archivo que crea tstmachines ejecutándolo desde un
    intérprete de comandos remoto.
  </li>
	<li>
    <e>¿Se pueden ejecutar programas de usuario en los sistemas remotos?</e> Esto
    comprueba que las librerías compartidas y otros componentes han sido correctamente
    instalados en todas las máquinas.
  </li>
</ul>

<p>
Y el test requerido para cada herramienta de desarrollo:
</p>

<pre caption="Comprobando una herramienta de desarrollo">
# <i>cd ~</i>
# <i>cp /usr/share/mpich/examples1/hello++.c ~</i>
# <i>make hello++</i>
# <i>mpirun -machinefile /usr/share/mpich/machines.LINUX -np 1 hello++</i>
</pre>

<p>
Para mayor información acerca de MPICH, puede consultarse la documentación de
<uri link="http://www-unix.mcs.anl.gov/mpi/mpich/docs/mpichman-chp4/mpichman-chp4.htm">http://www-unix.mcs.anl.gov/mpi/mpich/docs/mpichman-chp4/mpichman-chp4.htm</uri>.
</p>
</body>
</section>

<section>
<title>LAM</title>
<body>

<p>
(¡Pronto Disponible!)
</p>
</body>
</section>

<section>
<title>OMNI</title>
<body>

<p>
(¡Pronto Disponible!)
</p>
</body>
</section>
</chapter>

<chapter>
<title>Bibliografía</title>
<section>
<body>

<p>
El documento original se publicó en la página del <uri link="http://www.adelielinux.com">Centro Adelie Linux R&amp;D</uri>, y se ha reproducido aquí con el permiso de los autores y del <uri 
link="http://www.cyberlogic.ca">Centro Cyberlogic</uri> de Adelie Linux R&amp;D.
</p>

<ul>
	<li><uri>http://www.gentoo.org</uri>, Gentoo Technologies, Inc.</li>
	<li>
    <uri link="http://www.adelielinux.com">http://www.adelielinux.com</uri>, 
    Adelie Linux Research and Development Centre
  </li>
	<li>
    <uri link="http://nfs.sourceforge.net/">http://nfs.sourceforge.net</uri>, 
    Linux NFS Project
  </li>
	<li>
    <uri link="http://www-unix.mcs.anl.gov/mpi/mpich/">http://www-unix.mcs.anl.gov/mpi/mpich/</uri>, 
    Mathematics and Computer Science Division, Argonne National Laboratory
  </li>
	<li>
    <uri link="http://www.ntp.org/">http://ntp.org</uri>
  </li>
	<li>
    <uri link="http://www.eecis.udel.edu/~mills/">http://www.eecis.udel.edu/~mills/</uri>, 
    David L. Mills, University of Delaware
  </li>
	<li>
    <uri link="http://www.ietf.org/html.charters/secsh-charter.html">http://www.ietf.org/html.charters/secsh-charter.html</uri>, 
    Secure Shell Working Group, IETF, Internet Society
  </li>
	<li>
    <uri link="http://www.linuxsecurity.com/">http://www.linuxsecurity.com/</uri>, 
    Guardian Digital
  </li>
	<li>
    <uri link="http://www.openpbs.org/">http://www.openpbs.org/</uri>,  
    Altair Grid Technologies, LLC.
  </li>
</ul>
</body>
</section>
</chapter>
</guide>
