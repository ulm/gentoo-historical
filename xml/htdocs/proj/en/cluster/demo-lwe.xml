<?xml version='1.0' encoding="UTF-8"?>

<!DOCTYPE guide SYSTEM "/dtd/guide.dtd">
<guide link="/proj/en/cluster/demo-lwe.xml">
<title>Gentoo Cluster Demo</title>

<author title="Author"><mail
link="jean-francois@adelielinux.com">Jean-Francois
Richard</mail></author>

<abstract>This document was written by people at the <uri
link="http://www.adelielinux.com">Adelie Linux R&amp;D Center</uri>,
it presents the demonstration Gentoo cluster for the Linux World
Expo.</abstract>

<version>1.0</version>
<date>August 5, 2003</date>

<chapter><title>Hardware Platforms</title>

<section>
<body>

<p>Here are the hardware platforms accessible for the demo :</p>

<ul>
	<li><p>Galapagos: 5 node cluster featuring gigabit connectivity.</p>

	<ul>
	<li><p>Master node : AMD Duron(tm) : 1000 MHz</p></li>
	<li><p>4 slave nodes : AMD Athlon(tm) : 1250 MHz</p></li>
	</ul>

	</li>
</ul>

</body>
</section>
</chapter>

<chapter><title>Demo on Galapagos</title>
<section><body>

<p>We installed the following software on Galapagos :</p>

<ul>
        <li><p>POVRAY</p></li>
	<li><p>MPI-POVRAY</p></li>
	<li><p>PVM-POVRAY</p></li>
</ul>

<p>These are raytracing programs that run either on a single cpu or on
the whole cluster, using MPI or PVM for message passing.</p>

<p>An account named "demo@galapagos.cyberlogic.net" was created.  In
it, you can find a readme file and several .pov files to render.</p>

<p>You will need a VNC viewer to connect to the demo, with an SSH
tunnel that you can setup this way : </p>

<pre caption="SSH tunnel setup">
ssh -C -g -L 5901:localhost:5901 demo@galapagos.cyberlogic.net 
</pre>

<p>Once this is done, you can connect to your
<path>localhost:5901</path> with a VNC viewer, like KDE's "Remote
Desktop Connection" or "vncviewer".</p>

</body></section>
<section>
<title>Sample POVRAY commands</title>
<body>

<p>You can use the following commands from <path>~demo/</path>
to test the different POVRAYs installed :</p>

<p>We suggest running the vanilla POVRAY side-by-side with either the
MPI or PVM version to show the speed difference.  Just start two
terminals and run both with the following commands.</p>

<note>You can disable the X display by removing the "+D0 +D1 +p"
options.</note>

<note>You can resize the output by changing the "+wWIDTH +hHEIGHT"
parameters.</note>

<pre caption="POVRAY">
$ x-povray -i blue.pov +v1 -d +ft -x \
     +a0.300 +r3 -q9 -w640 -h480 -mv2.0 \
     +b1000 +D0 +D1 +p
</pre>

<p>This task takes 48 seconds.</p>

<pre caption="PVM-POVRAY">
$ pvm /etc/pvm_hosts
pvm> conf      should show node01-node04
pvm> quit

$ ${PVM_ROOT}/bin/LINUX/x-pvmpov \
    -i blue.pov \
    +v1 -d +ft -x +a0.300 +r3 -q9 -w640 \
    -h480 -mv2.0 +b1000 +D0 +D1 +p

Exit PVM
$ pvm
pvm> halt
</pre>

<p>This task takes 11 seconds.</p>

<pre caption="MPI-POVRAY">
$ lamboot /etc/lam-mpi/lam-bhost.lam
$ mpirun -np 5 `which mpi-x-povray` \
    -i blue.pov +v1 -d +ft -x +a0.300 +r3 \
    -q9 -w640 -h480 -mv2.0 +b1000 +D0 +D1 +p
$ lamhalt
</pre>

<note>You can change the number of CPUs used to compute the image by
changing the "-np X" option.</note>

<p>This task takes 9 seconds with "-np 5", 13 seconds with "-np 4", 20
seconds with "-np 3", 46 seconds with "-np 2".  The following chart
shows these results :</p>

<figure link="demo-lwe.png" short="MPI-POVRAY Results"
caption="MPI-POVRAY Results, showing a relation between the number of
nodes and the total processing time.  The red line shows the
non-parallel version of POVRAY."/>

<p>The above chart shows there is no significant gain between the 1
and 2 CPUs dots.  This difference is mostly caused by the
communication cost in the MPI version and the implementation
differences between POVRAY and MPI-POVRAY.  The comparison between
MPI-POVRAY and POVRAY cannot yield truly valid conclusions.</p>

<p>However, there is a a significant relation between the number of
CPUs and the overall speed, as predicted.  Note that the speed gain is
not linear and the speedup tends to zero when adding a lot of nodes.
This depends, again, on implementation and on the parallel algorithms
used.</p>

<p>Several scientific and industrial applications are designed to
scale to a lot more nodes while showing an almost-linear behaviour
regarding the speed gain vs.  number of nodes.  When using these
applications, clusters are a really interesting and cost-effective
solution to add processing power.</p>

</body>
</section>
</chapter>

<chapter><title>About this Document</title>
<section><body>

<p>The original document is published at the <uri
 link="http://www.adelielinux.com">Adelie Linux R&amp;D Centre</uri>
 web site, and is reproduced here with the permission of the authors
 and <uri link="http://www.cyberlogic.ca">Cyberlogic</uri>'s Adelie
 Linux R&amp;D Centre.</p>

</body></section></chapter>

</guide>
